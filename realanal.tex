\documentclass[12pt]{book}
%\usepackage{pdf14}
%Paper saving
%\documentclass[12pt,openany]{book}
%\documentclass[10pt,openany]{book}
%\documentclass[8pt,openany]{extbook}

\usepackage[T1]{fontenc}

% Footnotes should use symbols, not numbers.  Numbered footnotes are
% evil
\usepackage[perpage,symbol*]{footmisc}

%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{ifpdf}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
%\usepackage{color}
%\usepackage{graphics}
\usepackage[headings]{fullpage}
\usepackage{url}
\usepackage{varioref}
%\usepackage{floatflt}
%\usepackage{wrapfig}
\usepackage{makeidx}
\usepackage[pdftex]{hyperref}
\usepackage[all]{hypcap}
\usepackage[shortalphabetic]{amsrefs}
\usepackage[all]{xy}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{import}

%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkColor[gray]{0.95}
%\SetWatermarkScale{1}

\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=colon,singlelinecheck=false]{caption}


\usepackage{tikz}
\usepackage{rotating}

\newenvironment{myfigureht}{%
\begin{figure}[h!t]
\noindent\rule{\textwidth}{0.4pt}\vspace{12pt}\par\centering}%
{\par\noindent\rule{\textwidth}{0.4pt}
\end{figure}}


% Times
%\usepackage{txfonts}
% Times, but symbol/cm/ams math fonts
\usepackage{mathptmx}
% But we do want helvetica for sans
\usepackage{helvet}

%enumitem global options
\setlist{leftmargin=*,itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep}


% useful
\newcommand{\ignore}[1]{}

% analysis/geometry stuff
\newcommand{\ann}{\operatorname{ann}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Orb}{\operatorname{Orb}}
\newcommand{\hol}{\operatorname{hol}}
\newcommand{\aut}{\operatorname{aut}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\sing}{\operatorname{sing}}

% reals
\newcommand{\esssup}{\operatorname{ess~sup}}
\newcommand{\essran}{\operatorname{essran}}
\newcommand{\innprod}[2]{\langle #1 | #2 \rangle}
\newcommand{\linnprod}[2]{\langle #1 , #2 \rangle}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\Nul}{\operatorname{Nul}}
\newcommand{\Ran}{\operatorname{Ran}}
\newcommand{\sabs}[1]{\lvert {#1} \rvert}
\newcommand{\snorm}[1]{\lVert {#1} \rVert}
\newcommand{\abs}[1]{\left\lvert {#1} \right\rvert}
\newcommand{\norm}[1]{\left\lVert {#1} \right\rVert}

% sets (some)
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\D}{{\mathbb{D}}}
\newcommand{\F}{{\mathbb{F}}}

% consistent
\newcommand{\bB}{{\mathbb{B}}}
\newcommand{\bC}{{\mathbb{C}}}
\newcommand{\bR}{{\mathbb{R}}}
\newcommand{\bZ}{{\mathbb{Z}}}
\newcommand{\bN}{{\mathbb{N}}}
\newcommand{\bQ}{{\mathbb{Q}}}
\newcommand{\bD}{{\mathbb{D}}}
\newcommand{\bF}{{\mathbb{F}}}
\newcommand{\bH}{{\mathbb{H}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bP}{{\mathbb{P}}}
\newcommand{\bK}{{\mathbb{K}}}
\newcommand{\bV}{{\mathbb{V}}}
\newcommand{\CP}{{\mathbb{CP}}}
\newcommand{\RP}{{\mathbb{RP}}}
\newcommand{\HP}{{\mathbb{HP}}}
\newcommand{\OP}{{\mathbb{OP}}}
\newcommand{\sA}{{\mathcal{A}}}
\newcommand{\sB}{{\mathcal{B}}}
\newcommand{\sC}{{\mathcal{C}}}
\newcommand{\sF}{{\mathcal{F}}}
\newcommand{\sG}{{\mathcal{G}}}
\newcommand{\sH}{{\mathcal{H}}}
\newcommand{\sM}{{\mathcal{M}}}
\newcommand{\sO}{{\mathcal{O}}}
\newcommand{\sP}{{\mathcal{P}}}
\newcommand{\sQ}{{\mathcal{Q}}}
\newcommand{\sR}{{\mathcal{R}}}
\newcommand{\sS}{{\mathcal{S}}}
\newcommand{\sI}{{\mathcal{I}}}
\newcommand{\sL}{{\mathcal{L}}}
\newcommand{\sK}{{\mathcal{K}}}
\newcommand{\sU}{{\mathcal{U}}}
\newcommand{\sV}{{\mathcal{V}}}
\newcommand{\sX}{{\mathcal{X}}}
\newcommand{\sY}{{\mathcal{Y}}}
\newcommand{\sZ}{{\mathcal{Z}}}
\newcommand{\fS}{{\mathfrak{S}}}

\newcommand{\interior}{\operatorname{int}}

% Topo stuff
\newcommand{\id}{\textit{id}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\Torsion}{\operatorname{Torsion}}
\newcommand{\Ext}{\operatorname{Ext}}
\newcommand{\Hom}{\operatorname{Hom}}

%extra thingies
\newcommand{\mapsfrom}{\ensuremath{\text{\reflectbox{$\mapsto$}}}}
\newcommand{\from}{\ensuremath{\leftarrow}}
\newcommand{\dhat}[1]{\hat{\hat{#1}}}
\newcommand{\spn}{\operatorname{span}}

% San Serif fonts
%\renewcommand{\familydefault}{\sfdefault}

% To allow skrinking to 5.5 x 8.5 inches without whitespaces
% Make sure to rerun makeindex as well
% Useful for printing on lilu.com and saving on paper
%\addtolength{\textheight}{2.13in}
%\addtolength{\paperheight}{2.13in}

\hypersetup{
    %colorlinks,
    pdfborderstyle={/S/U/W 0.5},
    %pdfborder={0 0 0},
    %citecolor=black,
    %filecolor=black,
    %linkcolor=black,
    %urlcolor=black,
    pdfkeywords={real analysis, Riemann integral, derivative, limit, sequence},
    pdfsubject={Real Analysis},
    pdftitle={Basic Analysis: Introduction to Real Analysis},
    pdfauthor={Jiri Lebl}
}

% Set up our index
\makeindex

% Very simple indexing
\newcommand{\myindex}[1]{#1\index{#1}}

% define this to be empty to kill notes
\newcommand{\sectionnotes}[1]{\noindent \emph{Note: #1} \medskip \par}

% Define this to be empty to not skip page before the sections to
% save some paper
\newcommand{\sectionnewpage}{\clearpage}
%\newcommand{\sectionnewpage}{}

\author{Ji\v{r}\'i Lebl}

\title{Basic Analysis: Introduction to Real Analysis}

% Don't include subsections
\setcounter{tocdepth}{1}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}

\newtheoremstyle{exercise}% name
  {}% Space above
  {}% Space below
  {\itshape \small}% Body font
  {}% Indent amount 1
  {\bfseries \itshape \small}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\newenvironment{exnote}{\small}{}

\theoremstyle{exercise}
\newtheorem{exercise}{Exercise}[section]

\newtheoremstyle{example}% name
  {}% Space above
  {}% Space below
  {}% Body font
  {}% Indent amount 1
  {\bfseries}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\theoremstyle{example}
\newtheorem{example}[thm]{Example}

% referencing
\newcommand{\figureref}[1]{\hyperref[#1]{Figure~\ref*{#1}}}
\newcommand{\tableref}[1]{\hyperref[#1]{Table~\ref*{#1}}}
\newcommand{\chapterref}[1]{\hyperref[#1]{chapter~\ref*{#1}}}
\newcommand{\Chapterref}[1]{\hyperref[#1]{Chapter~\ref*{#1}}}
\newcommand{\sectionref}[1]{\hyperref[#1]{\S\ref*{#1}}}
\newcommand{\exerciseref}[1]{\hyperref[#1]{Exercise~\ref*{#1}}}
\newcommand{\remarkref}[1]{\hyperref[#1]{Remark~\ref*{#1}}}
\newcommand{\exampleref}[1]{\hyperref[#1]{Example~\ref*{#1}}}
\newcommand{\thmref}[1]{\hyperref[#1]{Theorem~\ref*{#1}}}
\newcommand{\propref}[1]{\hyperref[#1]{Proposition~\ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma~\ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary~\ref*{#1}}}
\newcommand{\defnref}[1]{\hyperref[#1]{Definition~\ref*{#1}}}

\begin{document}

\ifpdf
  \pdfbookmark{Title Page}{title}
\fi
\newlength{\centeroffset}
\setlength{\centeroffset}{-0.5\oddsidemargin}
\addtolength{\centeroffset}{0.5\evensidemargin}
%\addtolength{\textwidth}{-\centeroffset}
\thispagestyle{empty}
\vspace*{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\Huge\bfseries \sffamily Basic Analysis }
\noindent\rule[-1ex]{\textwidth}{5pt}\\[2.5ex]
\hfill\emph{\Large \sffamily Introduction to Real Analysis, Volume I}
\end{minipage}}

\vspace{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\bfseries 
by Ji{\v r}\'i Lebl\\[3ex]} 
\today
\\
(version 5.0)
% --- 5th edition, 0th update)
\end{minipage}}

%\addtolength{\textwidth}{\centeroffset}
\vspace{\stretch{2}}


\pagebreak

\vspace*{\fill}

%\begin{small} 
\noindent
Typeset in \LaTeX.


\bigskip

\noindent
Copyright \copyright 2009--2017 Ji{\v r}\'i Lebl

%\noindent
%ISBN XXXX

\bigskip

%\begin{floatingfigure}{1.4in}
%\vspace{-0.05in}
\noindent
\includegraphics[width=1.38in]{figures/license}
\quad
\includegraphics[width=1.38in]{figures/license2}
%\end{floatingfigure}

\bigskip

\noindent
This work is dual licensed under
the Creative Commons
Attribution-Non\-commercial-Share Alike 4.0 International License and
the Creative Commons
Attribution-Share Alike 4.0 International License.
To view a
copy of these licenses, visit
\url{http://creativecommons.org/licenses/by-nc-sa/4.0/}
or
\url{http://creativecommons.org/licenses/by-sa/4.0/}
or send a letter to
Creative Commons
PO Box 1866, Mountain View, CA 94042, USA.
%\end{small}

\bigskip

\noindent
%You can use, print, duplicate, share these notes as much as you want.  You can
%base your own notes on these and reuse parts if you keep the license the
%same.  If you plan to use these commercially (sell them for more than just
%duplicating cost), then you need to contact me and we will work something out.
%If you are printing a course pack for your students, then it is fine if the 
%duplication service is charging a fee for printing and selling the printed
%copy.  I consider that duplicating cost.
You can use, print, duplicate, share this book as much as you want.  You can
base your own notes on it and reuse parts if you keep the license the
same.  You can assume the license is either the CC-BY-NC-SA or CC-BY-SA,
whichever is compatible with what you wish to do, your derivative works must
use at least one of the licenses.

\bigskip

\noindent
During the writing of these notes, 
the author was in part supported by NSF grants DMS-0900885 and
DMS-1362337.

\bigskip

\noindent
The date is the main identifier of version.  The major version / edition
number is raised only if there have been substantial changes.  Edition
number started at 4, that is, version 4.0, as it was not kept track of
before.  %The edition given with the ISBN number is the major version.

\bigskip

\noindent
See \url{http://www.jirka.org/ra/} for more information
(including contact information).


% For large print do this
%\large

\microtypesetup{protrusion=false}
\tableofcontents
\microtypesetup{protrusion=true}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{INTRODUCTION}{INTRODUCTION}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{About this book}

This first volume of this book is a one semester course in basic analysis.
Together with the second volume it is a year-long course.
It started its life
as my lecture notes for teaching Math 444 at the
University of Illinois at Urbana-Champaign (UIUC) in Fall semester 2009.
Later I added the metric space chapter to teach Math 521 at University of
Wisconsin--Madison (UW).
Volume II was added to teach Math 4143/4153 at Oklahoma State University
(OSU).
A prerequisite for this course is a basic proof course,
using 
for example~\cite{Hammack}, \cite{GIAM}, or \cite{DW}.

It should be possible to use the book for
both a basic course for students who do not necessarily wish to
go to graduate school (such as UIUC 444), but also as a more advanced one-semester
course that also covers topics such as metric spaces (such as UW 521).
Here are my suggestions for what to cover in a semester course.  For a
slower course such as UIUC 444:
\begin{center}
\S0.3, \S1.1--\S1.4, \S2.1--\S2.5, \S3.1--\S3.4, \S4.1--\S4.2,
\S5.1--\S5.3, \S6.1--\S6.3
\end{center}
%\noindent
For a more rigorous course covering metric spaces that runs quite a bit faster
(such as UW 521):
\begin{center}
\S0.3, \S1.1--\S1.4, \S2.1--\S2.5, \S3.1--\S3.4, \S4.1--\S4.2,
\S5.1--\S5.3, \S6.1--\S6.2, \S7.1--\S7.6
\end{center}
%\noindent
It should also be possible to run a faster course without metric spaces
covering all sections of chapters 0 through 6.  The approximate number of
lectures given in the section notes through chapter 6 are a very rough
estimate and were designed for the slower course.
The first few chapters of the book can be used in an introductory proofs
course as is for example done at Iowa State University Math 201, where 
this book is used in conjunction with Hammack's Book of Proof~\cite{Hammack}.

With volume II one can run a year-long course that also covers multivariable
topics.  When I teach this course at OSU, I leave
metric spaces for the beginning of the
second semester and cover essentially all topics in this
first volume.

The book normally used for the class at UIUC is Bartle and Sherbert,
\emph{Introduction to Real Analysis} third edition
\cite{BS}.
The structure of the beginning of the book somewhat follows the
standard syllabus of UIUC Math 444 and therefore has some similarities with
\cite{BS}.
A major difference is that we define the Riemann integral using
Darboux sums and not tagged partitions.  The Darboux approach is far more
appropriate for a course of this level.

Our approach allows us to fit a course such as UIUC 444 within a semester
and still spend some time on the interchange of limits and end with
Picard's theorem on the existence and uniqueness of solutions of ordinary
differential equations.
This theorem is a wonderful example
that uses many results proved in the book.  For more advanced students,
material may be covered faster so that we arrive at metric spaces and
prove Picard's theorem using the fixed point theorem as is usual.

Other excellent books exist.  My favorite is 
Rudin's excellent
\emph{Principles of Mathematical Analysis}~\cite{Rudin:baby}
or as it is commonly and lovingly called \emph{baby Rudin}
(to distinguish it from his other great analysis textbook,
\emph{big Rudin}).  I took a
lot of inspiration and ideas from Rudin.  However, Rudin is a bit more
advanced and ambitious than this present course.
For those that wish to continue
mathematics, Rudin is a fine investment.
An inexpensive and somewhat simpler alternative to Rudin is
Rosenlicht's \emph{Introduction to Analysis}~\cite{Rosenlicht}.
%Rosenlicht
%may not be as dry as Rudin for those just starting out in mathematics.
There is also the freely downloadable \emph{Introduction to Real
Analysis} by William Trench~\cite{Trench}.
% for those that do not wish to
%invest much money.

\medskip

A note about the style of some of the proofs:  Many proofs
traditionally done by contradiction, I prefer to do by
a direct proof or by contrapositive.  While the book does include
proofs by contradiction, I only
do so when the contrapositive statement seemed too awkward, or when 
contradiction follows rather quickly.  In my opinion,
contradiction is more likely to get beginning students into trouble,
as we are talking about objects that do not exist.
%In a
%direct proof or a contrapositive proof one can be guided by intuition, but in a contradiction
%proof, intuition usually leads us astray.

I try to avoid unnecessary formalism where it is unhelpful.
Furthermore, the proofs and the language get slightly less formal as we
progress through the book, as more and more details are left out to avoid
clutter.

As a general rule, I use $:=$ instead of $=$ to define an
object rather than to simply show equality.  I use this symbol rather more
liberally than is usual for emphasis.
I use it even when the context is ``local,''
that is, I may simply define a function $f(x) := x^2$
for a single exercise or example.

%\medskip
%
%It is possible to skip or skim some material in the book as it is not used
%later on.  Some material that can safely be skipped is marked as such
%in the notes that appear below every section title.
%Section
%\sectionref{sec:basicset} can usually be covered lightly, or left as
%reading, depending on the prerequisites of the course.
%The section on Taylor's theorem
%(\sectionref{sec:taylor}) can safely be skipped as it is never used later.
%Other parts are marked in the section notes for each section.
%Uncountability of $\R$ in \sectionref{sec:intandsizeR} can safely be skipped.
%The alternative proof of Bolzano--Weierstrass in \sectionref{sec:bw} can
%safely be skipped.
%And of course, the section on Picard's theorem can also
%be skipped if there is no time at the end of the course, though I have not
%marked the section optional.

%It is possible to fill a semester with a course that ends with Picards
%theorem in \sectionref{sec:picard} by skipping little.  On the other hand,
%if metric spaces are covered, one might have to go a little faster and skip
%some topics in the first six chapters.

%\medskip
%
%Here is an approximate correspondence of the sections to \cite{BS}.  Note
%that the material in this book and in \cite{BS} differs.  The correspondence
%to other books is more complicated.
%
%FIXME: correspondence of new sections
%
%{\small
%
%\begin{center}
%\begin{tabular}{l|l|}
%Section
%& Section in \cite{BS}
%\\
%\hline
%\sectionref{sec:basicset} & \S1.1--\S1.3
%\\
%\sectionref{sec:basicpropsrn} & \S2.1 and \S2.3
%\\
%\sectionref{sec:setofreals} & \S2.3 and \S2.4
%\\
%\sectionref{sec:absval} & \S2.2
%\\
%\sectionref{sec:intandsizeR} & \S2.5
%\\
%\sectionref{sec:seqsandlims} & parts of \S3.1, \S3.2, \S3.3, \S3.4
%\\
%\sectionref{sec:factslimsseqs} & \S3.2
%\\
%\sectionref{sec:bw} & \S3.3 and \S3.4
%\\
%\sectionref{sec:cauchy} & \S3.5
%\\
%\sectionref{sec:series} & \S3.7
%\\
%\sectionref{sec:limoffunc} & \S4.1--\S4.2
%\\
%\sectionref{sec:cont} & \S5.1 and \S5.2
%\end{tabular}
%\begin{tabular}{|l|l}
%Section
%& Section in \cite{BS}
%\\
%\hline
%\sectionref{sec:minmaxint} & \S5.3
%\\
%\sectionref{sec:unifcont} & \S5.4
%\\
%\sectionref{sec:der} & \S6.1
%\\
%\sectionref{sec:mvt} & \S6.2
%\\
%\sectionref{sec:taylor} & \S6.3
%\\
%\sectionref{sec:rint} & \S7.1, \S7.2
%\\
%\sectionref{sec:rintprop} & \S7.2
%\\
%\sectionref{sec:ftc} & \S7.3
%\\
%\sectionref{sec:puconv} & \S8.1
%\\
%\sectionref{sec:liminter} & \S8.2
%\\
%\sectionref{sec:picard} & Not in \cite{BS}
%\\
%\chapterref{ms:chapter} & partially \S11
%\end{tabular}
%\end{center}
%}

%\medskip
%\hrule
\medskip

Finally, I would like to acknowledge Jana Ma\v{r}\'ikov\'a,
Glen Pugh, Paul Vojta, Frank Beatrous, S\"{o}nmez \c{S}ahuto\u{g}lu,
Jim Brandt, Kenji Kozai, and Arthur Busch,
for teaching with the book and giving me lots of useful feedback.
Frank Beatrous wrote the University of Pittsburgh version extensions, which
served as inspiration for many of the additions.
I would also like to thank
Dan Stoneham, Jeremy Sutter, Eliya Gwetta, Daniel Pimentel-Alarc\'on,
Steve Hoerning, Yi Zhang, Nicole Caviris,
Kristopher Lee, Baoyue Bi, Hannah Lund,
Trevor Mannella, Mitchel Meyer, Gregory Beauregard,
Chase Meadors, Andreas Giannopoulos, Nick Nelsen, Anton Petrunin,
an anonymous reader, and in general all the students in my classes for suggestions and
finding errors and typos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{About analysis} \label{sec:aboutra}

Analysis is the branch of mathematics that deals with inequalities and
limits.  The present course deals with the most basic 
concepts in analysis.  The goal of the course is to acquaint the reader
with rigorous proofs in analysis and also to
set a firm foundation for calculus of one variable.

Calculus has prepared you, the student, for using mathematics without telling
you why what you learned is true.  To use, or teach, mathematics
effectively, you cannot simply know \emph{what} is true, you must know
\emph{why} it is true.  This course shows you \emph{why} calculus
is true.  It is here to give you a good understanding of the concept of a
limit, the derivative, and the integral.

Let us use an analogy.
An auto mechanic that has learned to change the oil, fix broken headlights,
and charge the battery, will only be able to do those simple tasks.  He
will be unable to work independently to diagnose and fix problems.
A high school teacher that does not understand the definition of the Riemann
integral or the derivative may not be able to properly answer all the
students' questions.
To this day I remember several nonsensical statements I heard
from my calculus teacher in high school, who simply did not understand
the concept of the limit, though he could ``do'' all problems in calculus.

\medskip

We start with a discussion of the real number system, most importantly
its completeness property, which is the basis for all that comes after.
We then discuss the simplest form of a limit,
the limit of a sequence.  Afterwards, we study
functions of one variable, continuity, and the derivative.
Next, we define the Riemann integral and prove the fundamental theorem of
calculus.  We discuss sequences of functions and the
interchange of limits.  Finally, we give an introduction to metric
spaces.

Let us give the most important difference between analysis and
algebra.  In algebra, we prove equalities directly; we prove that
an object, a number perhaps, is equal to another object.  In analysis,
we usually prove inequalities.  To illustrate the point, consider the
following statement.

\medskip

\emph{Let $x$ be a real number.  If $x < \epsilon$ is true for all
real numbers
$\epsilon > 0$, then $x \leq 0$}.

\medskip

This statement is the general idea of what we do in analysis.  First, to
prove
$x = 0$, we prove two inequalities: $x \leq 0$ and $x \geq 0$.  To
prove the inequality
$x \leq 0$, we prove 
$x < \epsilon$ for all positive $\epsilon$.

\medskip

The term \emph{real analysis} is a little bit of a misnomer.  I prefer to
use simply \emph{analysis}.  The other type of analysis, 
\emph{complex analysis}, really builds up on the present material, rather than
being distinct.  Furthermore, a more advanced course on real
analysis would talk about complex numbers often.
I suspect the nomenclature is
historical baggage.

\medskip

Let us get on with the show\ldots


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Basic set theory} \label{sec:basicset}
\index{set theory}

\sectionnotes{1--3 lectures (some material can be skipped or covered lightly)}

Before we start talking about analysis we need to fix some language.
Modern\footnote{The term ``modern'' refers to late 19th century up to
the present.}
analysis uses the language of sets, and therefore that is where we start.
We talk about sets in a rather informal way, using the so-called
``\myindex{na\"ive set theory}.''  Do not worry, that is what majority of
mathematicians use, and it is hard to get into trouble.
We assume the reader has seen the very basics of set theory
and proof writing before, and this section should be a quick refresher.

\subsection{Sets}

\begin{defn}
A \emph{\myindex{set}} is a collection of objects called
\emph{elements}\index{element} or \emph{members}\index{member}.  A set with
no objects is called the \emph{\myindex{empty set}} and is denoted by
$\emptyset$ (or sometimes by $\{ \}$).
\end{defn}

Think of a set as a club with a certain membership.  For
example, the students who play chess are members of the chess club.  However,
do not take the analogy too far.  A set is only defined by the members
that form the set; two sets that have the same members are the same set.

Most of the time we will consider sets
of numbers.  For example, the set
\begin{equation*}
S := \{ 0, 1, 2 \}
\end{equation*}
is the set containing
the three elements 0, 1, and 2.
By ``$:=$'', we mean we are defining what $S$ is, rather than
just showing equality.
We write
\begin{equation*}
1 \in S
\end{equation*}
to denote that the number 1 belongs to the set $S$.  That is, 1 is a member
of $S$.  Similarly we write
\begin{equation*}
7 \notin S
\end{equation*}
to denote that the number 7 is not in $S$.  That is, 7 is not a member of
$S$.
The elements of all sets under consideration come from some set we call the
\emph{\myindex{universe}}.  For simplicity,
we often consider the universe to be the set that contains only the elements
we are interested in.
The universe is generally understood from context
and is not explicitly mentioned.  In this course, our universe will
most often be the set of real numbers.

While the elements of a set are often numbers,
other objects, such as other sets, can be elements of a set.
A set may also contain some of the same elements as another set.  For example,
\begin{equation*}
T := \{ 0, 2 \}
\end{equation*}
contains the numbers 0 and 2.  In this case all elements of $T$ also
belong to $S$.  We write $T \subset S$.  More formally we make the
following definition.

\begin{defn}
{\ }
\begin{enumerate}[(i)]
\item
A set $A$ is a \emph{\myindex{subset}}
of a set $B$ if $x \in A$ implies $x \in B$, and we write $A \subset B$.
That is, all members of $A$ are also members of $B$.
\item
Two sets $A$ and $B$ are \emph{\myindex{equal}} if $A \subset B$ and $B
\subset A$.  We write $A = B$.
That is, $A$ and $B$ contain exactly the same elements.
If it is not true that $A$ and $B$ are equal, then 
we write $A \not= B$.
\item
A set $A$ is a \emph{\myindex{proper subset}} of $B$ if $A \subset B$
and $A \not= B$.  We write $A \subsetneq B$.
\end{enumerate}
\end{defn}

For example, for $S$ and $T$ defined above $T \subset S$, but
$T \not= S$.  So $T$ is a proper subset of $S$.
If $A = B$, then $A$ and $B$ are simply two names for the
same exact set.
Let us mention the
\emph{\myindex{set building notation}},
\begin{equation*}
\{ x \in A : P(x) \} .
\end{equation*}
This notation refers to a subset of the set $A$ containing all elements
of $A$ that satisfy the property $P(x)$.
For example, using $S = \{ 0, 1, 2 \}$ as above, $\{ x \in S : x \not= 2 \}$
is the set $\{ 0, 1 \}$.
The notation is sometimes
abbreviated, $A$ is not mentioned when understood from context.
Furthermore, $x \in A$ is sometimes replaced with a formula to make the notation
easier to read.  

\begin{example}
The following are sets including the standard notations.
\begin{enumerate}[(i)]
\item The set of \emph{\myindex{natural numbers}}, $\N := \{ 1, 2, 3, \ldots
\}$.
\item The set of \emph{\myindex{integers}}, $\Z := \{ 0, -1, 1, -2, 2, \ldots
\}$.
\item The set of \emph{\myindex{rational numbers}}, $\Q := \{ \frac{m}{n} :  m, n \in \Z
\text{ and } n \not= 0 \}$.
\item The set of even natural numbers, $\{  2m : m \in \N \}$.
\item The set of real numbers, $\R$.
\end{enumerate}

Note that $\N \subset \Z \subset \Q \subset \R$.
\end{example}

There are many operations we want to do with sets.

\begin{defn}
{\ }
\begin{enumerate}[(i)]
\item
A \emph{\myindex{union}} of two sets $A$ and $B$ is defined as
\begin{equation*}
A \cup B := \{ x : x \in A \text{ or } x \in B \} .
\end{equation*}
\item
An \emph{\myindex{intersection}} of two sets $A$ and $B$ is defined as
\begin{equation*}
A \cap B := \{ x : x \in A \text{ and } x \in B \} .
\end{equation*}
\item
A \emph{complement of $B$ relative to $A$\index{complement relative to}}
(or \emph{\myindex{set-theoretic difference}} of $A$ and $B$) is defined as
\begin{equation*}
A \setminus B := \{ x : x \in A \text{ and } x \notin B \} .
\end{equation*}
\item
We say
\emph{\myindex{complement}} of $B$ and write $B^c$ instead of $A \setminus B$ if
the set $A$ is either the entire universe or is the obvious
set containing $B$, and is understood from context.
\item
We say sets $A$ and $B$ are \emph{\myindex{disjoint}} if $A \cap B =
\emptyset$.
\end{enumerate}
\end{defn}

The notation $B^c$ may be a little vague at this point.  If
the set $B$ is a subset of the real numbers $\R$, then $B^c$ means
$\R \setminus B$.  If $B$ is naturally a subset of the natural numbers,
then $B^c$
is $\N \setminus B$.  If ambiguity would ever arise, we will 
use the set difference notation $A \setminus B$.

\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figsetop.pdf_t}
\caption{Venn diagrams of set operations.\label{figsetop}}
%\end{center}
\end{myfigureht}
We illustrate the operations on the
\emph{Venn diagrams}\index{Venn diagram} in \figureref{figsetop}.
Let us now establish one of most basic theorems about sets and logic.

\begin{thm}[DeMorgan]\index{DeMorgan's theorem}
Let $A, B, C$ be sets.  Then
\begin{align*}
{(B \cup C)}^c &= B^c \cap C^c , \\
{(B \cap C)}^c &= B^c \cup C^c ,
\end{align*}
or, more generally,
\begin{align*}
A \setminus (B \cup C) &= (A \setminus B) \cap (A \setminus C) , \\
A \setminus (B \cap C) &= (A \setminus B) \cup (A \setminus C) .
\end{align*}
\end{thm}

\begin{proof}
The first statement is proved by the second statement if we
assume the set $A$ is our ``universe.''

Let us prove $A \setminus (B \cup C) = (A \setminus B) \cap (A \setminus C)$.
Remember the definition of equality of sets.  First, we must show that
if $x \in A \setminus (B \cup C)$, then
$x \in (A \setminus B) \cap (A \setminus C)$.  Second, we must also show that
if $x \in (A \setminus B) \cap (A \setminus C)$, then
$x \in A \setminus (B \cup C)$.

So let us assume $x \in A \setminus (B \cup C)$.  Then $x$ is in 
$A$, but not in $B$ nor $C$.  Hence $x$ is in $A$ and not in $B$, that is,
$x \in A \setminus B$.  Similarly $x \in A \setminus C$.  Thus
$x \in (A \setminus B) \cap (A \setminus C)$.

On the other hand suppose 
$x \in (A \setminus B) \cap (A \setminus C)$.  In particular
$x \in (A \setminus B)$, so 
$x \in A$ and $x \notin B$.  Also as $x \in (A \setminus C)$, then $x \notin C$.
Hence $x \in A \setminus (B \cup C)$.

The proof of the other equality is left as an exercise.
\end{proof}

The result above we called a \emph{Theorem}, while most results we will call
a \emph{Proposition}, and a few \emph{Lemma} (a result leading to another result) or
\emph{Corollary} (a quick consequence of the preceeding result).  Do not read too much into
the naming.  Some of it is traditional, some of it is stylistic choice.
It is not necessarily true that a \emph{Theorem} is ``more important'' than a
\emph{Proposition} or a \emph{Lemma}.

We will also need to intersect or union several sets at once.  If there are
only finitely many, then we simply apply the union or intersection operation
several times.  However, suppose we have an infinite collection
of sets (a set of sets)
$\{ A_1, A_2, A_3, \ldots \}$.  We define
\begin{align*}
& \bigcup_{n=1}^\infty A_n := \{ x : x \in A_n \text{ for some $n \in \N$}
\} , \\
& \bigcap_{n=1}^\infty A_n := \{ x : x \in A_n \text{ for all $n \in \N$}
\} .
\end{align*}

We can also have sets indexed by two integers.  For example, we can have
the set of sets
$\{ A_{1,1}, A_{1,2}, A_{2,1}, A_{1,3}, A_{2,2}, A_{3,1}, \ldots \}$.  Then
we write 
\begin{equation*}
\bigcup_{n=1}^\infty \bigcup_{m=1}^\infty A_{n,m}
=
\bigcup_{n=1}^\infty \left( \bigcup_{m=1}^\infty A_{n,m} \right) .
\end{equation*}
And similarly with intersections.

It is not hard to see that we can take the unions in any order.  However,
switching the order of unions and intersections is not generally permitted without proof.
For example:
\begin{equation*}
\bigcup_{n=1}^\infty
\bigcap_{m=1}^\infty
\{ k \in \N : mk < n \}
=
\bigcup_{n=1}^\infty \emptyset = \emptyset .
\end{equation*}
However,
\begin{equation*}
\bigcap_{m=1}^\infty
\bigcup_{n=1}^\infty
\{ k \in \N : mk < n \}
=
\bigcap_{m=1}^\infty
\N
=
\N.
\end{equation*}

Sometimes, the index set is not the natural numbers.  In this case we need a
more general notation.  Suppose $I$ is some set and for each $\iota \in
I$, we have a set $A_\iota$.  Then we define
\begin{equation*}
\bigcup_{\iota \in I} A_\iota := \{ x : x \in A_\iota \text{ for some $\iota \in I$}
\} 
\qquad
\bigcap_{\iota \in I} A_\iota := \{ x : x \in A_\iota \text{ for all $\iota \in I$}
\} .
\end{equation*}

\subsection{Induction}

When a statement includes an arbitrary natural number,
a common method of proof is the principle of induction.  
We start with the set of natural numbers $\N = \{ 1,2,3,\ldots \}$, and we
give them their natural ordering, 
that is, $1 < 2 < 3 < 4 < \cdots$.
By $S \subset \N$ having a \emph{\myindex{least element}}, we mean that
there exists an $x \in S$,
such that for every
$y \in S$, we have $x \leq y$.

%\pagebreak[2]

The natural numbers $\N$ ordered in the natural way
possess the so-called \emph{\myindex{well ordering property}}.
We take this property
as an axiom; we simply assume it is true.

\theoremstyle{plain}
\newtheorem*{wellordprop}{Well ordering property of $\N$}
\hypertarget{wop:link}{}%
\begin{wellordprop}
Every nonempty subset of $\N$ has a least (smallest) element.
\end{wellordprop}

The \emph{principle of \myindex{induction}}\index{principle of induction} is
the following theorem, which is equivalent to the well ordering property of
the natural numbers.

\begin{thm}[Principle of induction] \label{induction:thm}
Let $P(n)$ be a statement depending on a natural number $n$.  Suppose that
\begin{enumerate}[(i)]
\item \emph{(basis statement)} $P(1)$ is true,
\item \emph{(induction step)} if $P(n)$ is true, then $P(n+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for all $n \in \N$.
\end{thm}

\begin{proof}
Suppose $S$ is the set of natural numbers $m$ for which $P(m)$ is
not true.  Suppose
$S$ is nonempty.  Then $S$ has a least element by the well ordering
property.  Let us call $m$ the least element of $S$.  We know $1 \notin
S$ by assumption.  Therefore $m > 1$ and $m-1$ is a natural number as well.
Since $m$ was the least element of $S$, we know that $P(m-1)$ is true.
But by the induction step we see that $P(m-1+1) = P(m)$ is true, 
contradicting the statement that $m \in S$.  Therefore $S$ is empty and 
$P(n)$ is true for all $n \in \N$.
\end{proof}

Sometimes it is convenient to start at a different number than 1, but 
all that changes is the labeling.  The assumption that
$P(n)$ is true in ``if $P(n)$ is true,
then $P(n+1)$ is true''
is usually called the \emph{\myindex{induction hypothesis}}.

\begin{example}
Let us prove that for all $n \in \N$,
\begin{equation*}
2^{n-1} \leq n! \qquad \text{(recall $n! = 1 \cdot 2 \cdot 3 \cdots n$)}.
\end{equation*}
We let $P(n)$ be the statement that
$2^{n-1} \leq n!$ is true.  By plugging in $n=1$, we see that $P(1)$
is true.

Suppose $P(n)$ is true.  That is, suppose 
$2^{n-1} \leq n!$ holds.  Multiply both sides by 2 to obtain
\begin{equation*}
2^n \leq 2(n!) .
\end{equation*}
As $2 \leq (n+1)$ when $n \in \N$, we have
$2(n!) \leq (n+1)(n!) = (n+1)!$.  That is,
\begin{equation*}
2^n \leq 2(n!) \leq  (n+1)!,
\end{equation*}
and hence $P(n+1)$ is true.  By the principle of induction, we see that
$P(n)$
is true for all $n$, and hence
$2^{n-1} \leq n!$ is true for all $n \in \N$.
\end{example}

\begin{example} \label{example:geometricsum}
We claim that for all $c \not= 1$,
\begin{equation*}
1 + c + c^2 + \cdots + c^n = \frac{1-c^{n+1}}{1-c} .
\end{equation*}

Proof: It is easy to check that the equation holds with $n=1$.  Suppose 
it is true for $n$.  Then
\begin{equation*}
\begin{split}
1 + c + c^2 + \cdots + c^n + c^{n+1} & =
( 1 + c + c^2 + \cdots + c^n ) + c^{n+1} \\
& = \frac{1-c^{n+1}}{1-c}  + c^{n+1} \\
& = \frac{1-c^{n+1}  + (1-c)c^{n+1}}{1-c} \\
& = \frac{1-c^{n+2}}{1-c} .
\end{split}
\end{equation*}
\end{example}

Sometimes, it is easier to use in the inductive step
that $P(k)$ is true for all $k = 1,2,\ldots,n$, not just for $k=n$.
This principle is called \emph{\myindex{strong induction}} and is equivalent
to the normal induction above.  The proof that
equivalence is left as an exercise.

\begin{thm}[Principle of strong induction]
\index{principle of strong induction}\index{strong induction}
Let $P(n)$ be a statement depending on a natural number $n$.  Suppose that
\begin{enumerate}[(i)]
\item \emph{(basis statement)} $P(1)$ is true,
\item \emph{(induction step)} if $P(k)$ is true for all $k = 1,2,\ldots,n$, then $P(n+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for all $n \in \N$.
\end{thm}

\subsection{Functions}

Informally,
a \emph{\myindex{set-theoretic function}}\index{function}
$f$ taking a set $A$ to a set $B$
is a mapping that to each $x \in A$ assigns a unique $y \in B$.  We write
$f \colon A \to B$.  For example, we
define a function $f \colon S \to T$ taking $S = \{ 0, 1, 2 \}$ to $T = \{ 0, 2 \}$
by assigning $f(0) := 2$, $f(1) := 2$, and $f(2) := 0$.  That is, a function $f
\colon A \to B$ is
a black box, into which we stick an element of $A$ and the function
spits out an element of $B$.
Sometimes $f$ is called a \emph{\myindex{mapping}}
and we say $f$ \emph{maps $A$ to $B$}.


Often, functions are defined by some sort of
formula, however, you should really think of a function as just a very big
table of values.
The subtle issue here is that a single function can have several different
formulas, all giving the same function.  Also, for many functions, there is
no formula that expresses its values.

To define a function rigorously, first let us define the Cartesian product.

\begin{defn}
Let $A$ and $B$ be sets.  The \emph{\myindex{Cartesian product}}
is the set of tuples defined as
\begin{equation*}
A \times B :=
\{ (x,y) : x \in A, y \in B \} .
\end{equation*}
\end{defn}

For example, the set $[0,1] \times [0,1]$ is a set in the plane
bounded by a square with vertices $(0,0)$, $(0,1)$, $(1,0)$, and $(1,1)$.
When $A$ and $B$ are the same set we sometimes use a superscript 2 to denote
such a product.  For example $[0,1]^2 = 
[0,1] \times [0,1]$, or $\R^2 = \R \times \R$ (the Cartesian plane).

\begin{defn}
A \emph{function} $f \colon A \to B$ is a subset $f$ of $A \times B$
such that for each $x \in A$, there is a unique $(x,y) \in f$.  We then
write $f(x) = y$.  Sometimes
the set $f$ is called the \emph{\myindex{graph}} of the function rather than
the function itself.

The set $A$ is called the \emph{\myindex{domain}} of $f$ (and
sometimes confusingly denoted $D(f)$).  The set
\begin{equation*}
R(f) := \{ y \in B : \text{there exists an $x$ such that
%$(x,y) \in f$
$f(x)=y$
} \}
\end{equation*}
is called the \emph{\myindex{range}} of $f$.
\end{defn}

Note that $R(f)$ can possibly be a proper subset of $B$,
while the domain of $f$ is always equal to $A$.  We usually 
assume that the domain of $f$ is nonempty.

\begin{example}
From calculus, you are most familiar with functions taking real numbers to real
numbers.  However, you saw some other types of functions as well.  For
example, the derivative is a function mapping the set of
differentiable functions to the set of all functions.
Another example is the Laplace transform, which also
takes functions to functions.  Yet another example is the function that takes
a continuous function $g$ defined on the interval $[0,1]$ and returns the
number $\int_0^1 g(x) dx$.
\end{example}

\begin{defn}
Let $f \colon A \to B$ be a function, and $C \subset A$.  Define
the \emph{\myindex{image}} (or \emph{\myindex{direct
image}}) of $C$ as
\begin{equation*}
f(C) := \{ f(x) \in B : x \in C \} .
\end{equation*}
Let $D \subset B$.  Define the \emph{\myindex{inverse image}} as
\begin{equation*}
f^{-1}(D) := \{ x \in A : f(x) \in D \} .
\end{equation*}
\end{defn}
\begin{myfigureht}
%\begin{center}
\parbox{2.5in}{\subimport*{figures/}{funcimags.pdf_t}}
\parbox{2in}{%
$f(\{1,2,3,4\}) = \{ b, c, d \}$\\[3pt]
$f(\{1,2,4\}) = \{ b, d \}$\\[3pt]
$f(\{1\}) = \{ b \}$\\[3pt]
$f^{-1}(\{a,b,c\}) = \{ 1, 3, 4 \}$\\[3pt]
$f^{-1}(\{a\}) = \emptyset$\\[3pt]
$f^{-1}(\{b\}) = \{ 1, 4 \}$
}
\caption{Example of direct and inverse images for the function $f \colon \{ 1,2,3,4 \} \to \{
a,b,c,d \}$ defined by
$f(1) := b$,
$f(2) := d$,
$f(3) := c$,
$f(4) := b$.\label{figfuncimags}}
%\end{center}
\end{myfigureht}

\begin{example}
Define the function $f \colon \R \to \R$ by
$f(x) := \sin(\pi x)$.  Then $f([0,\nicefrac{1}{2}]) = [0,1]$, 
$f^{-1}(\{0\}) = \Z$, etc\ldots.
\end{example}

\begin{prop} \label{st:propinv}
Let $f \colon A \to B$.  Let $C, D$ be subsets of $B$.  Then
\begin{align*}
& f^{-1}( C \cup D) = f^{-1} (C) \cup f^{-1} (D) , \\
& f^{-1}( C \cap D) = f^{-1} (C) \cap f^{-1} (D) , \\
& f^{-1}( C^c) = {\left( f^{-1} (C) \right)}^c .
\end{align*}
\end{prop}

Read the last line as
$f^{-1}( B \setminus C) = A \setminus f^{-1} (C)$.

\begin{proof}
Let us start with the union.  Suppose $x \in 
f^{-1}( C \cup D)$.  That means 
$x$ maps to $C$ or $D$.  Thus
$f^{-1}( C \cup D) \subset f^{-1} (C) \cup f^{-1} (D)$.  Conversely
if $x \in f^{-1}(C)$, then $x \in f^{-1}(C \cup D)$.  Similarly for
$x \in f^{-1}(D)$.  Hence
$f^{-1}( C \cup D) \supset f^{-1} (C) \cup f^{-1} (D)$, and we have
equality.

The rest of the proof is left as an exercise.
\end{proof}

The proposition does not hold for direct images.  We do have
the following weaker result.

\begin{prop} \label{st:propfor}
Let $f \colon A \to B$.  Let $C, D$ be subsets of $A$.  Then
\begin{align*}
& f( C \cup D) = f (C) \cup f (D) , \\
& f( C \cap D) \subset f (C) \cap f (D) .
\end{align*}
\end{prop}

The proof is left as an exercise.

\begin{defn}
Let $f \colon A \to B$ be a function.
The function $f$ is said to be
\emph{\myindex{injective}} or
\emph{\myindex{one-to-one}} if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.  In
other words,
for all $y \in B$ the set
$f^{-1}(\{y\})$ is empty or consists of a single element.
We call such an $f$ an \emph{\myindex{injection}}.

The function $f$ is said to be
\emph{\myindex{surjective}} or
\emph{\myindex{onto}} if $f(A) = B$.
We call such an $f$ a \emph{\myindex{surjection}}.

A function $f$ that is both an injection and a surjection is
said to be \emph{\myindex{bijective}}, and we say $f$ is a
\emph{\myindex{bijection}}.
\end{defn}

When $f \colon A \to B$ is a bijection, then $f^{-1}(\{y\})$ is always
a unique element of $A$, and we can consider $f^{-1}$ as a function
$f^{-1} \colon B \to A$.
In this case, we call $f^{-1}$ the \emph{\myindex{inverse function}} of $f$.
For example, for the bijection $f \colon \R \to \R$ defined by $f(x) := x^3$ we have
$f^{-1}(x) = \sqrt[3]{x}$.

A final piece of notation for functions that
we need is the \emph{\myindex{composition of functions}}.

\begin{defn}
Let $f \colon A \to B$, $g \colon B \to C$.  The function 
$g \circ f \colon A \to C$ is defined as
\begin{equation*}
(g \circ f)(x) := g\bigl(f(x)\bigr) .
\end{equation*}
\end{defn}

\subsection{Cardinality}

A subtle issue in set theory and one generating a considerable amount of
confusion among students is that of cardinality, or ``size'' of sets.  The
concept of cardinality is important in modern mathematics in general and
in analysis in particular.  In this section, we will see the first really
unexpected theorem.

\begin{defn}
Let $A$ and $B$ be sets.  We say $A$ and $B$ have the same
\emph{\myindex{cardinality}}
when there exists a bijection $f \colon A \to B$.  We denote
by $\abs{A}$ the equivalence class of all sets with the same cardinality as
$A$ and we simply call $\abs{A}$ the cardinality of $A$.
\end{defn}

For example $\{ 1,2,3 \}$ has the same cardinality as $\{ a,b,c \}$ by
defining a bijection $f(1) := a$, $f(2) := b$, $f(3) := c$.  Clearly the
bijection is not unique.

Note that $A$ has the same cardinality as the empty set if and only
if $A$ itself is the empty set.  We then write $\abs{A} := 0$.

\begin{defn}
Suppose $A$ has the same cardinality as $\{ 1,2,3,\ldots,n \}$
for some $n \in \N$.
We then write $\abs{A} := n$.  If $A$ is empty we write $\abs{A} := 0$.
In either case we say that $A$ is
\emph{\myindex{finite}}.

We say $A$ is \emph{\myindex{infinite}} or ``of infinite cardinality''
if $A$ is not finite.
\end{defn}

That the notation $\abs{A} = n$ is justified we leave as an exercise.  That
is, for each nonempty finite set $A$, there exists a unique natural number
$n$ such that there exists a bijection from $A$ to $\{ 1,2,3,\ldots,n \}$.


We can order sets by size.

\begin{defn} \label{def:comparecards}
We write
\begin{equation*}
\abs{A} \leq \abs{B}
\end{equation*}
if there exists an injection from $A$ to $B$.  We write $\abs{A} = \abs{B}$
if $A$ and $B$ have the same cardinality.  We write $\abs{A} < \abs{B}$
if $\abs{A} \leq \abs{B}$, but $A$ and $B$ do not have the same cardinality.
\end{defn}

We state without proof that
$\abs{A} = \abs{B}$ have the same cardinality if and only if
$\abs{A} \leq \abs{B}$ and
$\abs{B} \leq \abs{A}$.  This is the so-called
\myindex{Cantor--Bernstein--Schr\"oder} theorem.
Furthermore, if $A$ and $B$ are any two sets,
we can always write $\abs{A} \leq \abs{B}$ or
$\abs{B} \leq \abs{A}$.  The issues surrounding this
last statement are very subtle.  As we do not require either
of these two statements, we omit proofs.

The truly interesting cases of cardinality are infinite sets.  We start with the following
definition.

\begin{defn}
If $\abs{A} = \abs{\N}$, then $A$ is said to be \emph{\myindex{countably
infinite}}.  If $A$ is finite or countably infinite, then we say $A$
is \emph{\myindex{countable}}.  If $A$ is not countable, then
$A$ is said to be \emph{\myindex{uncountable}}.
\end{defn}

The cardinality of $\N$ is usually denoted as
$\aleph_0$ (read as aleph-naught)\footnote{For the fans of the TV show
\emph{Futurama}, there is a movie theater in one episode
called an $\aleph_0$-plex.}.

\begin{example}
The set of even natural numbers has the same cardinality as $\N$.  Proof:
Let $E \subset \N$ be the set of even natural numbers.
Given $k \in E$, write $k=2n$ for some $n \in \N$.
Then $f(n) := 2n$ defines a bijection $f \colon \N \to E$.
\end{example}

In fact, let us mention without proof the following characterization
of infinite sets: \emph{A set is infinite if and only if it is in one-to-one
correspondence with a proper subset of itself}.

\begin{example}
$\N \times \N$ is a countably infinite set.  Proof: Arrange the
elements of $\N \times \N$ as follows
$(1,1)$, $(1,2)$, $(2,1)$, $(1,3)$, $(2,2)$, $(3,1)$, \ldots.  That is,
always write down first all the elements whose two entries sum to $k$,
then write down all the elements whose entries sum to $k+1$ and so on.
Then define a bijection with $\N$ by letting 1 go to $(1,1)$,
2 go to $(1,2)$ and so on.
\end{example}

\begin{example}
The set of rational numbers is countable.  Proof: (informal)
Follow the same procedure
as in the previous example, writing $\nicefrac{1}{1}$, $\nicefrac{1}{2}$,
$\nicefrac{2}{1}$, etc\ldots.  However,
leave out any fraction (such as $\nicefrac{2}{2}$)
that has already appeared.
\end{example}

For completeness we mention the following statements
from the exercises.
\emph{If $A \subset
B$ and $B$ is countable, then $A$ is countable.}  The contrapositive of the
statement is that if $A$ is
uncountable, then $B$ is uncountable.
%The proof is left as an exercise.
As a consequence if $\abs{A} < \abs{\N}$ then $A$ is
finite.
Similarly, if $B$ is finite and $A \subset B$, then $A$ is finite.

%  As we will not need this statement in
%the sequel, and as the proof requires the
%Cantor--Bernstein--Schr\"oder theorem mentioned above, we will not give it
%here.

We give the first truly striking result.  First, we need a notation for
the set of all subsets of a set.

\begin{defn}
If $A$ is a set,
we define the \emph{\myindex{power set}} of $A$, denoted by $\sP(A)$, to be
the set of all subsets of $A$.
\end{defn}

For example, if $A := \{ 1,2\}$, then $\sP(A) = \{ \emptyset, \{ 1 \}, \{ 2 \},
\{ 1, 2 \} \}$.  For a finite set $A$ of cardinality $n$, the
cardinality of $\sP(A)$ is $2^n$.  This fact is left as an exercise.  
Hence, for finite sets
the cardinality of $\sP(A)$ is strictly
larger than the
cardinality of $A$.  What is an unexpected and
striking fact is that this statement is still true for infinite sets.

\begin{thm}[Cantor%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Georg_Cantor}{Georg Ferdinand Ludwig
Philipp Cantor} (1845 -- 1918).}]
\index{Cantor's theorem}
\label{cantorspowersetthm}
$\abs{A} < \abs{\sP(A)}$.  In particular, there exists no surjection from
$A$ onto $\sP(A)$.
\end{thm}

\begin{proof}
There exists an injection $f \colon A \to \sP(A)$.
For any $x \in A$, define $f(x) := \{ x \}$.  Therefore
$\abs{A} \leq \abs{\sP(A)}$.

To finish the proof, we must show that
no function $g \colon A \to \sP(A)$ is a surjection.
Suppose 
$g \colon A \to \sP(A)$ is a function.  So for $x \in A$,
$g(x)$ is a subset of $A$.  Define the set
\begin{equation*}
B := \{ x \in A : x \notin g(x) \} .
\end{equation*}
We claim that $B$ is not in the range of $g$ and hence $g$ is not a
surjection.  Suppose there exists an $x_0$ such that $g(x_0) = B$.
Either $x_0 \in B$ or $x_0 \notin B$.  If $x_0 \in B$, then $x_0 \notin
g(x_0) = B$, which is a contradiction.  If $x_0 \notin B$, then $x_0 \in
g(x_0) = B$, which is again a contradiction.  Thus such an $x_0$ does not
exist.  Therefore, $B$ is not in the range of $g$, and $g$ is not a
surjection.  As $g$ was an arbitrary function, no surjection exists.
\end{proof}

One particular consequence of this 
theorem is that there do exist uncountable sets,
as $\sP(\N)$ must be uncountable.
A related fact is that
the set of real numbers (which we study in the next chapter) is uncountable.
The existence of uncountable sets may seem unintuitive, and the theorem
caused quite a controversy at the time
it was announced.  The theorem not only says that uncountable sets exist,
but that there in fact exist progressively larger
and larger infinite sets $\N$, $\sP(\N)$,
$\sP(\sP(\N))$, $\sP(\sP(\sP(\N)))$, etc\ldots.

\subsection{Exercises}

\begin{exercise}
Show
$A \setminus (B \cap C) = (A \setminus B) \cup (A \setminus C)$.
\end{exercise}

\begin{exercise}
Prove that the principle of strong induction is equivalent to the standard
induction.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{st:propinv}.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
\item
Prove \propref{st:propfor}.
\item
Find an example for which equality of sets
in 
$f( C \cap D) \subset f (C) \cap f (D)$
fails.  That is, find an $f$, $A$, $B$, $C$, and $D$ such that
$f( C \cap D)$ is a proper subset of $f(C) \cap f(D)$.
\end{enumerate}
\end{exercise}

\begin{exercise}[Tricky]
Prove that if $A$ is nonempty and finite, then there exists a unique
$n \in \N$ such
that there exists a bijection between $A$ and $\{ 1, 2, 3, \ldots, n \}$.
In other words, the notation $\abs{A} := n$ is justified.
Hint: Show that if $n > m$, then there is no injection from
$\{ 1, 2, 3, \ldots, n \}$ to
$\{ 1, 2, 3, \ldots, m \}$.
\end{exercise}


\begin{exercise}
Prove
\begin{enumerate}[a)]
\item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $A \Delta B$ denote the
\emph{\myindex{symmetric difference}}, that is, the set of all elements that
belong to either $A$ or $B$, but not to both $A$ and $B$.
\begin{enumerate}[a)]
\item
Draw a Venn diagram for
$A \Delta B$.
\item
Show $A \Delta B = (A \setminus B) \cup (B \setminus A)$.
\item
Show $A \Delta B = (A \cup B) \setminus ( A \cap B)$.
\end{enumerate}
\end{exercise}

\begin{exercise}
For each $n \in \N$, let $A_n := \{ (n+1)k : k \in \N \}$.
\begin{enumerate}[a)]
\item Find $A_1 \cap A_2$.
\item Find $\bigcup_{n=1}^\infty A_n$.
\item Find $\bigcap_{n=1}^\infty A_n$.
\end{enumerate}
\end{exercise}

\begin{samepage}
\begin{exercise}
Determine $\sP(S)$ (the power set) for each of the following:
\begin{enumerate}[a)]
\item $S = \emptyset$,
\item $S = \{1\}$,
\item $S = \{1,2\}$,
\item $S = \{1,2,3,4\}$.
\end{enumerate}
\end{exercise}
\end{samepage}


\begin{exercise}
Let $f \colon A \to B$ and $g \colon B \to C$ be functions.
\begin{enumerate}[a)]
\item
Prove that if $g \circ f$ is injective, then $f$ is injective.
\item
Prove that if $g \circ f$ is surjective, then $g$ is surjective.
\item
Find an explicit example where $g \circ f$ is bijective, but neither $f$
nor $g$ are bijective.
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove by induction that $n < 2^n$ for all $n \in \N$.
\end{exercise}

\begin{exercise}
Show that for a finite set $A$ of cardinality $n$, the cardinality
of $\sP(A)$ is $2^n$.
\end{exercise}

\begin{exercise}
Prove $\frac{1}{1\cdot 2} + 
\frac{1}{2\cdot 3} + \cdots + \frac{1}{n(n+1)} = \frac{n}{n+1}$
for all $n \in \N$.
\end{exercise}

\begin{exercise}
Prove $1^3 + 2^3 + \cdots + n^3 = {\left( \frac{n(n+1)}{2} \right)}^2$
for all $n \in \N$.
\end{exercise}

\begin{exercise}
Prove that $n^3 + 5n$ is divisible by $6$ for all $n \in \N$.
\end{exercise}

\begin{exercise}
Find the smallest $n \in \N$ such that $2{(n+5)}^2 < n^3$ and call it $n_0$.
Show that $2{(n+5)}^2 < n^3$ for all $n \geq n_0$.
\end{exercise}

\begin{exercise}
Find all $n \in \N$ such that $n^2 < 2^n$.
\end{exercise}

\begin{exercise}
Finish the proof that the \hyperref[induction:thm]{principle of induction}
is equivalent to the
\hyperlink{wop:link}{well ordering property} of $\N$.  That is,
prove the well ordering property for $\N$ using the principle of
induction.
\end{exercise}


\begin{exercise}
Give an example of a countable collection of finite sets $A_1, A_2, \ldots$,
whose union is not a finite set.
\end{exercise}

\begin{exercise}
Give an example of a countable collection of infinite sets $A_1, A_2, \ldots$,
with $A_j \cap A_k$ being infinite for all $j$ and $k$, such that
$\bigcap_{j=1}^\infty A_j$
is nonempty and finite.
\end{exercise}

\begin{exercise}
Suppose $A \subset B$ and $B$ is finite. Prove that $A$ is finite.
That is, if $A$ is nonempty, construct a bijection of $A$ to $\{ 1,2,\ldots,n \}$.
\end{exercise}

\begin{exercise}
a) Suppose $A \subset B$ and $B$ is countably infinite.  By constructing a
bijection, show that $A$ is
countable (that is, $A$ is empty, finite, or countably infinite).
\\
b)~Use part a) to show that if $\abs{A} < \abs{\N}$, then $A$ is finite.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:countsubsetbij}
Suppose $A \subset S$, and $A$ is countable.
Show that there exists a countable subset $B \subset S$ and
a bijection between $S \setminus B$ and $S$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Real Numbers} \label{rn:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic properties} \label{sec:basicpropsrn}

\sectionnotes{1.5 lectures}

The main object we work with in analysis is the set of
\myindex{real numbers}.  As this set is so fundamental, often much time is
spent on formally constructing the set of real numbers.  However, we 
take an easier approach here and just assume that a set with the correct
properties exists.  We need to start with the definitions of those
properties.

\begin{defn}
An \emph{\myindex{ordered set}} is a set $S$, together with
a relation $<$ such that
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
%\begin{enumerate}[(i),nolistsep]
\begin{enumerate}[(i)]
\item For any $x, y \in S$, exactly one of
$x < y$, $x=y$, or $y < x$ holds.
\item If $x < y$ and $y < z$, then $x < z$.
\end{enumerate}
We write $x \leq y$ if $x < y$ or $x=y$.  We define
$>$ and $\geq$ in the obvious way.
\end{defn}


The set of rational numbers $\Q$ is an ordered set by letting
$x < y$ if and only if $y-x$ is a positive rational number, that is
if $y-x = \nicefrac{p}{q}$ where $p,q \in \N$.  Similarly,
$\N$ and $\Z$ are also ordered sets.

There are other ordered sets than sets of numbers.  For example, the
set of countries can be ordered by landmass, so for example India $>$
Lichtenstein.  Any time you sort a set in some way, you are making an ordered
set.  A typical ordered set that you have used since primary school is the
dictionary.  It is the ordered set of words where the order is the
so-called lexicographic ordering.  Such ordered sets appear often for example in
computer science.  In this class we will mostly be interested in ordered
set of numbers however.

\begin{defn}
Let $E \subset S$, where $S$ is an ordered set.
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
%\begin{enumerate}[(i),nolistsep]
\begin{enumerate}[(i)]
\item If there exists a $b \in S$ such that $x \leq b$ for all $x \in E$,
then we say $E$ is \emph{\myindex{bounded above}} and $b$
is an \emph{\myindex{upper bound}} of $E$.
\item If there exists a $b \in S$ such that $x \geq b$ for all $x \in E$,
then we say $E$ is \emph{\myindex{bounded below}} and $b$
is a \emph{\myindex{lower bound}} of $E$.
\item If there exists an upper bound $b_0$ of $E$ such that whenever
$b$ is any upper bound for $E$ we have $b_0 \leq b$, then $b_0$
is called the \emph{\myindex{least upper bound}} or
the \emph{\myindex{supremum}}
of $E$.  We write
\begin{equation*}
\sup\, E := b_0  .
\end{equation*}
\item Similarly, if there exists a lower bound $b_0$ of $E$ such that whenever
$b$ is any lower bound for $E$ we have $b_0 \geq b$, then $b_0$
is called the \emph{\myindex{greatest lower bound}} or
the \emph{\myindex{infimum}}
of $E$.  We write
\begin{equation*}
\inf\, E := b_0  .
\end{equation*}
\end{enumerate}
When a set $E$ is both bounded above and bounded below, we say simply that
$E$ is \emph{bounded}\index{bounded set}.
\end{defn}

Let $S := \{ a, b, c, d, e \}$ be ordered as $a < b < c < d < e$, and
let $E := \{ a, c \}$.  Then $c$, $d$, or $e$ are upper bounds, and
$c$ is the least upper bound or supremum of $E$.

Notice that supremum (or infimum) is automatically unique (if it exists): If $b$ and
$b'$ are suprema of $E$, then $b \leq b'$ and $b' \leq b$, because both
$b$ and $b'$ are the least upper bounds, so $b=b'$.

A supremum or infimum for $E$ (even if they exist) need not be
in $E$.  For example, the set $E := \{ x \in \Q : x < 1 \}$ has a least upper
bound of 1, but 1 is not in the set $E$ itself.  On the other hand, if we
take $G := \{ x \in \Q : x \leq 1 \}$, then the least upper bound of $G$
is clearly also 1, and in this case $1 \in G$.  On the other hand,
the set $P := \{ x \in \Q : x \geq 0 \}$ has no upper bound (why?) and therefore
it cannot have a least upper bound.  On the other hand 0 is the greatest lower
bound of $P$.

\begin{defn} \label{defn:lub}
An ordered set $S$ has the \emph{\myindex{least-upper-bound property}} if
every nonempty
subset $E \subset S$ that is bounded above has a least upper bound,
that is $\sup\, E$ exists in $S$.
\end{defn}

The \emph{least-upper-bound property}
is sometimes called the \emph{\myindex{completeness property}} or the
\emph{\myindex{Dedekind completeness property}}%
\footnote{%
Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Richard_Dedekind}{Julius Wilhelm Richard Dedekind}
(1831 -- 1916).}.
As we will note in the
next section, the real numbers have this property.

\begin{example}
The set $\Q$ of rational numbers does not have the least-upper-bound property.  The subset
$\{ x \in \Q : x^2 < 2 \}$ does not have a supremum in $\Q$.  We will
see later that the supremum is 
$\sqrt{2}$, which is not rational%
\footnote{This is true for all other roots of 2, and interestingly,
the fact that $\sqrt[k]{2}$ is never rational for $k > 1$ implies no piano can
ever be perfectly tuned in all keys.  See for example:
\url{https://youtu.be/1Hqm0dYKUx4}.}.
Suppose $x \in \Q$ such that $x^2 = 2$.
Write $x=\nicefrac{m}{n}$ in lowest terms.  So ${(\nicefrac{m}{n})}^2 = 2$
or
$m^2 = 2n^2$.  Hence $m^2$ is divisible by 2 and so $m$ is divisible by
2.  Write $m = 2k$ and so ${(2k)}^2 = 2n^2$.  Divide by 2
and note that $2k^2 = n^2$, and hence $n$ is divisible by 2.  But that is a
contradiction as $\nicefrac{m}{n}$ was in lowest terms.
\end{example}

That $\Q$ does not have the least-upper-bound property is one of the most
important reasons why we work with $\R$ in analysis.  The set $\Q$
is just fine for algebraists.  But us analysts require the least-upper-bound
property to do any work.
We also require our real numbers to have many algebraic properties.  In
particular, we require that they are a field.


%\enlargethispage{\baselineskip}
\begin{defn}
A set $F$ is called a \emph{\myindex{field}} if it has two operations
defined on it, addition $x+y$ and multiplication $xy$, and if it satisfies
the following axioms:
\begin{enumerate}[({A}1)]
\item If $x \in F$ and $y \in F$, then $x+y \in F$.
\item \emph{(commutativity of addition)}
$x+y = y+x$ for all $x,y \in F$.
\item \emph{(associativity of addition)}
$(x+y)+z = x+(y+z)$ for all $x,y,z \in F$.
\item There exists an element $0 \in F$ such that
$0+x = x$ for all $x \in F$.
\item For every element $x\in F$ there exists an element $-x \in F$
such that $x + (-x) = 0$.
\end{enumerate}
\begin{enumerate}[({M}1)]
\item If $x \in F$ and $y \in F$, then $xy \in F$.
\item \emph{(commutativity of multiplication)}
$xy = yx$ for all $x,y \in F$.
\item \emph{(associativity of multiplication)}
$(xy)z = x(yz)$ for all $x,y,z \in F$.
\item There exists an element $1 \in F$ (and $1 \not= 0$) such that
$1x = x$ for all $x \in F$.
\item For every $x\in F$ such that $x \not= 0$ there exists an element
$\nicefrac{1}{x} \in F$
such that $x(\nicefrac{1}{x}) = 1$.
%\end{enumerate}
%\begin{enumerate}
\item[(D)] \emph{(distributive law)} $x(y+z) = xy+xz$
for all $x,y,z \in F$.
\end{enumerate}
\end{defn}

\begin{example}
The set $\Q$ of rational numbers is a field.  On the other hand $\Z$ is not a
field, as it does not contain multiplicative inverses.  For example,
there is no $x \in \Z$ such that $2x = 1$, so (M5) is not satisfied.  You
can check that (M5) is the only property that fails\footnote{An algebraist would say that $\Z$ is an ordered
ring, or perhaps more precisely a commutative ordered ring.}.
\end{example}

We will assume the basic facts about fields that are easily 
proved from the axioms.  For example, $0x = 0$ is easily proved
by noting that $xx = (0+x)x = 0x+xx$, using (A4), (D), and (M2).  Then
using (A5) on $xx$, along with (A2), (A3), and (A4), we obtain $0 = 0x$.

\begin{defn}
A field $F$ is said to be an \emph{\myindex{ordered field}} if
$F$ is also an ordered set such that:
\begin{enumerate}[(i)]
\item \label{defn:ordfield:i} For $x,y,z \in F$,  $x < y$ implies $x+z <
y+z$.
\item \label{defn:ordfield:ii} For $x,y \in F$, $x > 0$ and $y > 0$
implies $xy > 0$.
\end{enumerate}
If $x > 0$, we say $x$ is \emph{\myindex{positive}}.
If $x < 0$, we say $x$ is \emph{\myindex{negative}}.
We also say $x$ is \emph{\myindex{nonnegative}} if $x \geq 0$,
and $x$ is \emph{\myindex{nonpositive}} if $x \leq 0$.
\end{defn}

It can be checked that the rational numbers $\Q$ with the
standard ordering is an ordered field.

\begin{prop} \label{prop:bordfield}
Let $F$ be an ordered field and $x,y,z,w \in F$.  Then:
\begin{enumerate}[(i)]
\item \label{prop:bordfield:i} If $x > 0$, then $-x < 0$ (and vice-versa).
\item \label{prop:bordfield:ii} If $x > 0$ and $y < z$, then $xy < xz$.
\item \label{prop:bordfield:iii} If $x < 0$ and $y < z$, then $xy > xz$.
\item \label{prop:bordfield:iv} If $x \not= 0$, then $x^2 > 0$.
\item \label{prop:bordfield:v} If $0 < x < y$, then $0 < \nicefrac{1}{y} < \nicefrac{1}{x}$.
\item \label{prop:bordfield:vi} If $0 < x < y$, then $x^2 < y^2$.
\item \label{prop:bordfield:vii} If $x \leq y$ and $z \leq w$, then $x + z \leq y + w$.
\end{enumerate}
\end{prop}

Note that \ref{prop:bordfield:iv} implies in particular that $1 > 0$.

\begin{proof}
Let us prove \ref{prop:bordfield:i}.  The inequality $x > 0$ implies by item
\ref{defn:ordfield:i} of definition of ordered field that
$x + (-x) > 0 + (-x)$.  Now apply the algebraic properties of fields to
obtain $0 > -x$.  The ``vice-versa'' follows by similar calculation.

For \ref{prop:bordfield:ii}, first notice that $y < z$ implies
$0 < z - y$ by applying 
item \ref{defn:ordfield:i} of the definition of ordered fields.  
Now apply item 
\ref{defn:ordfield:ii} of the definition of ordered fields to obtain
$0 < x(z-y)$.  By algebraic properties we get $0 < xz - xy$,
and again applying item
\ref{defn:ordfield:i} of the definition we obtain $xy < xz$.

Part \ref{prop:bordfield:iii} is left as an exercise.

To prove part \ref{prop:bordfield:iv} first suppose $x > 0$.  Then
by item 
\ref{defn:ordfield:ii} of the definition of ordered fields we obtain
that $x^2 > 0$ (use $y=x$).  If $x < 0$, we use 
part \ref{prop:bordfield:iii} of this proposition.  Plug in $y=x$ and
$z=0$.

Finally to prove part \ref{prop:bordfield:v}, notice that
$\nicefrac{1}{x}$ cannot be equal to zero (why?).
Suppose $\nicefrac{1}{x} < 0$,
then $\nicefrac{-1}{x} > 0$ by \ref{prop:bordfield:i}.  Then apply
part \ref{prop:bordfield:ii} (as $x > 0$) to obtain
$x(\nicefrac{-1}{x}) > 0x$ or $-1 > 0$, which contradicts $1 > 0$ by using part
\ref{prop:bordfield:i} again.  Hence $\nicefrac{1}{x} > 0$.
Similarly $\nicefrac{1}{y} > 0$.  Thus $(\nicefrac{1}{x})(\nicefrac{1}{y}) > 0$
by definition of ordered field and by part \ref{prop:bordfield:ii}
\begin{equation*}
(\nicefrac{1}{x})(\nicefrac{1}{y})x < (\nicefrac{1}{x})(\nicefrac{1}{y})y .
\end{equation*}
By algebraic properties we get $\nicefrac{1}{y} < \nicefrac{1}{x}$.

Parts \ref{prop:bordfield:vi} and \ref{prop:bordfield:vii} are left as
exercises.
\end{proof}

Product of two positive numbers (elements of an ordered field) is positive.
However, it is not true that if the product is positive, then each of the two
factors must be positive.

\begin{prop}
Let $x,y \in F$ where $F$ is an ordered field.  Suppose 
$xy > 0$.  Then either both $x$ and $y$ are positive, or both are negative.
\end{prop}

\begin{proof}
Clearly both of the conclusions can happen.  If either
$x$ and $y$ are zero, then $xy$ is zero and hence not positive.
Hence we assume that $x$ and $y$ are nonzero,
and we simply need to show that if they have opposite signs, then
$xy < 0$.
Without loss of
generality suppose $x > 0$ and $y < 0$.  Multiply $y < 0$ by $x$ to get
$xy < 0x = 0$.  The result follows by contrapositive.
\end{proof}

\begin{example} \label{example:complexfield}
The reader may also know about the \emph{\myindex{complex numbers}},
usually denoted by
$\C$.  That is, $\C$ is the set of numbers of
the form $x + iy$, where $x$ and $y$ are real numbers, and $i$ is the
imaginary number, a number such that $i^2 = -1$.  The reader may
remember from algebra that $\C$ is also a field, however, it is not an
ordered field.  While one can make $\C$ into an ordered set in some way,
it can be proved that it is not possible to put an
order on $\C$ that will make it an ordered field.
\end{example}

Finally an ordered field that has the least-upper-bound property, has the
corresponding property for greatest lower bounds.

\begin{prop}
Let $F$ be an ordered field with the least-upper-bound property.
Let $A \subset F$ be a nonempty set that is bounded below.
Then $\inf\, A$ exists.
\end{prop}

\begin{proof}
Let $B := \{ -x : x \in A \}$. Let $b$ be a lower bound for $A$,
that is if $x \in A$, then $x \geq b$, or $-x \leq -b$.  So $-b$
is an upper bound for $B$, $c:=\sup\, B$ exists, and $c \leq -b$.
Since $y \leq c$
for all $y \in B$, then $-c \leq x$ for all $x \in A$.
So
$-c$ is a lower bound for $A$.  As $-c \geq b$,
$-c$ is the greatest lower bound of $A$.
\end{proof}

\subsection{Exercises}

\begin{exercise}
Prove part \ref{prop:bordfield:iii} of \propref{prop:bordfield}.
That is, let $F$ be an ordered field and $x,y,z \in F$.  Prove
If $x < 0$ and $y < z$, then $xy > xz$.
\end{exercise}


\begin{exercise} \label{exercise:finitesethasminmax}
Let $S$ be an ordered set.  Let $A \subset S$ be a nonempty finite subset.
Then $A$ is bounded.  Furthermore,
$\inf\, A$ exists and is in $A$ and 
$\sup\, A$ exists and is in $A$.  Hint: Use
\hyperref[induction:thm]{induction}.
\end{exercise}

\begin{exercise} \label{exercise:squareineq}
Prove part \ref{prop:bordfield:vi} of \propref{prop:bordfield}.
That is, let $x, y \in F$, where $F$ is an ordered field, such that
$0 < x < y$.  Show that $x^2 < y^2$.
\end{exercise}

\begin{exercise}
Let $S$ be an ordered set.  Let $B \subset S$ be bounded (above and
below).  Let $A \subset B$ be a nonempty subset.
Suppose all the $\inf$'s and
$\sup$'s exist. Show that
\begin{equation*}
\inf\, B \leq \inf\, A \leq \sup\, A \leq \sup\, B .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $S$ be an ordered set.  Let $A \subset S$ and suppose 
$b$ is an upper bound for $A$.  Suppose $b \in A$.  Show
that $b = \sup\, A$.
\end{exercise}

\begin{exercise}
Let $S$ be an ordered set.  Let $A \subset S$ be a nonempty subset
that is bounded above.
Suppose $\sup\, A$ exists and
$\sup\, A \notin A$.
Show that $A$ contains a countably infinite
subset.
In particular, $A$ is infinite.
\end{exercise}

\begin{exercise}
Find a (nonstandard) ordering of the set of natural numbers $\N$
such that there exists a nonempty proper subset $A \subsetneq \N$
and such that $\sup\, A$ exists in $\N$, but $\sup\, A \notin A$.
To keep things straight it might be a good idea to use a different
notation for the nonstandard ordering such as $n \prec m$.
\end{exercise}

\begin{exercise}
Let $F := \{ 0, 1, 2 \}$.  a)~Prove that there is
exactly one way to define addition and multiplication so that $F$ is
a field if $0$ and $1$ have their usual meaning of (A4) and (M4).
b)~Show that $F$ cannot be an ordered field.
\end{exercise}

\begin{exercise} \label{exercise:dominatingb}
Let $S$ be an ordered set and
$A$ is a nonempty subset such that $\sup \, A$ exists.  Suppose there
is a $B \subset A$ such that whenever $x \in A$ there is a $y \in B$
such that $x \leq y$.  Show that $\sup \, B$ exists and $\sup \, B = \sup \, A$.
\end{exercise}

\begin{exercise}
Let $D$ be the ordered set of all possible words (not just English words,
all strings of letters of arbitrary length)
using the Latin alphabet using only lower case letters.  The order is the
lexicographic order as in a dictionary (e.g.\ \emph{aa} $<$ \emph{aaa} $<$ \emph{dog} $<$ \emph{door}).
Let $A$ be the subset of $D$ containing the words whose
first letter is `a' (e.g.\ \emph{a} $\in A$, \emph{abcd} $\in A$).
Show that $A$ has a supremum and find what it is.
\end{exercise}

\begin{exercise}
Let $F$ be an ordered field and $x,y,z,w \in F$.
\\
a) Prove part \ref{prop:bordfield:vii} of \propref{prop:bordfield}.
That is,
if $x \leq y$ and $z \leq w$, then $x+z \leq y+w$.
\\
b) Prove that
if $x < y$ and $z \leq w$, then $x+z < y+w$.
\end{exercise}

\begin{exercise}
Prove that any ordered field must contain a countably infinite set.
\end{exercise}

\begin{exercise}
Let $\N_{\infty} := \N \cup \{ \infty \}$, where elements of $\N$ are
ordered in the usual way amongst themselves,
and $k < \infty$ for every $k \in \N$.  Show $\N_{\infty}$ is an ordered set
and that every subset $E \subset \N_{\infty}$ has a supremum in
$\N_{\infty}$ (make sure to also handle the case of an empty set).
\end{exercise}

\begin{exercise}
Let $S := \{ A_k : k \in \N \} \cup \{ B_k : k \in \N \}$, ordered such that
$A_k < B_j$ for any $k$ and $j$,
$A_k < A_m$ whenever $k < m$,
and $B_k > B_m$ whenever $k < m$.\\
a) Show that $S$ is an ordered set.\\
b) Show that any subset of $S$ is bounded (both above and below).\\
c) Find a bounded subset of $S$ which has no least upper bound.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{The set of real numbers} \label{sec:setofreals}

\sectionnotes{2 lectures, the extended real numbers are optional}

\subsection{The set of real numbers}

We finally get to the real number system.  To simplify matters, instead of constructing the
real number set from the rational numbers, we simply state their existence
as a theorem without proof.  Notice that $\Q$ is an ordered
field.

\begin{thm}
There exists a unique\footnote{Uniqueness is up to isomorphism, but we wish
to avoid excessive use of algebra.  For us, it is simply enough to assume
that a set of real numbers exists.  See Rudin~\cite{Rudin:baby} for the
construction and more details.}
ordered field $\R$ with the \hyperref[defn:lub]{least-upper-bound property}
such that $\Q \subset \R$.
\end{thm}

Note that also $\N \subset \Q$.  We saw that $1 > 0$.  By
\hyperref[induction:thm]{induction}
(exercise) we can prove that
$n > 0$ for all $n \in \N$.
Similarly we verify simple statements about rational numbers.
For example, we proved that if $n > 0$, then $\nicefrac{1}{n} > 0$.
Then $m < k$ implies $\nicefrac{m}{n} < \nicefrac{k}{n}$.
%Similarly we can
%verify all the statements we know about rational numbers and their
%natural ordering.

Let us prove one of the most basic but useful results about the real numbers.
The following proposition is essentially how an analyst proves an
inequality.

\begin{prop}
If $x \in \R$ is such that $x \leq \epsilon$ for all
$\epsilon \in \R$ where
$\epsilon > 0$, then $x \leq 0$.
\end{prop}

\begin{proof}
%If $x > 0$, then taking
%$\epsilon = x$ we get the contradiction $x < x$.  Therefore $x=0$.
If $x > 0$, then $0 < \nicefrac{x}{2} < x$ (why?).  Taking
$\epsilon = \nicefrac{x}{2}$ obtains a contradiction.  Thus $x \leq 0$.
\end{proof}

Another useful version of this idea is the following equivalent
statement for nonnegative numbers: \emph{If $x \geq 0$ is such that $x \leq \epsilon$ for all
$\epsilon > 0$, then $x = 0$.}  From now on, when we say
$x \geq 0$ or
$\epsilon > 0$, we automatically mean that $x \in \R$ and $\epsilon \in \R$.
And to prove that $x \geq 0$ in the first place, an analyst might prove
that all $x \geq -\epsilon$ for all $\epsilon > 0$.

A related simple fact is that 
any time we have two real numbers $a < b$, then there is another
real number $c$ such that
$a < c < b$.  Just take for example $c = \frac{a+b}{2}$ (why?).  In fact,
there are infinitely many real numbers between $a$ and $b$.

The most useful property of $\R$ for analysts
is not just that it is an ordered field, but that it has the
\hyperref[defn:lub]{least-upper-bound property}.  Essentially we want $\Q$, but we also
want to take suprema (and infima) willy-nilly.  So what we do is 
take $\Q$ and
throw in enough numbers to obtain $\R$.



We mentioned already that $\R$ must contain elements that are not in $\Q$
because of the \hyperref[defn:lub]{least-upper-bound property}.
We saw there is no
rational square root of two.  The set
$\{ x \in \Q : x^2 < 2 \}$ implies the existence of the real number
$\sqrt{2}$, although this fact requires a bit of work.  See also
\exerciseref{exercise:sqrt2QorR}.

\begin{example} \label{example:sqrt2}
Claim: There exists a unique positive
real number $r$ such that $r^2 = 2$.  We denote $r$ by $\sqrt{2}$.

\begin{proof}
Take the set
$A := \{ x \in \R : x^2 < 2 \}$.  First if $x^2 < 2$,
then $x < 2$.  To see this fact, note that $x \geq 2$ implies $x^2 \geq 4$
%(use \propref{prop:bordfield}, we will not explicitly
%mention its use from now on),
(see \exerciseref{exercise:squareineq}),
hence any number $x$ such that $x \geq 2$
is not in $A$.  Thus $A$ is bounded above.
On the other hand, $1 \in A$, so $A$ is nonempty.

Let us define $r := \sup\, A$.  We will show that $r^2 = 2$ by showing
that $r^2 \geq 2$ and $r^2 \leq 2$.  This is the way analysts show
equality, by showing two inequalities.
We already know that $r \geq 1 > 0$.

%\medskip
In the following, it may seem we are pulling certain expressions out of
a hat.  When writing a proof such as this we would, of course, come up with
the expressions only after playing around with what we wish to prove.  The
order in which we write the proof is not necessarily the order in which we
come up with the proof.

Let us first show that $r^2 \geq 2$.
Take a positive number $s$ such that $s^2 < 2$.  We wish to find an $h > 0$
such that ${(s+h)}^2 < 2$.
As $2-s^2 > 0$, we have $\frac{2-s^2}{2s+1} > 0$.
We choose an $h \in \R$ such that
$0 < h < \frac{2-s^2}{2s+1}$.
Furthermore, we assume $h < 1$.

\begin{equation*}
\begin{aligned}
{(s+h)}^2 - s^2 & = h(2s + h) \\
 & < h(2s+1) & & \quad \bigl(\text{since } h < 1\bigr) \\
 & < 2-s^2 & & \quad \bigl(\text{since } h < \tfrac{2-s^2}{2s+1} \bigr) .
\end{aligned}
\end{equation*}
Therefore, ${(s+h)}^2 < 2$.  Hence $s+h \in A$, but as $h > 0$
we have $s+h > s$.  So $s < r = \sup\, A$.  As $s$ was an arbitrary
positive number such that $s^2 < 2$, it follows that $r^2 \geq 2$.

%\medskip

Now take a positive number $s$ such that
$s^2 > 2$.  We wish to find an $h > 0$ such that
${(s-h)}^2 > 2$.
As 
$s^2-2 > 0$ we have $\frac{s^2-2}{2s} > 0$.
Let $h := \frac{s^2-2}{2s}$.
\begin{equation*}
\begin{aligned}
s^2 - {(s-h)}^2 & = 2sh - h^2 \\
 & < 2sh & & \quad \bigl( \text{since $h > 0$ so $h^2 > 0$} \bigr)  \\
 & \leq s^2-2 & & \quad \bigl( \text{since } h = \tfrac{s^2-2}{2s} \bigr) .
\end{aligned}
\end{equation*}
By subtracting $s^2$ from both sides and multiplying by $-1$, we find
${(s-h)}^2 > 2$.  Therefore $s-h \notin A$.

Furthermore, if $x \geq s-h$,
then $x^2 \geq {(s-h)}^2 > 2$ (as $x > 0$ and $s-h > 0$) and so $x \notin A$.
Thus
$s-h$ is an upper bound for $A$.  However, $s-h < s$,  or in other
words $s > r = \sup\, A$.  Thus $r^2 \leq 2$.

\medskip

Together, $r^2 \geq 2$ and $r^2 \leq 2$ imply
$r^2 = 2$.  The existence part is finished.  We still need to
handle uniqueness.  Suppose $s \in \R$ such that $s^2 = 2$ and $s > 0$.
Thus $s^2 = r^2$.  However, if $0 < s < r$, then $s^2 < r^2$.  Similarly
$0 < r < s$ implies $r^2 < s^2$.  Hence $s = r$.
\end{proof}
\end{example}

The number $\sqrt{2} \notin \Q$.  The set
$\R \setminus \Q$ is called the set of
\emph{\myindex{irrational}} numbers.  We just saw that $\R \setminus \Q$
is nonempty.  Not only is it nonempty, we will see later that is it very
large indeed.

Using the same technique as above, we can show that a positive real
number $x^{1/n}$ exists for all $n\in \N$ and all $x > 0$.
That is, for each $x > 0$,
there exists a unique positive real number $r$ such that $r^n = x$.
The proof is left as an exercise.

\subsection{Archimedean property}

As we have seen, there are plenty of real numbers in any interval.  But
there are also infinitely many rational numbers in any interval.  The
following is one of the fundamental facts about the real numbers.
The two parts of the next theorem are actually equivalent, even though it may
not seem like that at first sight.

\begin{thm} \label{thm:arch}
{\ }
\begin{enumerate}[(i)]
\item \label{thm:arch:i} \emph{(\myindex{Archimedean property})}%
\footnote{This property is Axiom V from Archimedes' ``On the Sphere and Cylinder'' 225 BC.}
If $x, y \in \R$ and
$x > 0$, then there exists an $n \in \N$ such that
\begin{equation*}
nx > y .
\end{equation*}
\item \label{thm:arch:ii} \emph{($\Q$ is dense in $\R$\index{density of
rational numbers})} If $x, y \in \R$ and
$x < y$, then there exists an $r \in \Q$ such that
$x < r < y$.
\end{enumerate}
\end{thm}

\begin{proof}
Let us prove \ref{thm:arch:i}.  Divide through by $x$ and then 
\ref{thm:arch:i} says that for any real number $t:= \nicefrac{y}{x}$,
we can find natural number $n$ such that $n > t$.  In other words,
\ref{thm:arch:i} says that $\N \subset \R$ is not bounded above.
Suppose for contradiction that $\N$ is bounded above.  Let $b := \sup \N$.
The number $b-1$ cannot possibly be an upper bound for $\N$ as it is strictly
less than $b$ (the supremum).  Thus there exists an $m \in \N$ such that $m > b-1$.
Add one to obtain $m+1 > b$, contradicting $b$ being an
upper bound.

\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figdensofQ.pdf_t}
\caption{Idea of the proof of the density of $\Q$: Find $n$ such that $y-x >
\nicefrac{1}{n}$, then take the least $m$ such that $x >
\nicefrac{m}{n}$.\label{figdensofQ}}
%\end{center}
\end{myfigureht}
Let us tackle \ref{thm:arch:ii}.
See \figureref{figdensofQ}
for a picture of the idea behind the proof.
First assume $x \geq 0$.
Note that $y-x > 0$.
By \ref{thm:arch:i}, there exists an $n \in \N$ such that
\begin{equation*}
n(y-x) > 1
\qquad \text{or} \qquad
y-x > \nicefrac{1}{n}.
\end{equation*}
Again by \ref{thm:arch:i} the set 
$A := \{ k \in \N : k > nx \}$ is nonempty.  By the
\hyperlink{wop:link}{well ordering property}
of $\N$, $A$ has a least element $m$, and.  As $m \in A$,
then $m > nx$.
Divide through by $n$ to get $x < \nicefrac{m}{n}$.
As $m$ is the least
element of $A$, $m-1 \notin A$.
If $m > 1$, then $m-1 \in \N$, but $m-1 \notin A$ and so $m-1 \leq nx$.
If $m=1$,
then $m-1 = 0$, and $m-1 \leq nx$ still holds as $x \geq 0$.
In other words,
\begin{equation*}
m-1 \leq nx \qquad \text{or} \qquad m \leq nx+1 . % < m .
\end{equation*}
On the other hand
from $n(y-x) > 1$ we obtain $ny > 1+nx$.
%  As $nx \geq m-1$ we get
%that $1+nx \geq m$ and
Hence $ny > 1+nx \geq m$, and therefore $y > \nicefrac{m}{n}$.
Putting everything together we obtain $x < \nicefrac{m}{n} < y$.
So let $r = \nicefrac{m}{n}$.

Now assume $x < 0$.  If $y > 0$, then just take $r=0$.  If
$y \leq 0$, then $0 \leq -y < -x$, and we
find a rational $q$ such that $-y < q < -x$.  Then take $r = -q$.
\end{proof}

Let us state and prove a simple but useful corollary of the
\hyperref[thm:arch:i]{Archimedean property}.

\begin{cor}
$\inf \{ \nicefrac{1}{n} : n \in \N \} = 0$.
\end{cor}

\begin{proof}
Let $A := \{ \nicefrac{1}{n} : n \in \N \}$.  Obviously $A$ is not empty.
Furthermore,
$\nicefrac{1}{n} > 0$ and so 0 is a lower bound, and $b := \inf\, A$ exists.
As 0 is a lower bound, then $b \geq 0$.
Now take an arbitrary $a > 0$.  By the
\hyperref[thm:arch:i]{Archimedean property} there exists an $n$ such that
$na > 1$, or in other words $a > \nicefrac{1}{n} \in A$.  Therefore
$a$ cannot be a lower bound for $A$.  Hence $b=0$.
\end{proof}

\subsection{Using supremum and infimum}

Suprema and infima are
compatible with algebraic operations.  For a set $A \subset \R$ and 
$x \in \R$ define
\begin{align*}
x + A & := \{ x+y \in \R : y \in A \} , \\
xA & := \{ xy \in \R : y \in A \} .
\end{align*}

\begin{prop} \label{prop:supinfalg}
Let $A \subset \R$ be nonempty.
\begin{enumerate}[(i)]
\item If $x \in \R$ and $A$ is bounded above, then $\sup (x+A) = x + \sup\, A$.
\item If $x \in \R$ and $A$ is bounded below, then $\inf (x+A) = x + \inf\, A$.
\item If $x > 0$ and $A$ is bounded above, then $\sup (xA) = x ( \sup\, A )$.
\item If $x > 0$ and $A$ is bounded below, then $\inf (xA) = x ( \inf\, A )$.
\item If $x < 0$ and $A$ is bounded below, then $\sup (xA) = x ( \inf\, A )$.
\item If $x < 0$ and $A$ is bounded above, then $\inf (xA) = x ( \sup\, A )$.
\end{enumerate}
\end{prop}

Do note that multiplying a set by a negative number switches supremum for an
infimum and vice-versa.  Also, as the proposition implies that
supremum (resp.\ infimum) of $x+A$ or $xA$ exists, 
it also implies that $x+A$ or $xA$
is nonempty and bounded from above (resp.\ from below).

\begin{proof}
Let us only prove the first statement.  The rest are left as exercises.

Suppose $b$ is an upper bound for $A$.  That is, $y \leq b$ for all $y \in A$.
Then $x+y \leq x+b$ for all $y \in A$, and so $x+b$ is an upper
bound for $x+A$.  In particular, if $b = \sup\, A$, then
\begin{equation*}
\sup (x+A) \leq x+b = x+ \sup\, A .
\end{equation*}

The other direction is similar.  If $b$ is an upper bound for $x+A$,
then $x+y \leq b$
for all $y \in A$ and so $y \leq b-x$ for all $y \in A$.
So $b-x$ is an upper bound for $A$.  If $b =
\sup (x+A)$, then 
\begin{equation*}
\sup\, A \leq b-x = \sup (x+A) -x .
\end{equation*}
And the result follows.
\end{proof}

Sometimes we need to apply supremum or infimum twice.  Here is an example.

\begin{prop} \label{infsupineq:prop}
Let $A, B \subset \R$ be nonempty sets such that $x \leq y$ whenever $x \in A$ and
$y \in B$.  Then $A$ is bounded above, $B$ is bounded below, and $\sup\, A \leq \inf\, B$.
\end{prop}

\begin{proof}
Any $x \in A$ is a lower bound for $B$.  Therefore
$x \leq \inf\, B$ for all $x \in A$, so $\inf\, B$ is an upper bound for
$A$.
Hence,
$\sup\, A \leq \inf\, B$.
\end{proof}

We must be careful about strict inequalities and taking suprema and
infima.  Note that
$x < y$ whenever $x \in A$ and
$y \in B$ still only implies $\sup\, A \leq \inf\, B$, and not a strict
inequality.  This is an important subtle point that comes up often.
For example, take $A := \{ 0 \}$ and take $B := \{ \nicefrac{1}{n}
: n \in \N \}$.
Then $0 < \nicefrac{1}{n}$
for all $n \in \N$.  However, $\sup\, A = 0$ and $\inf\, B = 0$.

%\medskip

The proof of the following
often used elementary fact is left to the reader.
A similar statement holds for infima.

\begin{prop} \label{prop:existsxepsfromsup}
If $S \subset \R$ is a nonempty set, bounded from above,
then for every $\epsilon > 0$ there exists $x \in S$ such
that $(\sup\, S) - \epsilon < x \leq \sup\, S$.
\end{prop}


To make using suprema and infima even easier, we may want to
write $\sup\, A$ and $\inf\, A$ without worrying about $A$ being
bounded and nonempty.  We make the following natural definitions.

\begin{defn}
Let $A \subset \R$ be a set.
\begin{enumerate}[(i)]
\item If $A$ is empty, then $\sup\, A := -\infty$.
\item If $A$ is not bounded above, then $\sup\, A := \infty$.
\item If $A$ is empty, then $\inf\, A := \infty$.
\item If $A$ is not bounded below, then $\inf\, A := -\infty$.
\end{enumerate}
\end{defn}

For convenience,  $\infty$ and $-\infty$ are sometimes treated as if they were
numbers, except we do not allow arbitrary arithmetic with them.
We make $\R^* := \R \cup \{ -\infty , \infty\}$ into an ordered set
by letting
\begin{equation*}
-\infty < \infty \quad \text{and} \quad
-\infty < x \quad \text{and} \quad
x < \infty \quad \text{for all $x \in \R$}.
\end{equation*}
The set $\R^*$ is called the set of \emph{\myindex{extended real numbers}}.
It is possible to define some arithmetic on $\R^*$.  Most operations
are extended in an obvious way, but we must leave
$\infty-\infty$, $0 \cdot (\pm\infty)$, and $\frac{\pm\infty}{\pm\infty}$
undefined.
We refrain from
using this arithmetic,
it leads to easy mistakes as $\R^*$ is not a field.
Now we can take suprema and infima without fear of emptiness or
unboundedness.  In this book we mostly avoid
using $\R^*$ outside of exercises, and leave such generalizations to the interested reader.

\subsection{Maxima and minima}

By \exerciseref{exercise:finitesethasminmax} we know 
a finite set of numbers always has a supremum or an infimum that is contained
in the set itself.  In this case we usually do not use the words
supremum or infimum.

When a set $A$ of real numbers is bounded above,
such that 
$\sup\, A \in A$, then we can use the word
\emph{\myindex{maximum}} and the notation $\max A$ to denote the supremum.
Similarly for infimum; when a set $A$ is bounded below
and $\inf\, A \in A$, then we can use the
word \emph{\myindex{minimum}} and the notation $\min\, A$.  For example,
\begin{align*}
& \max \{ 1,2.4,\pi,100 \} = 100 , \\
& \min \{ 1,2.4,\pi,100 \} = 1 .
\end{align*}
While writing $\sup$ and $\inf$ may be technically
correct in this situation, $\max$ and
$\min$ are generally used to emphasize that the supremum or infimum
is in the set itself.

\subsection{Exercises}

\begin{exercise}
Prove that
if $t > 0$ ($t \in \R$), then there exists an $n \in \N$ such that
$\dfrac{1}{n^2} < t$.
\end{exercise}

\begin{exercise}
Prove that
if $t \geq 0$ ($t \in \R$), then there exists an $n \in \N$ such that $n-1 \leq t < n$.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:supinfalg}.
\end{exercise}

\begin{exercise}
Let $x, y \in \R$.  Suppose $x^2 + y^2 = 0$.  Prove that 
$x = 0$ and $y = 0$.
\end{exercise}

\begin{exercise}
Show that $\sqrt{3}$ is irrational.
\end{exercise}

\begin{exercise}
Let $n \in \N$.
Show that either $\sqrt{n}$ is either an integer or it is
irrational.
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{arithmetic-geometric mean inequality}}.  That is, 
for two positive real numbers $x,y$ we have
\begin{equation*}
\sqrt{xy} \leq \frac{x+y}{2} .
\end{equation*}
Furthermore, equality occurs if and only if $x=y$.
\end{exercise}

\begin{exercise}
Show that for any two real numbers $x$ and $y$ such that $x < y$, there
exists an irrational number $s$ such that $x < s < y$.  Hint:
Apply the density of $\Q$ to $\dfrac{x}{\sqrt{2}}$ and
$\dfrac{y}{\sqrt{2}}$.
\end{exercise}

\begin{exercise} \label{exercise:supofsum}
Let $A$ and $B$ be two nonempty bounded sets of real numbers.  Let
$C := \{ a+b : a \in A, b \in B \}$.
Show that $C$ is a bounded set and that
\begin{equation*}
\sup\,C = \sup\,A + \sup\,B 
\qquad \text{and} \qquad
\inf\,C = \inf\,A + \inf\,B .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be two nonempty bounded sets of nonnegative real numbers.
Define the set
$C := \{ ab : a \in A, b \in B \}$.
Show that $C$ is a bounded set and that
\begin{equation*}
\sup\,C = (\sup\,A )( \sup\,B) 
\qquad \text{and} \qquad
\inf\,C = (\inf\,A )( \inf\,B).
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Given $x > 0$ and $n \in \N$, show that there exists a unique positive
real number $r$ such that $x = r^n$.  Usually $r$ is denoted by $x^{1/n}$.
\end{exercise}

\begin{exercise}[Easy]
Prove \propref{prop:existsxepsfromsup}.
\end{exercise}

\begin{exercise} \label{exercise:bernoulliineq}
Prove the so-called \emph{\myindex{Bernoulli's inequality}}%
\footnote{%
Named after the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Jacob_Bernoulli}{Jacob Bernoulli} (1655 --
1705).}%
: If $1+x > 0$ then
for all $n \in \N$ we have $(1+x)^n \geq 1+nx$.
\end{exercise}

\begin{exercise} \label{exercise:sqrt2QorR}
Prove $\sup \{ x \in \Q : x^2 < 2 \} = \sup \{ x \in \R : x^2 < 2 \}$.
\end{exercise}

\begin{exercise} \label{exercise:Dedekind}
a) Prove that given any $y \in \R$, we have $\sup \{ x \in \Q : x < y \} = y$.
\\
b) Let $A \subset \Q$ be a set that is bounded above such that whenever $x
\in A$ and $t \in \Q$ with $t < x$, then $t \in A$.  Further suppose that
$\sup\, A \not\in A$.  Show that there exists a $y \in \R$ such that
$A = \{ x \in \Q : x < y \}$.  A set such as $A$ is called a
\emph{\myindex{Dedekind cut}}.
\\
c) Show that there is a bijection between $\R$ and Dedekind cuts.
\\
Note: Dedekind used sets of the form from part b) in his construction of the
real numbers.
\end{exercise}

\begin{exercise}
Prove that if $A \subset \Z$ is a nonempty subset bounded from below, then
there exists a least element in $A$.  
Now describe why this statement would simplify the proof of
\thmref{thm:arch} part \ref{thm:arch:ii} so that you do not have to assume
$x \geq 0$.
\end{exercise}

\begin{exercise} \label{exercise:realpower}
Let us suppose we know $x^{1/n}$ exists for every $x > 0$ and every $n \in
\N$ (see exercise above).  For integers $p$ and $q > 0$, define $x^{p/q} :=
{(x^{1/q})}^p$.\\
a) Show that the power is well defined, that is if $\nicefrac{p}{q} =
\nicefrac{m}{k}$ where $m$ and $k > 0$ are integers, then 
${(x^{1/q})}^p = {(x^{1/m})}^k$.
\\
b)
Let $x$ and $y$ be two positive numbers and $r$ a rational number.
Assuming $r > 0$, show
$x < y$ if and only if $x^r < y^r$.  Then suppose $r < 0$ and show:
$x < y$ if and only if $x^r > y^r$.
\\
c) 
Suppose $x > 1$ and $r,s$ are rational where $r < s$.
Show $x^r < x^s$.  If $0 < x < 1$ and $r < s$, show that $x^r > x^s$.
Hint: Write $r$ and $s$ with the same denominator.
\\
d)%
\footnote{In
\sectionref{sec:logandexp}
we will define exponential and the logarithm and
define $x^z := \exp(z \ln x)$.  We will then have 
sufficient machinery to make proofs of these assertions far easier.  At this
point however, we do not yet have these tools.}
(Challenging)
For an irrational $z \in \R \setminus \Q$ and $x > 1$ define
$x^z := \sup \{ x^r : r \leq z, r \in \Q \}$,
for $x=1$ define $1^z = 1$,
and for $0 < x < 1$ define
$x^z := \inf \{ x^r : r \leq z, r \in \Q \}$.
Prove the two assertions of part b) for all real $z$.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Absolute value and bounded functions} \label{sec:absval}

\sectionnotes{0.5--1 lecture}

A concept we will encounter over and over is the concept of
\emph{\myindex{absolute value}}.
You want to think of the absolute value as the ``size'' of a real number.
Let us give a formal definition.
\begin{equation*}
\abs{x} :=
\begin{cases}
x & \text{ if $x \geq 0$}, \\
-x & \text{ if $x < 0$} .
\end{cases}
\end{equation*}

Let us give the main features of the absolute
value as a proposition.

\begin{prop} \label{prop:absbas}
{\ }
\begin{enumerate}[(i)]
\item \label{prop:absbas:i} $\abs{x} \geq 0$, and $\abs{x}=0$ if and only if $x = 0$.
\item \label{prop:absbas:ii} $\abs{-x} = \abs{x}$ for all $x \in \R$.
\item \label{prop:absbas:iii} $\abs{xy} = \abs{x}\abs{y}$ for all $x,y \in \R$.
\item \label{prop:absbas:iv} $\abs{x}^2 = x^2$ for all $x \in \R$.
\item \label{prop:absbas:v} $\abs{x} \leq y$ if and only if $-y \leq x \leq y$.
\item \label{prop:absbas:vi} $-\abs{x} \leq x \leq \abs{x}$ for all $x \in \R$.
\end{enumerate}
\end{prop}

\begin{proof}
\ref{prop:absbas:i}:
If $x \geq 0$, then $\abs{x} = x \geq 0$.  Also $\abs{x} = x = 0$ if and only
if $x=0$.
If $x < 0$, then $\abs{x} = -x > 0$, which is never zero.

\medskip

\ref{prop:absbas:ii}: Suppose $x > 0$, then $\abs{-x} = -(-x) = x =
\abs{x}$.  Similarly when $x < 0$, or $x = 0$.

\medskip

\ref{prop:absbas:iii}:
If $x$ or $y$ is zero, then the result is immediate.  When $x$ and
$y$ are both positive, then $\abs{x}\abs{y} = xy$.  $xy$ is also positive
and hence $xy = \abs{xy}$.  If $x$ and $y$ are both negative
then $xy$ is still positive and $xy = \abs{xy}$, and
$\abs{x}\abs{y} = (-x)(-y) = xy$.
Next assume
$x > 0$ and $y < 0$.  Then $\abs{x}\abs{y} = x(-y) = -(xy)$.  Now
$xy$ is negative and hence $\abs{xy} = -(xy)$.  Similarly if
$x < 0$ and $y > 0$.

\medskip

\ref{prop:absbas:iv}:
Immediate if $x \geq 0$.  If $x < 0$, then $\abs{x}^2 = {(-x)}^2 =
x^2$.

\medskip

\ref{prop:absbas:v}:  Suppose $\abs{x} \leq y$.  If $x \geq 0$,
then $x \leq y$.  It follows that $y \geq 0$, leading to $-y \leq 0 \leq x$.  So $-y \leq x \leq y$
holds.  If $x < 0$, then $\abs{x} \leq y$ means $-x \leq y$.  Negating both
sides we get $x \geq -y$.  Again $y \geq 0$ and so $y \geq 0 > x$.
Hence, $-y \leq x \leq y$.

On the other hand, suppose 
$-y \leq x \leq y$ is true.  If $x \geq 0$, then $x \leq y$ is equivalent
to $\abs{x} \leq y$.  If $x < 0$, then $-y \leq x$ implies
$(-x) \leq y$, which is equivalent to $\abs{x} \leq y$.

\medskip

\ref{prop:absbas:vi}:  Apply \ref{prop:absbas:v} with $y = \abs{x}$.
\end{proof}

A property used frequently enough to give it a name is the so-called
\emph{\myindex{triangle inequality}}.

\begin{prop}[Triangle Inequality]
$\abs{x+y} \leq \abs{x}+\abs{y}$
for all $x, y \in \R$.
\end{prop}

\begin{proof}
\propref{prop:absbas} gives
$- \abs{x} \leq x \leq \abs{x}$ and
$- \abs{y} \leq y \leq \abs{y}$.  Add these two inequalities to obtain
\begin{equation*}
- (\abs{x}+\abs{y}) \leq x+y \leq \abs{x}+ \abs{y} .
\end{equation*}
Apply \propref{prop:absbas} again to find
$\abs{x+y} \leq \abs{x}+\abs{y}$.
\end{proof}

There are other often applied versions of the triangle inequality.

\begin{cor}
Let $x,y \in \R$
\begin{enumerate}[(i)]
\item \emph{(\myindex{reverse triangle inequality})}
~
$\bigl\lvert (\abs{x}-\abs{y}) \bigr\rvert \leq \abs{x-y}$.
\item $\abs{x-y} \leq \abs{x}+\abs{y}$.
\end{enumerate}
\end{cor}

\begin{proof}
Let us plug in $x=a-b$ and $y=b$ into the standard
triangle inequality to obtain
\begin{equation*}
\abs{a} = \abs{a-b+b} \leq \abs{a-b} + \abs{b} ,
\end{equation*}
or $\abs{a}-\abs{b} \leq \abs{a-b}$.  Switching the roles of $a$ and $b$
we find 
$\abs{b}-\abs{a} \leq \abs{b-a} = \abs{a-b}$.  Applying
\propref{prop:absbas} again we obtain the reverse triangle
inequality.

The second version of the triangle inequality is obtained from the standard
one by just replacing $y$ with $-y$, and noting that $\abs{-y} =
\abs{y}$.
\end{proof}

\begin{cor}
Let $x_1, x_2, \ldots, x_n \in \R$.  Then
\begin{equation*}
\abs{x_1 + x_2 + \cdots + x_n} \leq 
\abs{x_1} + \abs{x_2} + \cdots + \abs{x_n} .
\end{equation*}
\end{cor}

\begin{proof}
We proceed by \hyperref[induction:thm]{induction}.
The conclusion holds trivially for $n=1$, and
for $n=2$ it is the standard triangle inequality.  Suppose the corollary
holds for $n$.  Take $n+1$ numbers $x_1,x_2,\ldots,x_{n+1}$ and 
first use the standard triangle inequality, then the induction
hypothesis
\begin{equation*}
\begin{split}
\sabs{x_1 + x_2 + \cdots + x_n + x_{n+1}} & \leq 
\sabs{x_1 + x_2 + \cdots + x_n} + \sabs{x_{n+1}} \\
& \leq 
\sabs{x_1} + \sabs{x_2} + \cdots + \sabs{x_n} + \sabs{x_{n+1}} .  \qedhere
\end{split}
\end{equation*}
\end{proof}

Let us see an example of the use of the triangle inequality.

\begin{example}
Find a number $M$ such that $\sabs{x^2-9x+1} \leq M$ for all $-1 \leq x \leq
5$.

Using the triangle inequality, write
\begin{equation*}
\sabs{x^2-9x+1} \leq \sabs{x^2}+\sabs{9x}+\sabs{1}
=
\sabs{x}^2+9\sabs{x}+1 .
\end{equation*}
The expression
$\sabs{x}^2+9\sabs{x}+1$ is largest when $\abs{x}$ is largest (why?).  In the interval
provided, $\abs{x}$ is largest when $x=5$ and so $\abs{x}=5$.  One
possibility for $M$ is
\begin{equation*}
M = 5^2+9(5)+1 = 71 .
\end{equation*}
There are, of course, other $M$ that work.  The bound of 71
is much higher than it
need be, but we didn't ask for the best possible $M$, just one that works.
\end{example}

The last example leads us to the concept of bounded functions.

\begin{defn}
Suppose $f \colon D \to \R$ is a function.  We say $f$ is
\emph{bounded}\index{bounded function}
if there exists a number $M$
such that $\abs{f(x)} \leq M$ for all $x \in D$.
\end{defn}

In the example we proved $x^2-9x+1$ is bounded when considered as a
function on $D = \{ x : -1 \leq x \leq 5 \}$.   On the other hand,
if we consider the same polynomial as a function on the whole real line $\R$,
then it is not bounded.

\begin{myfigureht}
%\begin{center}
% to get the fonts right input eepic
\subimport*{figures/}{boundedfunc.eepic}
\caption{Example of a bounded function, a bound $M$, and its supremum and infimum.\label{boundedfuncfig}}
%\end{center}
\end{myfigureht}
For a function $f \colon D \to \R$ we write (see \figureref{boundedfuncfig} for
an example)
\begin{align*}
& \sup_{x \in D} f(x) := \sup\, f(D) , \\
& \inf_{x \in D} f(x) := \inf\, f(D) .
\end{align*}
We also sometimes replace the ``$x \in D$'' with an expression.
For example if, as before, $f(x) = x^2-9x+1$, for $-1 \leq x \leq 5$, 
a little bit of calculus shows
\begin{equation*}
\sup_{x \in D} f(x) = 
\sup_{-1 \leq x \leq 5} ( x^2 -9x+1 ) = 11,
\qquad
\inf_{x \in D} f(x) = 
\inf_{-1 \leq x \leq 5} ( x^2 -9x+1 ) = \nicefrac{-77}{4} .
\end{equation*}



%To illustrate some common issues, let us prove the following proposition.

\begin{prop} \label{prop:funcsupinf}
If $f \colon D \to \R$ and $g \colon D \to \R$ ($D$ nonempty) are
bounded\footnote{The boundedness hypothesis is for simplicity,
it can be dropped if we allow for the extended real numbers.}
functions and
\begin{equation*}
f(x) \leq g(x) \qquad \text{for all $x \in D$},
\end{equation*}
then
\begin{equation} \label{prop:funcsupinf:eq}
\sup_{x \in D} f(x) \leq \sup_{x \in D} g(x)
\qquad \text{and} \qquad
\inf_{x \in D} f(x) \leq \inf_{x \in D} g(x) .
\end{equation}
\end{prop}

You should be careful with the variables.  The $x$ on the left side of
the inequality in \eqref{prop:funcsupinf:eq}
is different from the $x$ on the right.  You
should really think of the first inequality as
\begin{equation*}
\sup_{x \in D} f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
Let us prove this inequality.  If $b$ is an upper bound for $g(D)$, then
$f(x) \leq g(x) \leq b$ for all $x \in D$, and hence $b$ is an upper bound for $f(D)$.
Taking the least upper bound we get that for all $x \in D$
\begin{equation*}
f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
Therefore
$\sup_{y \in D} g(y)$ is an upper bound for $f(D)$ and thus greater than or
equal to the least upper bound of $f(D)$.
\begin{equation*}
\sup_{x \in D} f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
The second inequality (the statement about the inf) is left as an exercise.

\medskip

A common mistake is to conclude 
\begin{equation} \label{rn:av:ltnottrue}
\sup_{x \in D} f(x) \leq \inf_{y \in D} g(y) .
\end{equation}
The inequality \eqref{rn:av:ltnottrue} is not true given the hypothesis of
the proposition above.  For this stronger
inequality we need the stronger hypothesis
\begin{equation*}
f(x) \leq g(y) \qquad \text{for all $x \in D$ and $y \in D$.}
\end{equation*}
The proof as well as a counterexample is left as an exercise.

\subsection{Exercises}

\begin{exercise}
Show that
$\abs{x-y} < \epsilon$ if and only if $x-\epsilon < y < x+\epsilon$.
\end{exercise}

\begin{exercise}
Show: \qquad
%\begin{enumerate}[a)]
%\item
a)
$\max \{x,y\} = \frac{x+y+\abs{x-y}}{2}$
%\item
\qquad
b)
$\min \{x,y\} = \frac{x+y-\abs{x-y}}{2}$
%\end{enumerate}
\end{exercise}

\begin{exercise}
Find a number $M$ such that $\sabs{x^3-x^2+8x} \leq M$ for all $-2 \leq x \leq
10$.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:funcsupinf}.
That is, prove that
given any set $D$,
and two bounded functions
$f \colon D \to \R$ and $g \colon D \to \R$ 
such that $f(x) \leq g(x)$ for all $x \in D$, then 
\begin{equation*}
\inf_{x\in D} f(x) \leq \inf_{x\in D} g(x) .
\end{equation*}
\end{exercise}

\begin{exercise}
Let 
$f \colon D \to \R$ and $g \colon D \to \R$ be functions ($D$ nonempty).
\begin{enumerate}[a)]
%
\item
Suppose 
$f(x) \leq g(y)$ for all $x \in D$ and $y \in D$.  Show that
\begin{equation*}
\sup_{x\in D} f(x) \leq \inf_{x\in D} g(x) .
\end{equation*}
%
\item
Find a specific $D$, $f$, and $g$, such that
$f(x) \leq g(x)$ for all $x \in D$, but
\begin{equation*}
\sup_{x\in D} f(x) > \inf_{x\in D} g(x) .
\end{equation*}
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove \propref{prop:funcsupinf} without the assumption that
the functions are bounded.  Hint: You need to use the extended real
numbers.
\end{exercise}

\begin{exercise} \label{exercise:sumofsup}
Let $D$ be a nonempty set.
Suppose $f \colon D \to \R$ and $g \colon D \to \R$ are bounded functions.
\\
a)~Show 
\begin{equation*}
\sup_{x\in D} \bigl(f(x) + g(x) \bigr) \leq
\sup_{x\in D} f(x)
+
\sup_{x\in D} g(x)
\qquad \text{and} \qquad
\inf_{x\in D} \bigl(f(x) + g(x) \bigr) \geq
\inf_{x\in D} f(x)
+
\inf_{x\in D} g(x) .
\end{equation*}
b) Find examples where we obtain strict inequalities.
\end{exercise}

\begin{exercise}
Suppose $f \colon D \to \R$ and $g \colon D \to \R$ are bounded functions
and
$\alpha \in \R$.\\
a) Show that $\alpha f \colon D \to \R$ defined by $(\alpha f) (x) := \alpha
f(x)$ is a bounded function.\\
b) Show that $f+g \colon D \to \R$ defined by $(f+g) (x) := f(x) + g(x)$
is a bounded function.
\end{exercise}

\begin{exercise}
Let $f \colon D \to \R$ and $g \colon D \to \R$ be functions, $\alpha \in
\R$, and
recall what $f+g$ and $\alpha f$ means from the previous exercise.
\\
a) Prove that if $f+g$ and $g$ are bounded, then $f$ is bounded.
\\
b) Find an example where $f$ and $g$ are both unbounded, but $f+g$ is
bounded.
\\
c) Prove that is $f$ is bounded but $g$ is unbounded, then $f+g$ is
unbounded.
\\
d) Find an example where $f$ is unbounded but $\alpha f$ is bounded.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Intervals and the size of \texorpdfstring{$\R$}{R}}
\label{sec:intandsizeR}

\sectionnotes{0.5--1 lecture (proof of uncountability of $\R$ can be optional)}

You surely saw the notation for intervals\index{interval}
before, but let us give a formal
definition here.  For $a,  b \in \R$ such that $a < b$ we define
\begin{align*}
& [a,b] := \{ x \in \R : a \leq x \leq b \}, \\
& (a,b) := \{ x \in \R : a < x < b \}, \\
& (a,b] := \{ x \in \R : a < x \leq b \}, \\
& [a,b) := \{ x \in \R : a \leq x < b \} .
\end{align*}
The interval $[a,b]$ is called a \emph{\myindex{closed interval}}
and $(a,b)$ is called an \emph{\myindex{open interval}}.  The intervals
of the form $(a,b]$ and $[a,b)$ are called
\emph{half-open intervals}\index{half-open interval}.

The above intervals were all \emph{bounded intervals}\index{bounded
interval}, since both $a$ and $b$ were real numbers.  We 
define \emph{unbounded intervals}\index{unbounded interval},
\begin{align*}
& [a,\infty) := \{ x \in \R : a \leq x \}, \\
& (a,\infty) := \{ x \in \R : a < x \}, \\
& (-\infty,b] := \{ x \in \R : x \leq b \}, \\
& (-\infty,b) := \{ x \in \R : x < b \} .
\end{align*}
For completeness we define $(-\infty,\infty) := \R$.
The intervals $[a,\infty)$, $(-\infty,b]$, and $\R$ are sometimes called
\emph{\myindex{unbounded closed intervals}},
and $(a,\infty)$, $(-\infty,b)$, and $\R$ are sometimes called
\emph{\myindex{unbounded open intervals}}.

In short, an interval is a set with at least two points
that contains all points between any two
points.\footnote{Sometimes single point sets and the empty set are
also called intervals, but in this book, intervals have at least 2 points.}
The proof of the following proposition is left as an exercise.

\begin{prop} \label{prop:intervaldef}
Let $I \subset \R$ be a set.
Then $I$ is an interval if and only if
$I$ contains at least 2 points, and
whenever $a < b < c$ and $a, c \in I$, then $b \in I$.
\end{prop}

We have already seen that any open interval $(a,b)$ (where $a < b$ of course)
must be nonempty.  For example, it contains the number $\frac{a+b}{2}$.
An unexpected fact is that from a set-theoretic perspective,
all intervals have the same ``size,'' that is, they all have
the same cardinality.  For example the map $f(x) := 2x$
takes the interval $[0,1]$ bijectively to the interval $[0,2]$.

Maybe more interestingly,
the function $f(x) := \tan(x)$
is a bijective map from $(-\nicefrac{\pi}{2},\nicefrac{\pi}{2})$ to $\R$.  Hence the bounded
interval $(-\nicefrac{\pi}{2},\nicefrac{\pi}{2})$ has the same cardinality as $\R$.  It is not
completely straightforward to construct a bijective map from $[0,1]$ to
say $(0,1)$, but it is possible.

And do not worry, there does exist a way to measure the ``size'' of subsets
of real numbers that
``sees''
the difference between $[0,1]$ and $[0,2]$.  However, its proper definition
requires much more machinery than we have right now.

Let us say more about the cardinality of intervals and hence about the
cardinality of $\R$.  We have seen that there exist irrational numbers, that is
$\R \setminus \Q$ is nonempty.  The question is: How many irrational numbers
are there?  It turns out there are a lot more irrational numbers than rational
numbers.  We have seen that $\Q$ is countable, and we will show 
that $\R$ is uncountable.
In fact, the cardinality of $\R$ is the
same as the cardinality of $\sP(\N)$, although we will not prove this
claim here.

\begin{thm}[Cantor]\index{Cantor's theorem}
$\R$ is uncountable.
\end{thm}

We give a modified version of
Cantor's original proof from
1874 as this proof requires the least setup.  Normally this proof is stated
as a contradiction proof, but a proof by contrapositive is easier to
understand.

\begin{proof}
Let $X \subset \R$ be a countably infinite
subset such that for any two real numbers
$a < b$, there is an $x \in X$ such that $a < x < b$.  Were $\R$ countable,
then we could take $X = \R$.  If we show that $X$ is necessarily
a proper subset, then $X$ cannot equal $\R$, and $\R$ must be
uncountable.

As $X$ is countably infinite, 
there is a bijection from $\N$ to $X$.  Consequently, we write $X$ as
a sequence of real numbers $x_1, x_2, x_3,\ldots$, such that
each number in $X$
is given by $x_j$ for some $j \in \N$.

Let us inductively
construct two sequences of real numbers $a_1,a_2,a_3,\ldots$ and
$b_1,b_2,b_3,\ldots$.  Let
$a_1 := x_1$ and $b_1 := x_1+1$.  Note that $a_1 < b_1$ and
$x_1 \notin (a_1,b_1)$.
For $k > 1$, suppose $a_{k-1}$ and $b_{k-1}$ have been defined.
Let us also suppose $(a_{k-1},b_{k-1})$ does not contain any $x_j$
for any $j=1,\ldots,k-1$.
\begin{enumerate}[(i)]
\item Define $a_k := x_j$, where $j$ is the smallest $j \in \N$
such that $x_j \in (a_{k-1},b_{k-1})$.  Such an $x_j$ exists by our
assumption on $X$.  
\item Next, define $b_k := x_j$ where $j$ is the smallest $j \in \N$
such that $x_j \in (a_{k},b_{k-1})$.
\end{enumerate}
Notice that $a_k < b_k$ and $a_{k-1} < a_k < b_k < b_{k-1}$.
Also notice that $(a_{k},b_{k})$ does not contain $x_k$ and hence
does not contain any $x_j$ for $j=1,\ldots,k$.

Claim: $a_j < b_k$ for all $j$ and $k$ in $\N$.  Let us first
assume $j < k$.  Then $a_j < a_{j+1} < \cdots < a_{k-1} < a_k < b_k$.
Similarly for $j > k$.  The claim follows.

Let $A = \{ a_j : j \in \N \}$ and $B = \{ b_j : j \in \N \}$.
By \propref{infsupineq:prop} and the claim above we have
\begin{equation*}
\sup\, A \leq \inf\, B .
\end{equation*}
Define $y := \sup\, A$.  The number $y$ cannot be a member of $A$.  If $y = a_j$
for some $j$, then $y < a_{j+1}$, which is impossible.
Similarly $y$ cannot be a member of $B$.  Therefore,
$a_j < y$ for all $j\in \N$
and $y < b_j$ for all $j\in \N$.
In other words $y \in (a_j,b_j)$ for all $j\in \N$.

Finally we must show that $y \notin X$.  If we do so, then we will have
constructed a real number not in $X$ showing that $X$ must have been a
proper subset.  Take any $x_k \in X$.  By the above construction
$x_k \notin (a_k,b_k)$, so $x_k \not= y$ as $y \in (a_k,b_k)$.

Therefore, the sequence $x_1,x_2,\ldots$ cannot contain all elements of $\R$
and thus $\R$ is uncountable.
\end{proof}

\subsection{Exercises}

\begin{exercise}
For $a < b$, construct an explicit bijection from $(a,b]$ to $(0,1]$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to (0,1)$ is a bijection.
Using $f$, construct a
bijection from $[-1,1]$ to $\R$.
\end{exercise}

%\begin{exercise}[Hard]
%Show that the cardinality of $\R$ is the same as the cardinality of
%$\sP(\N)$.  Hint: If you have a binary representation of a real number
%in the interval $[0,1]$, then you have a sequence of $1$'s and $0$'s.  Use the
%sequence to construct a subset of $\N$.  The tricky part is to notice
%that some numbers have more than one binary representation.
%\end{exercise}

\begin{exercise} \label{exercise:intervaldef}
Prove \propref{prop:intervaldef}.
That is,
suppose $I \subset \R$ is a subset with at least 2 elements
such that if $a < b < c$ and $a, c \in I$, then $b \in I$.
Prove that $I$ is one of the nine types of intervals explicitly
given in this section.
Furthermore, prove that the intervals given in this section
all satisfy this property.
\end{exercise}

\begin{exercise}[Hard]
Construct an explicit bijection from $(0,1]$ to $(0,1)$.
Hint: One approach is as follows: First map $(\nicefrac{1}{2},1]$
to $(0,\nicefrac{1}{2}]$, then map
$(\nicefrac{1}{4},\nicefrac{1}{2}]$
to $(\nicefrac{1}{2},\nicefrac{3}{4}]$, etc\ldots.
Write down the map explicitly, that
is, write down an algorithm that tells you exactly what number goes where.
Then prove that the map is a bijection.
\end{exercise}

\begin{exercise}[Hard]
Construct an explicit bijection from $[0,1]$ to $(0,1)$.
\end{exercise}

\begin{exercise}
a) Show that every closed interval $[a,b]$ is the intersection
of countably many open intervals.  b) Show that every open interval $(a,b)$
is a countable union of closed intervals.  c) Show that an intersection
of a possibly infinite family of bounded closed intervals,
$\bigcap\limits_{\iota \in I} [a_\iota,b_\iota]$,
is either empty, a single point,
or a bounded closed interval.
\end{exercise}

\begin{exercise}
Suppose $S$ is a set of disjoint open intervals in $\R$.  That is, 
if $(a,b) \in S$ and $(c,d) \in S$, then either $(a,b) = (c,d)$
or $(a,b) \cap (c,d) = \emptyset$.  Prove $S$ is a countable set.
\end{exercise}

\begin{exercise}
Prove that the cardinality of $[0,1]$ is the same as the cardinality of
$(0,1)$ by showing that
$\abs{[0,1]} \leq \abs{(0,1)}$ and
$\abs{(0,1)} \leq \abs{[0,1]}$.  See 
\defnref{def:comparecards}.
Note that this requires the Cantor--Bernstein--Schr\"oder theorem we
stated without proof.  Also note that this proof does not give you an
explicit bijection.
\end{exercise}

\begin{exercise}[Challenging]
A number $x$ is \emph{algebraic}\index{algebraic number} if $x$ is a root of a polynomial with
integer coefficients, in other words, $a_n x^n + a_{n-1} x^{n-1}  + \ldots
+ a_1 x + a_0 = 0$ where all $a_n \in \Z$.\\
a) Show that there are only
countably many algebraic numbers.\\
b) Show that there exist non-algebraic
numbers (follow in the footsteps of Cantor, use uncountability of $\R$).\\
Hint: Feel free to use the fact that a polynomial of degree $n$ has at most $n$ real
roots.
\end{exercise}

\begin{exercise}[Challenging]
Let $F$ be the set of all functions $f \colon \R \to \R$.
Prove $\abs{\R} < \abs{F}$
using Cantor's \thmref{cantorspowersetthm}.\footnote{Interestingly,
if $C$ would be the set of continuous functions then $\abs{\R} = \abs{C}$.}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Decimal representation of the reals}
\label{sec:decimals}

\sectionnotes{1 lecture (optional)}

We often think of real numbers as their
\emph{\myindex{decimal representation}}.  For
a positive integer $n$, we find the digits $d_K,d_{K-1},\ldots,d_2,d_1,d_0$ for some
$K$,
where each $d_j$ is an integer between $0$ and $9$, then
\begin{equation*}
n = d_K {10}^K + d_{K-1} {10}^{K-1} + \cdots + d_2 {10}^2 + d_1 10 + d_0 .
\end{equation*}
We often assume $d_K \not= 0$.  To represent $n$ we write the sequence of
digits: $n = d_K d_{K-1} \cdots d_2 d_1 d_0$.
By a (decimal)
\emph{\myindex{digit}}\index{decimal digit}, we mean an integer
between $0$ and $9$.

Similarly we
represent some rational numbers.  That is, for certain
numbers $x$, we can find
negative integer $-M$, a positive integer $K$, and digits
$d_K,d_{K-1},\ldots,d_1,d_0,d_{-1},\ldots,d_{-M}$, such that
\begin{equation*}
x = d_K {10}^K + d_{K-1} {10}^{K-1} + \cdots + d_2 {10}^2 + d_1 10 + d_0 
+ d_{-1} {10}^{-1} + d_{-2} {10}^{-2} + \cdots + d_{-M} {10}^{-M} .
\end{equation*}
We write $x = d_K d_{K-1} \cdots d_1 d_0 \, . \, d_{-1} d_{-2} \cdots d_{-M}$.

Not every real number has such a representation, even the simple
rational number $\nicefrac{1}{3}$ does not.  The irrational number $\sqrt{2}$ 
does not have such a representation either.  To get a representation for
all real numbers we must allow infinitely many digits.

Let us consider only real numbers in the interval $(0,1]$.  If
we find a representation for these, adding 
integers to them obtains a representation for all real numbers.
Take an infinite sequence of decimal digits:
\begin{equation*}
0.d_1d_2d_3\ldots.
\end{equation*}
That is, we have a digit $d_j$ for every $j \in \N$.
We renumbered the digits to avoid the negative signs.
We call the number
\begin{equation*}
D_n := 
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n} .
\end{equation*}
the truncation of $x$ to $n$ decimal digits.
We say this
sequence of digits represents a real number $x$ if
\begin{equation*}
x =
\sup_{n \in \N} \left(
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n}
\right) =
\sup_{n \in \N} \, D_n .
\end{equation*}

\begin{prop} \label{prop:decimalprop}
{~}
\begin{enumerate}[(i)]
\item
Every infinite sequence of digits
$0.d_1d_2d_3\ldots$ represents a unique real number $x \in [0,1]$, and
\begin{equation*}
D_n \leq x \leq D_n+\frac{1}{{10}^n} \qquad \text{for all $n \in \N$}.
\end{equation*}
\item
For every $x \in (0,1]$ there exists an infinite sequence of digits
$0.d_1d_2d_3\ldots$ that represents $x$.
There exists a unique representation such that
\begin{equation*}
D_n < x \leq D_n+\frac{1}{{10}^n} \qquad \text{for all $n \in \N$}.
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.  Take an arbitrary infinite sequence of
digits $0.d_1d_2d_3\ldots$.  Use the geometric sum formula to write
\begin{equation*}
\begin{split}
D_n =
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n} 
& \leq
\frac{9}{10} + 
\frac{9}{{10}^2} + 
\frac{9}{{10}^3} + 
\cdots +
\frac{9}{{10}^n} 
\\
& =
\frac{9}{10}
\bigl( 1 + \nicefrac{1}{10} + 
{(\nicefrac{1}{10})}^2 + \cdots + 
{(\nicefrac{1}{10})}^{n-1} \bigr)
\\
& =
\frac{9}{10}
\left(
\frac{1-{(\nicefrac{1}{10})}^{n}}{1-\nicefrac{1}{10}}
\right)
= 1-{(\nicefrac{1}{10})}^{n}
< 1 .
\end{split}
\end{equation*}
In particular, $D_n < 1$ for all $n$.  A sum of nonnegative numbers is
nonnegative so $D_n \geq 0$, and hence
\begin{equation*}
0 \leq \sup_{n\in \N} \, D_n \leq 1 .
\end{equation*}
Therefore, $0.d_1d_2d_3\ldots$ represents a unique number $x := \sup_{n\in
\N} D_n \in [0,1]$.
As $x$ is a supremum then $D_n \leq x$.
Take $m \in \N$.  If $m < n$, then $D_m - D_n \leq 0$.  If $m > n$, then
computing as above
\begin{equation*}
D_m - D_n
=
\frac{d_{n+1}}{{10}^{n+1}} + 
\frac{d_{n+2}}{{10}^{n+2}} + 
\frac{d_{n+3}}{{10}^{n+3}} + 
\cdots +
\frac{d_{m}}{{10}^m} 
\leq
\frac{1}{{10}^{n}}
\bigl(
1-{(\nicefrac{1}{10})}^{m-n}
\bigr)
<
\frac{1}{{10}^{n}} .
\end{equation*}
Take the supremum over $m$ to find
\begin{equation*}
x - D_n
\leq
\frac{1}{{10}^{n}} .
\end{equation*}

We move on to the
second item.  Take any $x \in (0,1]$.
First let us tackle the existence.
For convenience let $D_0 := 0$.
Then,
$D_0 < x \leq D_0 + {10}^{-0}$.
Suppose we defined the digits $d_1,d_2,\ldots,d_n$,
and that 
$D_k < x \leq D_k + {10}^{-k}$, for $k=0,1,2,\ldots,n$.  We need to define $d_{n+1}$.

By the 
\hyperref[thm:arch:i]{Archimedean property} of the real numbers,
find an integer $j$ such that
$x-D_n \leq j {10}^{-(n+1)}$.  Take the least such $j$ and obtain 
\begin{equation} \label{eq:theDnjineq}
(j-1){10}^{-(n+1)} < x-D_n \leq j {10}^{-(n+1)} .
\end{equation}
Let $d_{n+1} := j-1$.
As $D_n < x$,
then $d_{n+1} = j-1 \geq 0$.  On the other hand since
$x-D_n \leq {10}^{-n}$ we have that
$j$ is at most 10, and therefore $d_{n+1} \leq 9$.
So $d_{n+1}$ is a
decimal digit.
Since $D_{n+1} = D_n + d_{n+1} {10}^{-(n+1)}$
add $D_n$ to the inequality
\eqref{eq:theDnjineq} above:
\begin{equation*}
D_{n+1} = D_n + (j-1){10}^{-(n+1)} < x \leq
%D_n + j {10}^{-(n+1)} =
D_n + (j-1) {10}^{-(n+1)} +
{10}^{-(n+1)} = D_{n+1} + {10}^{-(n+1)} .
\end{equation*}
And so
$D_{n+1} < x \leq D_{n+1} + {10}^{-(n+1)}$ holds.
We inductively
defined an infinite sequence of digits $0.d_1d_2d_3\ldots$.

Consider $D_{n} < x \leq D_{n} + {10}^{-n}$.
As $D_n < x$ for all $n$, then
$\sup \{ D_n : n \in \N \} \leq x$.
The second inequality for $D_n$ implies
\begin{equation*}
x - \sup \{ D_m : m \in \N \}
\leq
x - D_n \leq 10^{-n} .
\end{equation*}
As the inequality holds for all $n$ and
${10}^{-n}$ can be made arbitrarily small (see
\exerciseref{exercise:bnlimit}) we have $x \leq 
\sup \{ D_m : m \in \N \}$.
Therefore
$\sup \{ D_m : m \in \N \} = x$.

What is left to show is the uniqueness.
Suppose $0.e_1e_2e_3\ldots$ is another representation of $x$.
Let $E_n$ be the $n$-digit truncation of $0.e_1e_2e_3\ldots$, and suppose
$E_n < x \leq E_n + {10}^{-n}$ for all $n \in \N$.
Suppose for some $K \in \N$, $e_n = d_n$ for all $n < K$, so
$D_{K-1} = E_{K-1}$.  Then
\begin{equation*}
E_K = D_{K-1} + e_K{10}^{-K} < x \leq E_K + {10}^{-K} = D_{K-1} +
e_K{10}^{-K} + {10}^{-K} .
\end{equation*}
Subtracting $D_{K-1}$ and multiplying by ${10}^{K}$ we get
\begin{equation*}
e_K < (x - D_{K-1}){10}^K \leq e_K + 1 .
\end{equation*}
Similarly,
\begin{equation*}
d_K < (x - D_{K-1}){10}^K \leq d_K + 1 .
\end{equation*}
Hence, both $e_K$ and $d_K$ are the largest integer $j$
such that $j < (x - D_{K-1}){10}^K$, and therefore $e_K = d_K$.  That is,
the representation is unique.
\end{proof}

The representation is not unique if we do not require
$D_n < x$ for all $n$.
For example, for the
number $\nicefrac{1}{2}$ the method in the proof obtains the representation
\begin{equation*}
0.49999\ldots .
\end{equation*}
However, we also have the representation $0.50000\ldots$.
%The key requirement that makes the representation in the proposition unique is
%$D_n < x$ for all $n$.  The inequality
%$x \leq D_n + {10}^{-n}$ is true for every representation
%by the computation in the beginning of the proof.
%In other words, 
%$D_n \leq x \leq D_n + {10}^{-n}$ is true for every decimal representation.

The only numbers that have nonunique
representations are ones that end either in an infinite sequence of $0$s
or $9$s, because the only representation for which
$D_n = x$ is one where all digits past the $n$th digit are zero.  In this case
there are exactly two representations of $x$ (see the exercises).

Let us give another proof of the uncountability of the reals using decimal
representations.
This is Cantor's second proof, and is probably better known.
This proof may seem shorter, but it is because we already did
the hard part above and we are left with a slick trick to prove that $\R$ is
uncountable.  This trick is called
\emph{\myindex{Cantor diagonalization}}\index{diagonalization} and
finds use in other proofs as well.

\begin{thm}[Cantor]
The set $(0,1]$ is uncountable.
\end{thm}

\begin{proof}
Let $X := \{ x_1,x_2,x_3,\ldots \}$ be any countable subset of real numbers in $(0,1]$.
We will construct a real number not in $X$.  Let
\begin{equation*}
x_n = 0.d_1^nd_2^nd_3^n\ldots
\end{equation*}
be the unique representation from the proposition, that is, $d_j^n$ is the
$j$th digit of the $n$th number.  Let
\begin{equation*}
e_n :=
\begin{cases}
1 & \text{if $d_n^n \not= 1$}, \\
2 & \text{if $d_n^n = 1$}.
\end{cases}
\end{equation*}
Let $E_n$ be the $n$-digit truncation of $y = 0.e_1e_2e_3\ldots$.  Because
all the digits are nonzero we get that $E_n < E_{n+1} \leq y$.  Therefore
\begin{equation*}
E_n < y \leq E_n + {10}^{-n} 
\end{equation*}
for all $n$, and the representation is the unique one for $y$ from 
the proposition.  For every $n$, the $n$th digit
of $y$ is different from the $n$th digit of $x_n$, so $y \not= x_n$.
Therefore $y \notin X$, and as $X$ was an arbitrary countable subset,
$(0,1]$ must be uncountable.  See \figureref{cantorexamplefig} for an
example.
\begin{myfigureht}
\begin{tabular}{cccccccc}
$x_1 =$ & $0.$ & \fbox{1} & 3        & 2        & 1        & 0        & $\cdots$ \\
$x_2 =$ & $0.$ & 7        & \fbox{9} & 4        & 1        & 3        & $\cdots$ \\
$x_3 =$ & $0.$ & 3        & 0        & \fbox{1} & 3        & 4        & $\cdots$ \\
$x_4 =$ & $0.$ & 8        & 9        & 2        & \fbox{5} & 6        & $\cdots$ \\
$x_5 =$ & $0.$ & 1        & 6        & 0        & 2        & \fbox{4} & $\cdots$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$
\end{tabular}
\qquad
\parbox{1.6in}{
Number not in the list:\\
$y = 0.21211\ldots$
}
\caption{Example of Cantor diagonalization, the diagonal digits $d_n^n$
marked.\label{cantorexamplefig}}
\end{myfigureht}
\end{proof}

Using decimal digits we can also find lots of numbers that are not rational.
The following proposition is true for every
rational number, but we give it only for $x \in (0,1]$ for simplicity.

\begin{prop} \label{prop:rationaldecimal}
If $x \in (0,1]$ is a rational number and $x = 0.d_1d_2d_3\ldots$,
then the decimal digits eventually start repeating.  That is, there are 
positive integers $N$ and $P$, such that for all $n \geq N$, $d_n = d_{n+P}$.
\end{prop}

\begin{proof}
Let $x = \nicefrac{p}{q}$ for positive integers $p$ and $q$.
Suppose $x$ is a number with a unique representation, as
otherwise we have seen above that both its representations are repeating,
see also \exerciseref{exercise:nonuniquedecimals}.  This also means
that $x \not= 1$ so $p < q$.

To compute the first digit we take $10 p$ and divide by
$q$.  Let $d_1$ be the quotient, and the remainder $r_1$ is some integer
between 0 and $q-1$.  That is, $d_1$ is the largest integer
such that $d_1 q \leq 10p$ and then $r_1 = 10p - d_1q$.
As $p < q$, then $d_1 < 10$, so $d_1$ is a digit.
Furthermore,
\begin{equation*}
\frac{d_1}{10} \leq \frac{p}{q} =
\frac{d_1}{10} + \frac{r_1}{10q} \leq \frac{d_1}{10} +
\frac{1}{10} .
\end{equation*}
The first inequality must be strict since $x$ has a
unique representation.  That is, $d_1$ really is the first digit.
What is left is $\nicefrac{r_1}{(10q)}$.  This is the same as computing the
first digit of $\nicefrac{r_1}{q}$.
To compute $d_2$ divide $10 r_1$ by $q$, and so on.
After computing $n-1$ digits, we have
$\nicefrac{p}{q} = D_{n-1} + \nicefrac{r_{n-1}}{(10^{n} q)}$.
To get the $n$th digit,
divide $10 r_{n-1}$ by $q$
to get quotient $d_n$, remainder $r_n$, and the inequalities
\begin{equation*}
\frac{d_n}{10} \leq \frac{r_{n-1}}{q} =
\frac{d_n}{10} + \frac{r_n}{10q} \leq \frac{d_n}{10} +
\frac{1}{10} .
\end{equation*}
Dividing by $10^{n-1}$ and adding $D_{n-1}$ we find
\begin{equation*}
D_n \leq D_{n-1} + \frac{r_{n-1}}{10^{n} q} = \frac{p}{q} \leq D_n +
\frac{1}{10^n} .
\end{equation*}
By uniqueness we really have 
the $n$th digit $d_n$ from the construction.

The new digit depends only the remaineder from the previous
step.  There
are at most $q$ possible remainders
and hence at some step the process must start repeating itself.
In fact, $P$ is at most $q$.
\end{proof}

The converse of the proposition is also true and is left as an exercise.

\begin{example}
The number
\begin{equation*}
x = 0.101001000100001000001\ldots,
\end{equation*}
is irrational.  That is, the digits are $n$ zeros, then a one, then $n+1$
zeros, then a one, and so on and so forth.  The fact that $x$ is irrational follows from the
proposition; the digits never start repeating.  For every $P$,
if we go far enough, we find a 1 that is followed by at least $P+1$ zeros.
\end{example}

\subsection{Exercises}

\begin{exercise}[Easy]
What is the decimal representation of $1$ guaranteed by
\propref{prop:decimalprop}?  Make sure to show that it does satisfy
the condition.
\end{exercise}

\begin{exercise}
Prove the converse of \propref{prop:rationaldecimal}, that is,
if the digits in the decimal representation of $x$ are eventually repeating, then 
$x$ must be rational.
\end{exercise}

\begin{exercise} \label{exercise:nonuniquedecimals}
Show that real numbers $x \in (0,1)$ with nonunique decimal representation
are exactly the rational numbers that can be written
as $\frac{m}{10^n}$ for some integers $m$ and $n$.  In this case show that
there exist exactly two representations of $x$.
\end{exercise}

\begin{exercise}
Let $b \geq 2$ be an integer.  Define a representation of a real number in
$[0,1]$ in terms of base $b$ rather than base 10 and prove
\propref{prop:decimalprop} for base $b$.
\end{exercise}

\begin{exercise}
Using the previous exercise with $b=2$ (binary), 
show that cardinality of $\R$ is the same as the cardinality of $\sP(\N)$,
obtaining yet another (though related) proof that $\R$ is uncountable.
Hint: Construct two injections, one from $[0,1]$ to $\sP(\N)$
and one from $\sP(\N)$ to $[0,1]$.  Hint 2: Given a
set $A \subset \N$, let the $n$th binary digit of $x$ be 1 if $n\in A$.
\end{exercise}

\begin{exercise}[Challenging]
Construct a bijection between $[0,1]$ and $[0,1] \times [0,1]$.\footnote{%
If you can't do it, try to at least construct an injection from
$[0,1] \times [0,1]$ to
$[0,1]$.}
Hint:
Consider even and odd digits to construct a bijection between 
$[0,1] \setminus A$ and $[0,1] \times [0,1]$ for a countable set $A$ (be
careful about uniqueness of representation).
Then construct a bijection between $([0,1] \times [0,1]) \setminus B$
and $[0,1] \times [0,1]$ for a countable set $B$ (e.g.\ use 
that $\N$ and the even natural numbers are bijective).
\end{exercise}

\begin{exercise}
Prove that if $x = \nicefrac{p}{q} \in (0,1]$ is a rational number, $q > 1$,
then the period $P$ of repeating digits is in fact less then or equal to $q-1$.
\end{exercise}

\begin{exercise} \label{exercise:bnlimit}
Prove that if $b \in \N$ and $b \geq 2$, then for any $\epsilon > 0$,
there is an $n \in \N$ such that 
we have $b^{-n} < \epsilon$.  Hint:
One possibility is to first prove that $b^n > n$ for all $n \in \N$ by induction.
%May be easier to prove that $\{ b^n : n
%\in \N \}$ is an unbounded subset of the natural numbers.
\end{exercise}

\begin{exercise}
Explicitly construct an injection $f \colon \R \to \R \setminus \Q$ using
\propref{prop:rationaldecimal}.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Sequences and Series} \label{seq:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sequences and limits}
\label{sec:seqsandlims}

\sectionnotes{2.5 lectures}

Analysis is essentially about taking limits.  The most basic type of a limit
is a limit of a sequence of real numbers.
We have already seen sequences used informally.  Let us give the formal
definition.

\begin{defn}
A \emph{\myindex{sequence}} (of real numbers) is a function $x \colon \N \to \R$.  Instead of $x(n)$ we 
usually denote the $n$th element in the sequence by $x_n$.  We 
use the notation $\{ x_n \}$, or more precisely
\begin{equation*}
\{ x_n \}_{n=1}^\infty,
\end{equation*}
to denote a sequence.

A sequence $\{ x_n \}$ is \emph{bounded}\index{bounded sequence} if
there exists a $B \in \R$ such that
\begin{equation*}
\abs{x_n} \leq B \qquad \text{for all $n \in \N$.}
\end{equation*}
In other words, the sequence $\{x_n\}$ is bounded whenever
the set $\{ x_n : n \in \N \}$
is bounded.
\end{defn}

When we need
to give a concrete sequence we often give each term as a formula in
terms of $n$.
For example, $\{ \nicefrac{1}{n} \}_{n=1}^\infty$, or simply $\{
\nicefrac{1}{n} \}$, stands for
the sequence $1, \nicefrac{1}{2}, \nicefrac{1}{3}, \nicefrac{1}{4},
\nicefrac{1}{5}, \ldots$.
The sequence $\{ \nicefrac{1}{n} \}$
is a bounded sequence ($B=1$ will
suffice).  On the other hand the sequence $\{ n \}$ stands for
$1,2,3,4,\ldots$, and this sequence is not bounded (why?).

While the notation for a sequence
is similar\footnote{\cite{BS} use the notation $(x_n)$ to denote
a sequence instead of $\{ x_n \}$, which is what \cite{Rudin:baby} uses.
Both are common.}
to that of a set, the notions are
distinct.  For example, the sequence $\{ {(-1)}^n \}$ is the sequence
$-1,1,-1,1,-1,1,\ldots$, whereas the set of values, the
\emph{range of the sequence}\index{range of a sequence},
is just the set $\{ -1, 1 \}$.  We can write this set
as $\{ {(-1)}^n : n \in \N \}$.   When ambiguity can arise, we
use the words \emph{sequence} or \emph{set} to distinguish the two
concepts.

Another example of a sequence is the so-called \emph{\myindex{constant sequence}}.
That is a sequence $\{ c \} = c,c,c,c,\ldots$ consisting of a single
constant $c \in \R$ repeating indefinitely.

We now get to the idea of a \emph{\myindex{limit of a sequence}}.  We will
see in \propref{prop:limisunique}
that the notation below is well defined.  That is, if a limit exists, then
it is unique.  So it makes sense to talk about \emph{the} limit of a sequence.

\begin{defn}
A sequence $\{ x_n \}$ is said to \emph{\myindex{converge}} to a number
$x \in \R$, if for every $\epsilon > 0$, there exists an $M \in \N$ such
that $\abs{x_n - x} < \epsilon$ for all $n \geq M$.  The number $x$
is said to be the \emph{limit} of $\{ x_n \}$.  We write
\begin{equation*}
\lim_{n\to \infty} x_n := x .
\end{equation*}

A sequence
that converges is said to be \emph{convergent}\index{convergent sequence}.
Otherwise, the sequence is said to be
\emph{divergent}\index{divergent sequence}.
\end{defn}

It is good to know intuitively what a limit means.  It means that eventually
every number in the sequence is close to the number $x$.  More precisely,
we can get arbitrarily close to the limit, provided we go far enough in the
sequence.  It does not mean we ever reach the limit.  It is possible,
and quite common, that there is no $x_n$ in the sequence that equals the
limit $x$.
We illustrate the concept in \figureref{figsequenceconvergence}.  In the
figure we first think of the sequence as a graph, as it is a function of
$\N$.   Secondly we also plot it as a sequence of labeled points on the real
line.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sequence-convergence.eepic}

\vspace*{12pt}

\hspace{13pt} \subimport*{figures/}{sequence-convergence-2.eepic}
\caption{Illustration of convergence.  On top, the first ten points of the sequence as a graph
with $M$ and the interval around the limit $x$ marked.  On bottom, the points of the same sequence marked on the
number line.\label{figsequenceconvergence}}
%\end{center}
\end{myfigureht}

When we write $\lim\, x_n = x$ for some real number $x$, we are saying two
things.  First, that $\{ x_n \}$ is convergent, and second that the limit is
$x$.

The above definition is one of the most important definitions in analysis,
and it is necessary to understand it perfectly.  The key point in the
definition is that given \emph{any} $\epsilon > 0$, we can find an $M$.  The
$M$ can depend on $\epsilon$, so we only pick an $M$ once we know
$\epsilon$.  Let us illustrate this concept on a few examples.

\begin{example}
The constant sequence $1,1,1,1,\ldots$ is convergent and the limit is 1.  For
every $\epsilon > 0$, we pick $M = 1$.
\end{example}

\begin{example}
Claim: The sequence $\{ \nicefrac{1}{n} \}$ is convergent and
\begin{equation*}
\lim_{n\to \infty} \frac{1}{n} = 0 .
\end{equation*}
Proof: Given an $\epsilon > 0$, we find an $M \in \N$ such that
$0 < \nicefrac{1}{M} < \epsilon$
(\hyperref[thm:arch:i]{Archimedean property} at work).
Then for all $n \geq M$ we have that
\begin{equation*}
\abs{x_n - 0} = \abs{\frac{1}{n}} = \frac{1}{n} \leq \frac{1}{M} < \epsilon .
\end{equation*}
\end{example}

\begin{example}
The sequence $\{ {(-1)}^n \}$ is divergent.  Proof: If there
were a limit $x$, then for $\epsilon = \frac{1}{2}$ we expect an $M$ that
satisfies the definition.  Suppose
such an $M$ exists, then for an even $n \geq M$ we compute
\begin{equation*}
\nicefrac{1}{2} > \abs{x_n - x}  = \abs{1 - x}
\qquad \text{and} \qquad
\nicefrac{1}{2} > \abs{x_{n+1} - x}  = \abs{-1 - x} .
\end{equation*}
But
\begin{equation*}
2 = \abs{1 - x - (-1 -x)} \leq
\abs{1 - x} + \abs{-1 -x} < \nicefrac{1}{2} + \nicefrac{1}{2} = 1 ,
\end{equation*}
and that is a contradiction.
\end{example}

\begin{prop} \label{prop:limisunique}
A convergent sequence has a unique limit.
\end{prop}

The proof of this proposition exhibits a useful technique in
analysis.  Many proofs follow the same general scheme.  We want to
show a certain quantity is zero.  We write the quantity using the 
triangle inequality as two quantities, and we estimate each one
by arbitrarily small numbers.

\begin{proof}
%NOTE: should be word for word the same as 7.3.3
Suppose the sequence $\{ x_n \}$ has the limit $x$ and the limit $y$.
Take an arbitrary $\epsilon > 0$.
From the definition find an $M_1$ such that for all $n \geq M_1$,
$\abs{x_n-x} < \nicefrac{\epsilon}{2}$.  Similarly find an $M_2$
such that for all $n \geq M_2$ we have
$\abs{x_n-y} < \nicefrac{\epsilon}{2}$.
Now take an $n$ such that $n \geq M_1$ and also $n \geq M_2$, and estimate
\begin{equation*}
\begin{split}
\abs{y-x}
& =
\abs{x_n-x - (x_n -y)} \\
& \leq
\abs{x_n-x} + \abs{x_n -y} \\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
As $\abs{y-x} < \epsilon$ for all $\epsilon > 0$, then $\abs{y-x} = 0$
and $y=x$.  Hence the limit (if it exists) is unique.
\end{proof}

\begin{prop}
A convergent sequence $\{ x_n \}$ is bounded.
\end{prop}

\begin{proof}
Suppose $\{ x_n \}$ converges to $x$.  Thus there exists an $M \in \N$
such that for all $n \geq M$ we have
$\abs{x_n - x} < 1$.  Let $B_1 := \abs{x}+1$ and note that for $n \geq M$ we
have
\begin{equation*}
\begin{split}
\abs{x_n} & = \abs{x_n - x + x}
\\
& \leq \abs{x_n - x} + \abs{x}
\\
& < 1 + \abs{x} = B_1 .
\end{split}
\end{equation*}
The set $\{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}} \}$
is a finite set and hence let
\begin{equation*}
B_2 := \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}} \} .
\end{equation*}
Let $B := \max \{ B_1, B_2 \}$.  Then for all $n \in \N$ we have
\begin{equation*}
\abs{x_n} \leq B. \qedhere
\end{equation*}
\end{proof}

The sequence $\{ {(-1)}^n \}$ shows that the converse
does not hold.  A bounded sequence is not necessarily convergent.

\begin{example}
Let us show the sequence $\left\{ \frac{n^2+1}{n^2+n} \right\}$ converges and
\begin{equation*}
\lim_{n\to\infty} \frac{n^2+1}{n^2+n} = 1 .
\end{equation*}

Given $\epsilon > 0$,
find $M \in \N$ such that $\frac{1}{M} < \epsilon$.  Then for any $n \geq
M$ we have
\begin{equation*}
\begin{split}
%\abs{\frac{n^2+1}{n^2+n} - 1} & =
%\abs{\frac{n^2+1 - (n^2+n)}{n^2+n}} \\
\abs{\frac{n^2+1}{n^2+n} - 1}  =
\abs{\frac{n^2+1 - (n^2+n)}{n^2+n}}
& =
\abs{\frac{1 - n}{n^2+n}} \\
& =
\frac{n-1}{n^2+n} \\
& \leq 
\frac{n}{n^2+n} 
 =
\frac{1}{n+1}  \\
& \leq \frac{1}{n}
\leq \frac{1}{M} < \epsilon .
\end{split}
\end{equation*}
Therefore,
$\lim \frac{n^2+1}{n^2+n} = 1$.
This example shows that sometimes to get what you want, you must throw away
some information to get a simpler estimate.
\end{example}

\subsection{Monotone sequences}

The simplest type of a sequence is a monotone sequence.  Checking that
a monotone sequence converges is as easy as checking that it is bounded.
It is also easy to find
the limit for a convergent
monotone sequence, provided we can find the supremum or infimum
of a countable set of numbers.

\begin{defn}
A sequence $\{ x_n \}$ is \emph{monotone increasing}\index{monotone
increasing sequence} if $x_n \leq x_{n+1}$ for all $n \in \N$.  
%
A sequence $\{ x_n \}$ is \emph{monotone decreasing}\index{monotone
decreasing sequence} if $x_n \geq x_{n+1}$ for all $n \in \N$.  
%
If a sequence is either monotone increasing or monotone decreasing, we
can simply say the sequence is \emph{monotone}\index{monotone sequence}.  Some
authors also use the word \emph{monotonic}\index{monotonic sequence}.
\end{defn}

For example, $\{ \nicefrac{1}{n} \}$ is monotone decreasing,
the constant sequence $\{ 1 \}$ is both monotone increasing and monotone
decreasing, and $\{ {(-1)}^n \}$ is not monotone.
First few terms of a sample monotone increasing sequence
are shown in 
\figureref{figsequenceincreasing}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sequence-increasing.eepic}
\caption{First few terms of a monotone increasing sequence as a
graph.\label{figsequenceincreasing}}
%\end{center}
\end{myfigureht}

\begin{prop} \label{prop:monotoneconv}
A monotone sequence $\{ x_n \}$ is bounded if and only if it is convergent.

Furthermore, if $\{ x_n \}$ is monotone increasing and bounded, then
\begin{equation*}
\lim_{n\to \infty} x_n = \sup \{ x_n : n \in \N \} .
\end{equation*}
If $\{ x_n \}$ is monotone decreasing and bounded, then
\begin{equation*}
\lim_{n\to \infty} x_n = \inf \{ x_n : n \in \N \} .
\end{equation*}
\end{prop}

\begin{proof}
Let us suppose the sequence is monotone increasing.  Suppose 
the sequence is bounded, so there exists a $B$
such that $x_n \leq B$ for all $n$, that is the set
$\{ x_n : n \in  \N \}$ is bounded from above.  Let
\begin{equation*}
x := \sup \{ x_n : n \in \N \} .
\end{equation*}
Let $\epsilon > 0$ be arbitrary.  As $x$ is the supremum, then
there must be at least one $M \in \N$ such that $x_{M} > x-\epsilon$
(because $x$ is the supremum).  As $\{ x_n \}$ is monotone increasing,
then it is easy to see (by \hyperref[induction:thm]{induction}) that
$x_n \geq x_{M}$ for all $n \geq M$.  Hence
\begin{equation*}
\abs{x_n-x} = x-x_n \leq x-x_{M} < \epsilon  .
\end{equation*}
Therefore the sequence converges to $x$.
We already know that a convergent sequence is bounded, which completes the
other direction of the implication.

The proof for monotone decreasing sequences is left as an exercise.
\end{proof}

\begin{example}
Take the sequence $\{ \frac{1}{\sqrt{n}} \}$.

First $\frac{1}{\sqrt{n}} > 0$ for all $n \in \N$, and hence the sequence is
bounded from below.
Let us show that it is monotone decreasing.  We start with
$\sqrt{n+1} \geq \sqrt{n}$ (why is that true?).  From this inequality
we obtain
\begin{equation*}
\frac{1}{\sqrt{n+1}} \leq \frac{1}{\sqrt{n}} .
\end{equation*}
So the sequence is monotone decreasing and bounded from below (hence
bounded).  We apply the theorem to note that the sequence is
convergent and in fact
\begin{equation*}
\lim_{n\to \infty} \frac{1}{\sqrt{n}}
=
\inf \left\{ \frac{1}{\sqrt{n}} : n \in \N \right\} .
\end{equation*}
We already know that the infimum is greater than or equal to 0, as
0 is a lower bound.  Take a number $b \geq 0$ such
that $b \leq \frac{1}{\sqrt{n}}$ for all $n$.  We square both sides to
obtain
\begin{equation*}
b^2 \leq \frac{1}{n} \qquad \text{for all $n \in \N$}.
\end{equation*}
We have seen before that this implies that $b^2 \leq 0$ (a consequence
of the \hyperref[thm:arch:i]{Archimedean property}).  As we also have $b^2 \geq 0$, then $b^2 = 0$
and so $b = 0$.
Hence $b=0$ is the greatest lower bound, and $\lim \frac{1}{\sqrt{n}} = 0$.
\end{example}

\begin{example}
A word of caution:  We must show that a monotone sequence is bounded
in order to use \propref{prop:monotoneconv}.  For
example, 
the sequence $\{ 1 + \nicefrac{1}{2} + \cdots + \nicefrac{1}{n} \}$
is a monotone increasing
sequence that grows very slowly.  We will see, once we get to series,
that this sequence has no upper bound and so does not converge.  It is not
at all obvious that this sequence has no upper bound.
\end{example}

A common example of where monotone sequences arise is the following
proposition.  The proof is left as an exercise.

\begin{prop} \label{prop:supinfseq}
Let $S \subset \R$ be a nonempty bounded set.
Then there exist monotone sequences
$\{ x_n \}$ and $\{ y_n \}$ such that $x_n, y_n \in S$ and
\begin{equation*}
\sup\,S = \lim_{n\to \infty} x_n \qquad \text{and} \qquad \inf\,S =
\lim_{n\to\infty} y_n .
\end{equation*}
\end{prop}

\subsection{Tail of a sequence}

\begin{defn}
For a sequence $\{ x_n \}$,
the \emph{$K$-tail} (where $K \in \N$)
or just the
\emph{tail}\index{tail of a sequence} of
the sequence is the sequence starting at $K+1$, usually written as
\begin{equation*}
\{ x_{n+K} \}_{n=1}^\infty
\qquad \text{or} \qquad \{ x_n \}_{n=K+1}^\infty .
\end{equation*}
\end{defn}
For example, the $4$-tail of $\{ \nicefrac{1}{n} \}$ is
$\nicefrac{1}{5}, \nicefrac{1}{6}, \nicefrac{1}{7}, \nicefrac{1}{8},
\ldots$.  The $0$-tail of a sequence is the sequence itself.
The main result about the tail of a sequence is the following proposition.

\begin{prop}
Let $\{ x_n \}_{n=1}^\infty$ be a sequence.  Then the following
statements are equivalent:
\begin{enumerate}[(i)]
\item \label{prop:ktail:i}
The sequence $\{ x_n \}_{n=1}^\infty$ converges.
\item \label{prop:ktail:ii}
The $K$-tail $\{ x_{n+K} \}_{n=1}^\infty$ converges for all $K \in \N$.
\item \label{prop:ktail:iii}
The $K$-tail $\{ x_{n+K} \}_{n=1}^\infty$ converges for some $K \in \N$.
\end{enumerate}
Furthermore, if any (and hence all) of the limits exist, then for any $K \in \N$
\begin{equation*}
\lim_{n\to \infty} x_n = \lim_{n \to \infty} x_{n+K} .
\end{equation*}
\end{prop}

\begin{proof}
It is clear that
\ref{prop:ktail:ii} implies \ref{prop:ktail:iii}.
We will therefore show first that
\ref{prop:ktail:i}
implies
\ref{prop:ktail:ii},
and then we will show that
\ref{prop:ktail:iii}
implies
\ref{prop:ktail:i}.  In the process we will also show that the limits are equal.

Let us start with \ref{prop:ktail:i} implies \ref{prop:ktail:ii}.
Suppose $\{x_n \}$ converges to some $x \in \R$.
Let $K \in \N$ be arbitrary.
Define $y_n := x_{n+K}$, we wish to show that $\{ y_n \}$ converges
to $x$.
Given an $\epsilon > 0$, there exists an $M \in \N$ such that
$\abs{x-x_n} < \epsilon$ for all $n \geq M$.
Note that $n \geq M$ implies $n+K \geq M$.  Therefore, for
all $n \geq M$ we have that 
\begin{equation*}
\abs{x-y_n} = \abs{x-x_{n+K}} < \epsilon .
\end{equation*}
Consequently, $\{ y_n \}$ converges to $x$.

Let us move to \ref{prop:ktail:iii} implies \ref{prop:ktail:i}.
Let $K \in \N$ be given, define
$y_n := x_{n+K}$, and suppose that $\{ y_n \}$ converges to $x \in \R$.
That is, given an $\epsilon > 0$, there exists an $M' \in \N$ such that
$\abs{x-y_n} < \epsilon$ for all $n \geq M'$.
Let $M := M'+K$.  Then $n \geq M$ implies $n-K \geq M'$.
Thus, whenever $n \geq M$ we have
\begin{equation*}
\abs{x-x_n} = \abs{x-y_{n-K}} < \epsilon.
\end{equation*}
Therefore $\{ x_n \}$ converges to $x$.
\end{proof}

Essentially, the limit does not care about how the sequence begins, it only
cares about the tail of the sequence.  That is, the beginning of the sequence
may be arbitrary.

For example, the sequence defined by $x_n := \frac{n}{n^2+16}$ is decreasing
if we start at $n=4$ (it is increasing before).  That is,
$\{ x_n \} =
\nicefrac{1}{17},
\nicefrac{1}{10},
\nicefrac{3}{25},
\nicefrac{1}{8},
\nicefrac{5}{41},
\nicefrac{3}{26},
\nicefrac{7}{65},
\nicefrac{1}{10},
\nicefrac{9}{97},
\nicefrac{5}{58},\ldots$, and 
\begin{equation*}
\nicefrac{1}{17} <
\nicefrac{1}{10} <
\nicefrac{3}{25} <
\nicefrac{1}{8} >
\nicefrac{5}{41} >
\nicefrac{3}{26} >
\nicefrac{7}{65} >
\nicefrac{1}{10} >
\nicefrac{9}{97} >
\nicefrac{5}{58} > \ldots .
\end{equation*}
If we throw away the first 3 terms
and look at the 3 tail it is decreasing.  The proof is left as an exercise.  Since the 3-tail
is monotone and bounded below by zero, it is convergent, and therefore the sequence is convergent.

\subsection{Subsequences}

A very useful concept related to sequences is that of a subsequence.
A subsequence of $\{ x_n \}$ is a sequence that contains
only some of the numbers from $\{ x_n \}$ in the same order.

\begin{defn}
Let $\{ x_n \}$ be a sequence.
Let $\{ n_i \}$ be a strictly increasing sequence of natural
numbers, that is $n_i < n_{i+1}$ for all $i$ (in other words $n_1 < n_2 < n_3 < \cdots$).  
The sequence
\begin{equation*}
\{ x_{n_i} \}_{i=1}^\infty
\end{equation*}
is called 
a \emph{\myindex{subsequence}} of $\{ x_n \}$.
\end{defn}

For example, take the sequence $\{ \nicefrac{1}{n} \}$.  The sequence
$\{ \nicefrac{1}{3n} \}$ is a subsequence.  To see how these two
sequences fit in the definition, take $n_i := 3i$.  
The numbers in the
subsequence must come from the original sequence, so $1,0,\nicefrac{1}{3},0,
\nicefrac{1}{5},\ldots$
is not a subsequence of $\{ \nicefrac{1}{n} \}$.  Similarly order
must be preserved, so
the sequence $1,\nicefrac{1}{3},\nicefrac{1}{2},\nicefrac{1}{5},\ldots$
is not a subsequence of $\{ \nicefrac{1}{n} \}$.

A tail of a sequence is one special type of a subsequence.  For an arbitrary
subsequence, we have the following proposition about convergence.

\begin{prop} \label{prop:seqtosubseq}
If $\{ x_n \}$ is a convergent sequence,
then any subsequence $\{ x_{n_i} \}$ is also convergent and
\begin{equation*}
\lim_{n\to \infty} x_n = 
\lim_{i\to \infty} x_{n_i} .
\end{equation*}
\end{prop}

\begin{proof}
Suppose $\lim_{n\to \infty} x_n = x$.  That means that for every
$\epsilon > 0$ we have an $M \in \N$ such that for all $n \geq M$
\begin{equation*}
\abs{x_n - x} < \epsilon .
\end{equation*}
It is not hard to prove (do it!) by \hyperref[induction:thm]{induction} that
$n_i \geq i$.  Hence $i \geq M$ implies $n_i \geq M$.  Thus,
for all $i \geq M$ we have
\begin{equation*}
\abs{x_{n_i} - x} < \epsilon ,
\end{equation*}
and we are done.
\end{proof}

\begin{example}
Existence of a convergent subsequence does not imply
convergence of the sequence itself.
Take the sequence $0,1,0,1,0,1,\ldots$.  That is,
$x_n = 0$ if $n$ is odd, and $x_n = 1$ if $n$ is even.  The sequence
$\{ x_n \}$ is divergent, however, the subsequence
$\{ x_{2n} \}$ converges to 1 and the subsequence
$\{ x_{2n+1} \}$ converges to 0.  Compare \propref{seqconvsubseqconv:prop}.
\end{example}

\subsection{Exercises}

\begin{exnote}
In the following exercises, feel free to use what you know from calculus to
find the limit, if it exists.  But you must \emph{prove}
that you
found the correct limit, or prove that the series is divergent.
\end{exnote}

\begin{exercise}
Is the sequence
$\{ 3n \}$
bounded?  Prove or disprove.
\end{exercise}

\begin{exercise}
Is the sequence
$\{ n \}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise}
Is the sequence
$\left\{ \dfrac{{(-1)}^n}{2n} \right\}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise}
Is the sequence
$\{ 2^{-n} \}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise}
Is the sequence
$\left\{ \dfrac{n}{n+1} \right\}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise}
Is the sequence
$\left\{ \dfrac{n}{n^2+1} \right\}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise} \label{exercise:absconv}
Let $\{ x_n \}$ be a sequence.
\begin{enumerate}[a)]
\item Show that $\lim\, x_n = 0$ (that is, the limit exists and is zero)
if and only if $\lim \abs{x_n} = 0$.
\item Find an example such that $\{ \abs{x_n} \}$ converges and $\{ x_n \}$
diverges.
\end{enumerate}
\end{exercise}

\begin{exercise}
Is the sequence
$\left\{ \dfrac{2^n}{n!} \right\}$
convergent?  If so, what is the limit?
\end{exercise}

\begin{exercise}
Show that the sequence
$\left\{ \dfrac{1}{\sqrt[3]{n}} \right\}$ is monotone, bounded, and use
\propref{prop:monotoneconv} to find the limit.
\end{exercise}

\begin{exercise}
Show that the sequence
$\left\{ \dfrac{n+1}{n} \right\}$
is monotone, bounded, and use
\propref{prop:monotoneconv} to find the limit.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:monotoneconv} for monotone decreasing
sequences.
\end{exercise}

\begin{exercise}
Prove \propref{prop:supinfseq}.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a convergent monotone sequence.  Suppose 
there exists a $k \in \N$ such that
\begin{equation*}
\lim_{n\to \infty} x_n = x_k .
\end{equation*}
Show that $x_n = x_k$ for all $n \geq k$.
\end{exercise}

\begin{exercise}
Find a convergent subsequence of the sequence
$\{ {(-1)}^n \}$.
\end{exercise}

\begin{exercise}
Let $\{x_n\}$ be a sequence defined by
\begin{equation*}
x_n := 
\begin{cases}
n & \text{if $n$ is odd} , \\
\nicefrac{1}{n} & \text{if $n$ is even} .
\end{cases}
\end{equation*}
\begin{enumerate}[a)]
\item Is the sequence bounded? (prove or disprove)
\item Is there a convergent subsequence?  If so, find it.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence.
Suppose there are two convergent subsequences $\{ x_{n_i} \}$ and
$\{ x_{m_i} \}$.  Suppose 
\begin{equation*}
\lim_{i\to\infty} x_{n_i} = a
\qquad \text{and} \qquad
\lim_{i\to\infty} x_{m_i} = b,
\end{equation*}
where $a \not= b$.  Prove that $\{ x_n \}$ is not convergent, without
using \propref{prop:seqtosubseq}.
\end{exercise}

\begin{exercise}[Tricky]
Find a sequence $\{ x_n \}$ such that for any $y \in \R$, there exists a
subsequence $\{ x_{n_i} \}$ converging to $y$.
\end{exercise}

\begin{exercise}[Easy]
Let $\{ x_n \}$ be a sequence and $x \in \R$.
Suppose for any $\epsilon > 0$, there is an $M$ such that for
all $n \geq M$, $\abs{x_n-x} \leq \epsilon$.  Show that $\lim\, x_n = x$.
\end{exercise}

\begin{exercise}[Easy]
Let $\{ x_n \}$ be a sequence and $x \in \R$ such that
there exists a $k \in \N$ such that for all $n \geq k$,
$x_n = x$.  Prove that $\{ x_n \}$ converges to $x$.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence and
define a sequence $\{ y_n \}$ by
$y_{2k} := x_{k^2}$ and $y_{2k-1} = x_k$ for all $k \in \N$.
Prove that $\{ x_n \}$ converges if and only if $\{ y_n \}$ converges.
Furthermore, prove that if they converge, then
$\lim\, x_n = \lim\, y_n$.
\end{exercise}

\begin{exercise}
Show that the 3-tail of the sequence defined by $x_n := \frac{n}{n^2+16}$ is
monotone decreasing.  Hint: Suppose $n \geq m \geq 4$ and consider the 
numerator of the expression $x_n-x_m$.
\end{exercise}

\begin{exercise}
Suppose that $\{ x_n \}$ is a sequence such that
the subsequences $\{ x_{2n} \}$, $\{ x_{2n-1} \}$, and
$\{ x_{3n} \}$ all converge.  Show that $\{ x_n \}$ is convergent.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Facts about limits of sequences}
\label{sec:factslimsseqs}

\sectionnotes{2--2.5 lectures, recursively defined sequences can safely be
skipped}

In this section we go over some basic results about the limits of
sequences.
We start by looking at how sequences interact with inequalities.

\subsection{Limits and inequalities}

A basic lemma about limits and inequalities is the so-called squeeze lemma.
It allows us to show convergence of sequences in difficult cases
if we find two other simpler convergent sequences that 
``squeeze'' the original sequence.

\begin{lemma}[Squeeze lemma]\index{squeeze lemma} \label{squeeze:lemma}
Let $\{ a_n \}$, 
$\{ b_n \}$, and 
$\{ x_n \}$ be sequences such that
\begin{equation*}
a_n \leq x_n \leq b_n \quad \text{ for all $n \in \N$} .
\end{equation*}
Suppose $\{ a_n \}$ and $\{ b_n \}$ converge and
\begin{equation*}
\lim_{n\to \infty} a_n
=
\lim_{n\to \infty} b_n .
\end{equation*}
Then $\{ x_n \}$ converges and
\begin{equation*}
\lim_{n\to \infty} x_n
=
\lim_{n\to \infty} a_n
=
\lim_{n\to \infty} b_n .
\end{equation*}
\end{lemma}

The intuitive idea of the proof is illustrated in
\figureref{figsqueeze}.
If $x$ is the limit of $a_n$ and $b_n$, then if they are both within
$\nicefrac{\epsilon}{3}$ of $x$, then the distance between $a_n$ and $b_n$
is at most $\nicefrac{2\epsilon}{3}$.  As $x_n$ is between $a_n$ and $b_n$
it is at most $\nicefrac{2\epsilon}{3}$ from $a_n$.  Since $a_n$ is
at most $\nicefrac{\epsilon}{3}$ away from $x$, then $x_n$ must be at
most $\epsilon$ away from $x$.  Let us follow through on this intuition
rigorously.
\begin{myfigureht}
%\begin{center}
\newcommand{\ltepsilon}{< \nicefrac{2\epsilon}{3} + \nicefrac{\epsilon}{3} =
\epsilon}
\subimport*{figures/}{figsqueeze.pdf_t}
\caption{Squeeze lemma proof in picture.\label{figsqueeze}}
%\end{center}
\end{myfigureht}

\begin{proof}
Let $x := \lim\, a_n = \lim\, b_n$.
Let $\epsilon > 0$ be given.

Find an $M_1$ such that for all $n \geq M_1$ we have
that $\abs{a_n-x} < \nicefrac{\epsilon}{3}$, and an $M_2$
such that for all $n \geq M_2$
we have $\abs{b_n-x} < \nicefrac{\epsilon}{3}$.  Set $M := \max \{M_1, M_2 \}$.
Suppose $n \geq M$.  We compute
\begin{equation*}
\begin{split}
\abs{x_n - a_n} = x_n-a_n & \leq b_n-a_n \\
& = \abs{b_n - x + x - a_n} \\
& \leq \abs{b_n - x} + \abs{x - a_n} \\
& < \frac{\epsilon}{3} + \frac{\epsilon}{3} = \frac{2\epsilon}{3} .
\end{split}
\end{equation*}
Armed with this information we estimate
\begin{equation*}
\begin{split}
\abs{x_n - x}
&= \abs{x_n - x + a_n - a_n}
\\
&\leq \abs{x_n - a_n} + \abs{a_n - x}
\\
& < \frac{2\epsilon}{3} +  \frac{\epsilon}{3} = \epsilon .
\end{split}
\end{equation*}
And we are done.
\end{proof}

\begin{example}
One application of
the \hyperref[squeeze:lemma]{squeeze lemma} is to compute limits of 
sequences using limits that are already known.  For example, suppose 
we have the sequence $\{ \frac{1}{n\sqrt{n}} \}$.
Since $\sqrt{n} \geq 1$ for all $n \in \N$, we have
\begin{equation*}
0 \leq \frac{1}{n\sqrt{n}} \leq \frac{1}{n}
\end{equation*}
for all $n \in \N$.  We already know $\lim \nicefrac{1}{n} = 0$. 
Hence, using
the constant sequence $\{ 0 \}$ and the sequence $\{ \nicefrac{1}{n} \}$ in the
squeeze lemma, we conclude
\begin{equation*}
\lim_{n\to\infty} \frac{1}{n\sqrt{n}} = 0 .
\end{equation*}
\end{example}

Limits also preserve inequalities.

\begin{lemma} \label{limandineq:lemma}
Let $\{ x_n \}$ and $\{ y_n \}$ be
convergent sequences and
\begin{equation*}
x_n \leq y_n ,
\end{equation*}
for all $n \in \N$.  Then
\begin{equation*}
\lim_{n\to\infty} x_n \leq
\lim_{n\to\infty} y_n .
\end{equation*}
\end{lemma}

\begin{proof}
Let $x := \lim\, x_n$ and $y := \lim\, y_n$. 
Let 
$\epsilon > 0$ be given.  Find an $M_1$ such that for all $n \geq M_1$
we have $\abs{x_n-x} < \nicefrac{\epsilon}{2}$.  Find an $M_2$ such that
for all $n \geq M_2$ we have
$\abs{y_n-y} < \nicefrac{\epsilon}{2}$.  In particular,
for some $n \geq \max\{ M_1, M_2 \}$ we have
$x-x_n < \nicefrac{\epsilon}{2}$ and
$y_n-y < \nicefrac{\epsilon}{2}$.  We add these inequalities to
obtain
\begin{equation*}
y_n-x_n+x-y < \epsilon, \qquad \text{or} \qquad
y_n-x_n < y-x+ \epsilon .
\end{equation*}
Since $x_n \leq y_n$ we have
$0 \leq y_n-x_n$ and hence $0 < y-x+ \epsilon$.
In other words
\begin{equation*}
x-y < \epsilon .
\end{equation*}
Because $\epsilon > 0$ was arbitrary we obtain
$x-y \leq 0$.
%, as
%we have seen that a nonnegative
%number less than any positive $\epsilon$ is zero.
Therefore $x \leq y$.
\end{proof}

The following corollary is
proved
using constant sequences in
\lemmaref{limandineq:lemma}.  The proof is left as an exercise.

\begin{samepage}
\begin{cor} \label{limandineq:cor}
{\ }
\begin{enumerate}[(i)]
\item Let $\{ x_n \}$ be a convergent sequence such that $x_n \geq 0$,
then
\begin{equation*}
\lim_{n\to\infty} x_n \geq 0.
\end{equation*}
\item
Let $a,b \in \R$ and
let $\{ x_n \}$ be a convergent sequence such that
\begin{equation*}
a \leq x_n \leq b ,
\end{equation*}
for all $n \in \N$.  Then
\begin{equation*}
a \leq \lim_{n\to\infty} x_n \leq b.
\end{equation*}
\end{enumerate}
\end{cor}
\end{samepage}

In \lemmaref{limandineq:lemma} and \corref{limandineq:cor} we cannot simply replace
all the non-strict inequalities with
strict inequalities.  For example,
let $x_n := \nicefrac{-1}{n}$ and $y_n := \nicefrac{1}{n}$.
Then $x_n < y_n$, $x_n < 0$,
and $y_n > 0$ for all $n$.  However, these inequalities are
not preserved by the limit operation as we have
$\lim\, x_n = \lim\, y_n = 0$.
The moral of this example is that strict inequalities may become non-strict
inequalities when limits are applied; if we know
$x_n < y_n$ for all $n$,
we may only conclude 
\begin{equation*}
\lim_{n \to \infty} x_n \leq
\lim_{n \to \infty} y_n .
\end{equation*}
This issue is a common source of errors.

\subsection{Continuity of algebraic operations}

Limits interact nicely with algebraic operations.

\begin{prop} \label{prop:contalg}
Let $\{ x_n \}$ and $\{ y_n \}$ be convergent sequences.
\begin{enumerate}[(i)]
\item \label{prop:contalg:i}
The sequence $\{ z_n \}$, where $z_n := x_n + y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n + y_n) = 
\lim_{n \to \infty} z_n = 
\lim_{n \to \infty} x_n + 
\lim_{n \to \infty} y_n .
\end{equation*}
\item \label{prop:contalg:ii}
The sequence $\{ z_n \}$, where $z_n := x_n - y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n - y_n) = 
\lim_{n \to \infty} z_n = 
\lim_{n \to \infty} x_n - 
\lim_{n \to \infty} y_n .
\end{equation*}
\item \label{prop:contalg:iii}
The sequence $\{ z_n \}$, where $z_n := x_n y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n y_n) = 
\lim_{n \to \infty} z_n = 
\left( \lim_{n \to \infty} x_n \right)
\left( \lim_{n \to \infty} y_n \right) .
\end{equation*}
\item \label{prop:contalg:iv}
If $\lim\, y_n \not= 0$ and $y_n \not= 0$ for all $n \in \N$, then
the sequence $\{ z_n \}$, where $z_n := \dfrac{x_n}{y_n}$, converges and
\begin{equation*}
\lim_{n \to \infty} \frac{x_n}{y_n} = 
\lim_{n \to \infty} z_n = 
%\frac{\lim_{n \to \infty} x_n}{\lim_{n \to \infty} y_n} .
\frac{\lim\, x_n}{\lim\, y_n} .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with \ref{prop:contalg:i}.
Suppose $\{ x_n \}$ and $\{ y_n \}$ are convergent sequences and
write $z_n := x_n + y_n$.  Let $x := \lim\, x_n$,
$y := \lim\, y_n$, and $z := x+y$.

Let $\epsilon > 0$ be given.  
Find an $M_1$ such that for all $n \geq M_1$
we have
$\abs{x_n - x} < \nicefrac{\epsilon}{2}$.  
Find an $M_2$ such that for all $n \geq M_2$
we have
$\abs{y_n - y} < \nicefrac{\epsilon}{2}$.  Take $M := \max \{ M_1, M_2 \}$.
For all $n \geq M$ we have
\begin{equation*}
\begin{split}
\abs{z_n - z} &=
\abs{(x_n+y_n) - (x+y)} \\
& =
\abs{x_n-x + y_n-y} \\
& \leq
\abs{x_n-x} + \abs{y_n-y} \\
& <
\frac{\epsilon}{2} +
\frac{\epsilon}{2}
= \epsilon.
\end{split}
\end{equation*}
Therefore \ref{prop:contalg:i} is proved.
Proof of \ref{prop:contalg:ii} is almost identical and is left as an
exercise.

Let us tackle 
\ref{prop:contalg:iii}.
Suppose again that $\{ x_n \}$ and $\{ y_n \}$ are convergent sequences and
write $z_n := x_n y_n$.  Let $x := \lim\, x_n$,
$y := \lim\, y_n$, and $z := xy$.

Let $\epsilon > 0$ be given.
As $\{ x_n \}$ is convergent, it is bounded.  Therefore, find
a $B >0$ such that $\abs{x_n} \leq B$ for all $n \in \N$.
Find an $M_1$ such that for all $n \geq M_1$
we have
$\abs{x_n - x} < \frac{\epsilon}{2(\abs{y}+1)}$.  
Find an $M_2$ such that for all $n \geq M_2$
we have
$\abs{y_n - y} < \frac{\epsilon}{2B}$.  Take $M := \max \{ M_1, M_2 \}$.
For all $n \geq M$ we have
\begin{equation*}
\begin{split}
\abs{z_n - z} &=
\abs{(x_ny_n) - (xy)} \\
& =
\abs{x_ny_n - (x+x_n-x_n)y} \\
& =
\abs{x_n(y_n -y) + (x_n - x)y} \\
& \leq
\abs{x_n(y_n -y)} + \abs{(x_n - x)y} \\
& =
\abs{x_n}\abs{y_n -y} + \abs{x_n - x}\abs{y} \\
& \leq
B\abs{y_n -y} + \abs{x_n - x}\abs{y} \\
& <
B\frac{\epsilon}{2B} + \frac{\epsilon}{2(\abs{y}+1)}\abs{y}
\\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}

Finally let us tackle
\ref{prop:contalg:iv}.  Instead of proving 
\ref{prop:contalg:iv} directly, we prove the following simpler claim:

\emph{Claim: If $\{ y_n \}$ is a convergent sequence such that
$\lim\, y_n \not= 0$ and $y_n \not= 0$ for all $n \in \N$, then
$\{ \nicefrac{1}{y_n} \}$ converges and}
\begin{equation*}
\lim_{n\to\infty} \frac{1}{y_n} = \frac{1}{\lim\, y_n}  .
\end{equation*}

Once the claim is proved, we take the sequence $\{ \nicefrac{1}{y_n} \}$,
multiply it by the sequence $\{ x_n \}$ and apply item
\ref{prop:contalg:iii}.

Proof of claim:  Let $\epsilon > 0$ be given.
Let $y := \lim\, y_n$.
Find an $M$ such that for all $n \geq M$
we have
\begin{equation*}
\abs{y_n - y} < \min \left\{ \abs{y}^2\frac{\epsilon}{2}, \, \frac{\abs{y}}{2}
\right\} .
\end{equation*}
Notice that we can make this claim as the right hand side is positive
because $\abs{y} \not= 0$.  Therefore for all $n \geq M$ we have
$\abs{y - y_n} < \frac{\abs{y}}{2}$, and so
\begin{equation*}
\abs{y} = 
\abs{y - y_n + y_n } \leq
\abs{y - y_n} + \abs{ y_n } < \frac{\abs{y}}{2} + \abs{y_n}.
\end{equation*}
Subtracting $\nicefrac{\abs{y}}{2}$ from both sides we obtain
$\nicefrac{\abs{y}}{2} < \abs{y_n}$, or in other words,
\begin{equation*}
\frac{1}{\abs{y_n}} < \frac{2}{\abs{y}} .
\end{equation*}
%or in other words $\abs{y_n} \geq \abs{y} - \abs{y - y_n}$.
%Now 
%$\abs{y_n - y} < \frac{\abs{y}}{2}$ implies 
%\begin{equation*}
%\abs{y} - \abs{y_n - y} > \frac{\abs{y}}{2} .
%\end{equation*}
%Therefore
%\begin{equation*}
%\abs{y_n} \geq \abs{y} - \abs{y - y_n} > \frac{\abs{y}}{2} ,
%\end{equation*}
%and consequently
%\begin{equation*}
%\frac{1}{\abs{y_n}} < \frac{2}{\abs{y}} .
%\end{equation*}
Now we finish the proof of the claim:
\begin{equation*}
\begin{split}
\abs{\frac{1}{y_n} - \frac{1}{y}} &=
\abs{\frac{y - y_n}{y y_n}} \\
& =
\frac{\abs{y - y_n}}{\abs{y} \abs{y_n}} \\
& <
\frac{\abs{y - y_n}}{\abs{y}} \, \frac{2}{\abs{y}} \\
& <
\frac{\abs{y}^2 \frac{\epsilon}{2}}{\abs{y}} \, \frac{2}{\abs{y}}
= \epsilon .
\end{split}
\end{equation*}
And we are done.
\end{proof}

By plugging in constant sequences, we get several easy corollaries.
If $c \in \R$ and $\{ x_n \}$ is a convergent sequence, then
for example
\begin{equation*}
\lim_{n \to \infty} c x_n = 
c \left( \lim_{n \to \infty} x_n \right) \qquad
\text{and}
\qquad
\lim_{n \to \infty} (c + x_n) = 
c + \lim_{n \to \infty} x_n .
\end{equation*}
Similarly with constant subtraction and division.

As we can take limits past multiplication we can show (exercise)
that $\lim\, x_n^k = {(\lim\, x_n)}^k$ for all $k \in \N$.
That is, we can take limits
past powers.  Let us see if we can do the same with roots.

\begin{prop}
Let $\{ x_n \}$ be a convergent sequence such
that $x_n \geq 0$.
Then
\begin{equation*}
\lim_{n\to\infty} \sqrt{x_n} =
\sqrt{ \lim_{n\to\infty} x_n } .
\end{equation*}
\end{prop}

Of course to even make this statement, we need to apply
\corref{limandineq:cor} to show
that
$\lim\, x_n \geq 0$, so that we can take the square root without
worry.

\begin{proof}
Let $\{ x_n \}$ be a convergent sequence and let $x := \lim\, x_n$.
As we just mentioned, $x \geq 0$.

First suppose $x=0$.  Let $\epsilon > 0$ be given.
Then there is an $M$ such that for all $n \geq M$ we have
$x_n = \abs{x_n} < \epsilon^2$, or in other words $\sqrt{x_n} < \epsilon$.
Hence
\begin{equation*}
\abs{\sqrt{x_n} - \sqrt{x}} =
\sqrt{x_n} < \epsilon.
\end{equation*}

Now suppose $x > 0$ (and hence $\sqrt{x} > 0$).
\begin{equation*}
\begin{split}
\abs{\sqrt{x_n}-\sqrt{x}} &= 
\abs{\frac{x_n-x}{\sqrt{x_n}+\sqrt{x}}} \\
&=
\frac{1}{\sqrt{x_n}+\sqrt{x}}
\abs{x_n-x} \\
& \leq
\frac{1}{\sqrt{x}}
\abs{x_n-x} .
\end{split}
\end{equation*}
We leave the rest of the proof to the reader.
\end{proof}

A similar proof works for the $k$th root.  That is, we also
obtain
$\lim\, x_n^{1/k} = {( \lim\, x_n )}^{1/k}$.  We leave this to the reader
as a challenging exercise.

We may also want to take the limit past the absolute value sign.
The converse of this proposition is not true, see
\exerciseref{exercise:absconv} part b).

\begin{prop}
If $\{ x_n \}$ is a convergent sequence, then $\{ \abs{x_n} \}$
is convergent and
\begin{equation*}
\lim_{n\to\infty} \abs{x_n} = 
\abs{\lim_{n\to\infty} x_n} .
\end{equation*}
\end{prop}

\begin{proof}
We simply note the reverse triangle inequality
\begin{equation*}
\big\lvert \abs{x_n} - \abs{x} \big\rvert \leq \abs{x_n-x} .
\end{equation*}
Hence if $\abs{x_n -x}$ can be made arbitrarily small, so can
$\big\lvert \abs{x_n} - \abs{x} \big\rvert$.
Details are left to the reader.
\end{proof}

Let us see an example putting the above propositions together.  Since
we know that $\lim \nicefrac{1}{n} = 0$, then
\begin{equation*}
\lim_{n\to \infty}
\abs{\sqrt{1 + \nicefrac{1}{n}} - \nicefrac{100}{n^2}} =  
\abs{\sqrt{1 + (\lim \nicefrac{1}{n})} - 100 (\lim \nicefrac{1}{n})(\lim
\nicefrac{1}{n})} = 1.
\end{equation*}
That is, the limit on the left hand side exists because the right hand
side exists.  You really should read the above equality from right to left.

\subsection{Recursively defined sequences}

Now that we know we can interchange limits and algebraic operations, we can
compute the limits of many sequences.
One such class are recursively defined sequences, that is, sequences where
the next number in the sequence is computed using a formula from a fixed number
of preceding elements in the sequence.

\begin{example}
Let $\{ x_n \}$ be defined by $x_1 := 2$ and
\begin{equation*}
x_{n+1} := x_n - \frac{x_n^2-2}{2x_n} .
\end{equation*}
We must first find out if this sequence is well defined; we must show we never
divide by zero.
Then we must find out if the sequence converges.  Only then
can we attempt to find the limit.

First let us prove 
$x_n$ exists and $x_n > 0$ for all $n$ (so the sequence is well defined
and bounded below).
Let us show this by \hyperref[induction:thm]{induction}.  We know that
$x_1 = 2 > 0$.  For the induction step, suppose $x_n > 0$.  Then
\begin{equation*}
x_{n+1} = x_n - \frac{x_n^2-2}{2x_n} =
\frac{2x_n^2 - x_n^2+2}{2x_n} =
\frac{x_n^2+2}{2x_n} .
\end{equation*}
It is always true that $x_n^2+2 > 0$,
and as
$x_n > 0$, then $\frac{x_n^2+2}{2x_n} > 0$ and hence $x_{n+1} > 0$.

Next let us
show that the sequence is monotone decreasing.  If we show that
$x_n^2-2 \geq 0$ for all $n$, then $x_{n+1} \leq x_n$ for all $n$.
Obviously $x_1^2-2 = 4-2 = 2 > 0$.  For an arbitrary $n$ we have 
\begin{equation*}
x_{n+1}^2-2 =
{\left( \frac{x_n^2+2}{2x_n} \right)}^2 - 2
=
\frac{x_n^4+4x_n^2+4 - 8x_n^2}{4x_n^2}
=
\frac{x_n^4-4x_n^2+4}{4x_n^2}
=
\frac{{\left( x_n^2-2 \right)}^2}{4x_n^2} .
\end{equation*}
Since any square is nonnegative,
$x_{n+1}^2-2 \geq 0$ for all $n$.  Therefore,
$\{ x_n \}$ is monotone decreasing and bounded ($x_n > 0$ for all $n$), and 
so the limit exists.  It remains to find the limit.

Write
\begin{equation*}
2x_nx_{n+1} = x_n^2+2 .
\end{equation*}
Since $\{ x_{n+1} \}$ is the 1-tail of $\{ x_n \}$, it converges to the
same limit.  Let us define $x := \lim\, x_n$.  Take the limit of
both sides to obtain
\begin{equation*}
2x^2 = x^2+2 ,
\end{equation*}
or $x^2 = 2$.  As $x_n > 0$ for all $n$ we get $x \geq 0$, and therefore $x = \sqrt{2}$.
\end{example}

You may have seen the above sequence before.  It is the
\emph{Newton's method}%
\footnote{%
Named after the English physicist and mathematician
\href{http://en.wikipedia.org/wiki/Isaac_Newton}{Isaac Newton} (1642 --
1726/7).}
for finding the square root of 2.  This method comes up very often in
practice and converges very rapidly.  Notice that we used the fact that
$x_1^2 -2 >0$, although it was not strictly needed to show convergence by
considering a tail of the sequence.
In fact the sequence converges as long as $x_1 \not= 0$, although with a negative $x_1$
we would arrive at $x=-\sqrt{2}$.  By replacing the 2 in the numerator we 
obtain the square root of any positive number.  These statements are left as
an exercise.

You should, however, be careful.  Before taking any limits, you must
make sure the sequence converges.  Let us see an example.

\begin{example}
Suppose $x_1 := 1$ and $x_{n+1} := x_n^2+x_n$.
If we blindly assumed that the limit exists (call it $x$), then we
would get the equation $x = x^2+x$, from which we might
conclude $x=0$.  However, it is not hard
to show that $\{ x_n \}$ is unbounded and therefore does not converge.

The thing to notice in this example is that the method still works, but
it depends on the initial value $x_1$.  If we set $x_1 := 0$,
then the sequence converges and the limit really is 0.
An entire branch of mathematics, called dynamics, deals precisely with these
issues.
See
\exerciseref{exercise:convergentinitialvalues}.
\end{example}

\subsection{Some convergence tests}

It is not always necessary to go back to the definition of convergence
to prove that a sequence is convergent.  We first give a simple convergence test.
The main idea is that 
$\{ x_n \}$ converges to $x$ if and only if 
$\{ \abs{ x_n - x } \}$ converges to zero.

\begin{prop} \label{convzero:prop}
Let $\{ x_n \}$ be a sequence. 
Suppose there is an $x \in \R$
and a convergent sequence $\{ a_n \}$
such that
\begin{equation*}
\lim_{n\to\infty} a_n = 0
\end{equation*}
and 
\begin{equation*}
\abs{x_n - x} \leq a_n
\end{equation*}
for all $n$.  Then $\{ x_n \}$ converges and $\lim\, x_n = x$.
\end{prop}

\begin{proof}
Let $\epsilon > 0$ be given.  Note that $a_n \geq 0$
for all $n$.  Find an $M \in \N$ such that for
all $n \geq M$ we have
$a_n = \abs{a_n - 0} < \epsilon$.  Then, for all $n \geq M$
we have
\begin{equation*}
\abs{x_n - x} \leq a_n < \epsilon . \qedhere
\end{equation*}
\end{proof}

As the proposition shows, to study when a sequence has a limit is 
the same as studying when another sequence goes to zero.
%However, it may not give us an easily applicable test for convergence.
In general it may be hard to decide if a sequence converges, but
for certain sequences there exist easy to apply tests that tell us
if the sequence converges or not.  Let us see one such test.
%For some special sequences we can test the convergence easily.
First let
us compute the limit of a very specific sequence.
\begin{prop}
Let $c > 0$.
\begin{enumerate}[(i)]
\item
If $c < 1$, then
\begin{equation*}
\lim_{n\to\infty} c^n = 0.
\end{equation*}
\item
If $c > 1$, then $\{ c^n \}$ is unbounded.
\end{enumerate}
\end{prop}

\begin{proof}
First suppose $c > 1$.  Write
$c = 1+r$ for some $r > 0$.  By \hyperref[induction:thm]{induction} (or using the binomial theorem
if you know it) we have Bernoulli's inequality (see also
\exerciseref{exercise:bernoulliineq}):
\begin{equation*}
c^n = {(1+r)}^n \geq 1+nr .
\end{equation*}
By the \hyperref[thm:arch:i]{Archimedean property}
of the real numbers, the sequence $\{ 1+nr \}$
is unbounded (for any number $B$, we find an $n \in \N$ such that $nr \geq
B-1$).  Therefore $c^n$ is unbounded.

Now suppose $c < 1$.  Write $c = \frac{1}{1+r}$, where $r > 0$.  Then
\begin{equation*}
c^n = \frac{1}{{(1+r)}^n} \leq
\frac{1}{1+nr} \leq \frac{1}{r} \frac{1}{n} .
\end{equation*}
As $\{ \frac{1}{n} \}$ converges to zero, so does
$\{ \frac{1}{r} \frac{1}{n} \}$.  Hence, $\{ c^n \}$ converges to zero.
\end{proof}

If we look at the above proposition, we note that the
ratio of the $(n+1)$th term and the $n$th term is $c$.  We 
generalize this simple result to a larger class of sequences.
The following lemma will come up again once we get to series.

\begin{lemma}[Ratio test for sequences]\index{ratio test for sequences}
\label{seq:ratiotest}
Let $\{ x_n \}$ be a sequence such that $x_n \not= 0$ for all $n$ and such that
the limit
\begin{equation*}
L := \lim_{n\to\infty} \frac{\abs{x_{n+1}}}{\abs{x_n}}
\end{equation*}
exists.
\begin{enumerate}[(i)]
\item
If $L < 1$, then $\{ x_n \}$ converges and $\lim\, x_n = 0$.
\item
If $L > 1$, then $\{ x_n \}$ is unbounded (hence diverges).
\end{enumerate}
\end{lemma}

If $L$ exists, but $L=1$, the lemma says nothing.  We cannot make any
conclusion based on that information alone.  For example,
the sequence $\{ \nicefrac{1}{n} \}$ converges to zero, but $L=1$.
The constant sequence $\{ 1 \}$ converges to 1, not zero, and 
$L=1$.  The sequence $\{ {(-1)}^n \}$ does not converge at all, and $L=1$ as
well.
Finally the sequence $\{  n \}$ is unbounded, yet again $L=1$.
The statement may be strengthened, see 
exercises \ref{exercise:strongerratiotest1} and
\ref{exercise:strongerratiotest2}.

\begin{proof}
Suppose $L < 1$.
As
$\frac{\abs{x_{n+1}}}{\abs{x_n}} \geq 0$ for all $n$, then $L \geq 0$.  Pick
$r$ such that $L < r < 1$.
We wish to compare the sequence to the sequence $r^n$.  The idea is that
while the sequence is not going to be less than $L$ eventually,
it will eventually be less than $r$, which is still less than 1.
The intuitive idea of the proof is illustrated in \figureref{figratseq}.
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figratseq.pdf_t}
\caption{Proof of ratio test in picture.  The short lines represent the
ratios 
$\frac{\abs{x_{n+1}}}{\abs{x_n}}$
approaching $L$.\label{figratseq}}
%\end{center}
\end{myfigureht}

As $r-L > 0$, there exists an $M \in \N$ such that for
all $n \geq M$ we have
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < r-L 
\end{equation*}
Therefore, for $n \geq M$,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} - L < r-L 
\qquad \text{or} \qquad
\frac{\abs{x_{n+1}}}{\abs{x_n}} < r .
\end{equation*}
For $n > M$ (that is for $n \geq M+1$)
write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
<
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
The sequence $\{ r^n \}$ converges to zero and hence 
$\abs{x_M} r^{-M} r^n$ converges to zero.  By \propref{convzero:prop},
the $M$-tail of 
$\{x_n\}$ converges to zero and therefore $\{x_n\}$ converges to zero.

Now suppose $L > 1$.  Pick
$r$ such that $1 < r < L$.  As $L-r > 0$,
there exists an $M \in \N$ such that for
all $n \geq M$
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < L-r .
\end{equation*}
Therefore,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} > r .
\end{equation*}
Again for $n > M$,
write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
>
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
The sequence $\{ r^n \}$ is unbounded (since $r > 1$), and therefore
$\{x_n\}$ cannot be bounded (if $\abs{x_n} \leq B$ for all $n$, then
$r^n < \frac{B}{\abs{x_M}} r^{M}$ for all $n$, which is impossible).
Consequently, $\{ x_n \}$ cannot converge.
\end{proof}

\begin{example}
A simple application of the above lemma is to prove 
\begin{equation*}
\lim_{n\to\infty} \frac{2^n}{n!} = 0 .
\end{equation*}

Proof:
Compute
\begin{equation*}
\frac{2^{n+1} / (n+1)!}{2^n/n!}
=
\frac{2^{n+1}}{2^n}\frac{n!}{(n+1)!}
=
\frac{2}{n+1} .
\end{equation*}
It is not hard to see that $\{ \frac{2}{n+1} \}$ converges to zero.
The conclusion follows by the lemma.
\end{example}

\begin{example} \label{example:nto1overn}
A more complicated (and useful) application of the above lemma is to prove 
\begin{equation*}
\lim_{n\to\infty} n^{1/n} = 1 .
\end{equation*}

Proof:
Let $\epsilon > 0$ be given.  Consider the sequence
$\{ \frac{n}{{(1+\epsilon)}^n} \}$.  Compute
\begin{equation*}
\frac{(n+1)/{(1+\epsilon)}^{n+1}}{n/{(1+\epsilon)}^{n}}
=
\frac{n+1}{n} \frac{1}{1+\epsilon} .
\end{equation*}
The limit of $\frac{n+1}{n} = 1+\frac{1}{n}$ as $n \to \infty$ is 1, and
\begin{equation*}
\lim_{n\to \infty} \frac{(n+1)/{(1+\epsilon)}^{n+1}}{n/{(1+\epsilon)}^{n}}
=
\frac{1}{1+\epsilon}  < 1 .
\end{equation*}
Therefore, $\{ \frac{n}{{(1+\epsilon)}^n} \}$ converges to 0.  In particular
there exists an $N$ such that for $n \geq N$, we have
$\frac{n}{{(1+\epsilon)}^n} < 1$, or 
$n < {(1+\epsilon)}^n$, or 
$n^{1/n} < 1+\epsilon$.  As $n \geq 1$, then $n^{1/n} \geq 1$, and
so $0 \leq n^{1/n}-1 < \epsilon$. Consequently,
$\lim\, n^{1/n} = 1$.
\end{example}

\subsection{Exercises}

\begin{exercise}
Prove \corref{limandineq:cor}.  Hint: Use constant sequences
and \lemmaref{limandineq:lemma}.
\end{exercise}

\begin{exercise}
Prove part \ref{prop:contalg:ii} of \propref{prop:contalg}.
\end{exercise}

\begin{exercise}
Prove that if $\{ x_n \}$ is a convergent sequence, $k \in \N$, then
\begin{equation*}
\lim_{n\to\infty} x_n^k = 
{\left( \lim_{n\to\infty} x_n \right)}^k .
\end{equation*}
Hint: Use \hyperref[induction:thm]{induction}.
\end{exercise}

\begin{exercise}
Suppose $x_1 := \frac{1}{2}$ and $x_{n+1} := x_n^2$.  Show that
$\{ x_n \}$ converges and find
$\lim\, x_n$.  Hint: You cannot divide by zero!
\end{exercise}

\begin{exercise}
Let $x_n := \frac{n-\cos(n)}{n}$.  Use the
\hyperref[squeeze:lemma]{squeeze lemma} to show that
$\{ x_n \}$ converges and find the limit.
\end{exercise}

\begin{exercise}
Let $x_n := \frac{1}{n^2}$ and $y_n := \frac{1}{n}$.  Define
$z_n := \frac{x_n}{y_n}$ and 
$w_n := \frac{y_n}{x_n}$.  Do $\{ z_n \}$ and $\{ w_n \}$
converge?  What are the limits?  Can you apply \propref{prop:contalg}?
Why or why not?
\end{exercise}

\begin{exercise}
True or false, prove or find a counterexample.  If $\{ x_n \}$ is a sequence
such that $\{ x_n^2 \}$ converges, then $\{ x_n \}$ converges.
\end{exercise}

\begin{exercise}
Show that
\begin{equation*}
\lim_{n\to\infty} \frac{n^2}{2^n} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a sequence and suppose for
some $x \in \R$, the limit
\begin{equation*}
L := \lim_{n \to \infty} \frac{\abs{x_{n+1}-x}}{\abs{x_n-x}}
\end{equation*}
exists and $L < 1$.  Show that $\{ x_n \}$ converges to $x$.
\end{exercise}

\begin{exercise}[Challenging]
Let $\{ x_n \}$ be a convergent sequence such
that $x_n \geq 0$ and $k \in \N$.
Then
\begin{equation*}
\lim_{n\to\infty} x_n^{1/k} =
{\left( \lim_{n\to\infty} x_n \right)}^{1/k} .
\end{equation*}
Hint: Find an expression $q$ such that $\frac{x_n^{1/k}-x^{1/k}}{x_n-x} =
\frac{1}{q}$.
\end{exercise}

\begin{exercise}
Let $r > 0$.  Show that starting with any $x_1 \not= 0$, the sequence
defined by
\begin{equation*}
x_{n+1} := x_n - \frac{x_n^2-r}{2x_n}
\end{equation*}
converges to $\sqrt{r}$ if $x_1 > 0$ and $-\sqrt{r}$ if $x_1 < 0$.
\end{exercise}

\begin{exercise}
a) Suppose $\{ a_n \}$ is a bounded sequence and $\{ b_n \}$ is a sequence
converging to 0. Show that $\{ a_n b_n \}$ converges to 0.\\
b) Find an example where $\{ a_n \}$ is unbounded, $\{ b_n \}$ converges to
0, and $\{ a_n b_n \}$ is not convergent.\\
c) Find an example where $\{ a_n \}$ is bounded, $\{ b_n \}$ converges to
some $x \not= 0$, and $\{ a_n b_n \}$ is not convergent.
\end{exercise}

\begin{exercise}[Easy] \label{exercise:strongerratiotest1}
Prove the following stronger version of \lemmaref{seq:ratiotest}, the ratio
test.  
Suppose $\{ x_n \}$ is a sequence such that $x_n \not= 0$ for all
$n$.
\\
a) Prove that if there exists an $r < 1$ and $M \in \N$ such that
for all $n \geq M$ we have
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} < r ,
\end{equation*}
then $\{ x_n \}$ converges to $0$.
\\
b) Prove that if there exists an $r > 1$ and $M \in \N$ such that
for all $n \geq M$ we have
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} > r ,
\end{equation*}
then $\{ x_n \}$ is unbounded.
\end{exercise}

\begin{exercise} \label{exercise:convergentinitialvalues}
Suppose $x_1 := c$ and $x_{n+1} := x_n^2+x_n$.
Show that $\{ x_n \}$ converges if and only if $-1 \leq c \leq 0$, in which
case it converges to 0.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Limit superior, limit inferior, and Bolzano--Weierstrass}
\label{sec:bw}

\sectionnotes{1--2 lectures, alternative proof of BW optional}

In this section we study bounded sequences and their subsequences.
In particular we define the so-called limit superior and limit inferior
of a bounded sequence and talk about limits of subsequences.
Furthermore, we prove the
Bolzano--Weierstrass theorem%
\footnote{%
Named after the Czech mathematician
\href{http://en.wikipedia.org/wiki/Bernard_Bolzano}{Bernhard Placidus Johann Nepomuk Bolzano}
(1781 -- 1848), and the German mathematician
\href{http://en.wikipedia.org/wiki/Karl_Weierstrass}{Karl Theodor Wilhelm Weierstrass}
(1815 -- 1897).}, which is an
indispensable tool in analysis.

We have seen that every convergent sequence is bounded,
although there exist many bounded divergent sequences.  For example,
the sequence $\{ {(-1)}^n \}$ is bounded,
but it is divergent.  All is not lost however and we can
still compute certain limits with a bounded divergent sequence.

\subsection{Upper and lower limits}

There are ways of creating monotone sequences out of any sequence, and
in this fashion we
get the so-called \emph{\myindex{limit superior}} and
\emph{\myindex{limit inferior}}.  These limits always exist for bounded
sequences.

If a sequence $\{ x_n \}$ is bounded, then 
the set $\{ x_k : k \in \N \}$ is bounded.  Then for every $n$
the set $\{ x_k : k \geq n \}$ is also bounded (as it is a subset).

\begin{defn} \label{liminflimsup:def}
Let $\{ x_n \}$ be a bounded sequence.  Define the sequences $\{ a_n \}$
and $\{ b_n \}$ by
$a_n := \sup \{ x_k : k \geq n \}$ and
$b_n := \inf \{ x_k : k \geq n \}$.  
%The
%sequence $\{ a_n \}$ is bounded monotone decreasing
%and $\{ b_n \}$ is bounded monotone increasing (more on this point below).
Define, if the limits exist,
\begin{align*}
\limsup_{n \to \infty} \, x_n & := \lim_{n \to \infty} a_n ,
\\
\liminf_{n \to \infty} \, x_n & := \lim_{n \to \infty} b_n .
\end{align*}
\end{defn}

For a bounded sequence, liminf and limsup always exist (see below).  It is possible
to define liminf and limsup for unbounded sequences if we allow $\infty$
and $-\infty$.  It is not hard to generalize the following results to
include unbounded sequences, however, we first restrict our attention to
bounded ones.

\begin{prop}
Let $\{ x_n \}$ be a bounded sequence.  Let $a_n$ and $b_n$ be as in
the definition above.
\begin{enumerate}[(i)]
\item
The
sequence $\{ a_n \}$ is bounded monotone decreasing
and $\{ b_n \}$ is bounded monotone increasing.  In particular,
$\liminf x_n$ and $\limsup x_n$ exist.
\item
$\displaystyle \limsup_{n \to \infty} \, x_n = \inf \{ a_n : n \in \N \}$
and
$\displaystyle \liminf_{n \to \infty} \, x_n = \sup \{ b_n : n \in \N \}$.
\item
$\displaystyle \liminf_{n \to \infty} \, x_n \leq
\limsup_{n \to \infty} \, x_n$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us see why $\{ a_n \}$ is a decreasing sequence.  As $a_n$ is the least upper
bound for $\{ x_k : k \geq n \}$, it is also
an upper bound for the subset $\{ x_k : k \geq (n+1) \}$.  Therefore
$a_{n+1}$, the least upper bound for
$\{ x_k : k \geq (n+1) \}$, has to be less than or equal to $a_n$,
that is,
$a_n \geq a_{n+1}$.  Similarly (an exercise), $b_n$ is an increasing
sequence.  It is left as an exercise to show that
if $x_n$ is bounded, then $a_n$ and $b_n$ must be bounded.

The second item in the proposition follows as the sequences
$\{ a_n \}$ and $\{ b_n \}$ are monotone.

For the third item, note that $b_n \leq a_n$, as the $\inf$ of a set
is less than or equal to its $\sup$.  The sequences $\{ a_n \}$ and $\{ b_n \}$
converge to the limsup and the liminf respectively.
Apply \lemmaref{limandineq:lemma} to obtain
\begin{equation*}
\lim_{n\to \infty} b_n \leq \lim_{n\to \infty} a_n.  \qedhere
\end{equation*}
%As $a_n$ is decreasing, then
%for every $k \leq n$ we have
%\begin{equation*}
%b_n \leq a_n \leq a_k.
%\end{equation*}
%Similarly as $b_n$ is increasing, then for every $k \geq n$ we have
%\begin{equation*}
%b_k \leq b_n \leq a_n .
%\end{equation*}
%In other words, $b_k \leq a_n$ for any pair of natural numbers $k$ and $n$.
%We have seen before that we can thus conclude that
%\begin{equation*}
%\sup \{ b_n : n \in \N \} \leq \sup \{ a_n : n \in \N \} .
%\end{equation*}
%By the first item of the proposition, we get
%\begin{equation*}
%\liminf_{n \to \infty} \, x_n \leq
%\limsup_{n \to \infty} \, x_n .
%\end{equation*}
\end{proof}
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sequence-limsupliminf_an_bn.eepic}
\caption{First 50 terms of an example sequence.  Terms $x_n$ of the sequence are
marked with dots (\raisebox{0.25ex}{\tiny$\bullet$}), $a_n$ are marked with
circles ($\circ$), and
$b_n$ are marked with diamonds ($\diamond$).\label{sequence-limsupliminf_an_bn}}
%\end{center}
\end{myfigureht}

\begin{example} \label{example:liminfsupex}
Let $\{ x_n \}$ be defined by
\begin{equation*}
x_n :=
\begin{cases}
\frac{n+1}{n} & \text{ if $n$ is odd,} \\
0 & \text{ if $n$ is even.}
\end{cases}
\end{equation*}
Let us compute the $\liminf$ and $\limsup$ of this sequence.  See also
\figureref{sequence-limsupliminf_an_bn-example}.  First the
limit inferior:
\begin{equation*}
\liminf_{n\to\infty} \, x_n = 
\lim_{n\to\infty}
\left(
\inf \{ x_k : k \geq n \}
\right)
=
\lim_{n\to\infty} 0 = 0 .
\end{equation*}
For the limit superior we write
\begin{equation*}
\limsup_{n\to\infty} \, x_n = 
\lim_{n\to\infty}
\left(
\sup \{ x_k : k \geq n \}
\right) .
\end{equation*}
It is not hard to see that
\begin{equation*}
\sup \{ x_k : k \geq n \} =
\begin{cases}
\frac{n+1}{n} & \text{ if $n$ is odd,} \\
\frac{n+2}{n+1} & \text{ if $n$ is even.}
\end{cases}
\end{equation*}
We leave it to the reader to show that the limit is 1.  That is,
\begin{equation*}
\limsup_{n\to\infty} \, x_n = 1 .
\end{equation*}
Do note that the sequence $\{ x_n \}$ is not a convergent sequence.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sequence-limsupliminf_an_bn-example.eepic}
\caption{First 20 terms of the sequence in \exampleref{example:liminfsupex}.
The marking is the same as in \figureref{sequence-limsupliminf_an_bn}.
\label{sequence-limsupliminf_an_bn-example}}
%\end{center}
\end{myfigureht}
\end{example}

We associate certain subsequences with $\limsup$ and $\liminf$.
It is important to notice that $\{ a_n \}$ and $\{ b_n \}$ are not
necessarily subsequences of $\{ x_n \}$, nor do they have to even 
consist of the same numbers.
For example, for the sequence $\{ \nicefrac{1}{n} \}$,
$b_n = 0$ for all $n \in N$.

\begin{thm} \label{subseqlimsupinf:thm}
If $\{ x_n \}$ is a bounded sequence, then there exists a subsequence
$\{ x_{n_k} \}$ such that
\begin{equation*}
\lim_{k\to \infty} x_{n_k} = \limsup_{n \to \infty} \, x_n .
\end{equation*}
Similarly, there exists a (perhaps different) subsequence
$\{ x_{m_k} \}$ such that
\begin{equation*}
\lim_{k\to \infty} x_{m_k} = \liminf_{n \to \infty} \, x_n .
\end{equation*}
\end{thm}

\begin{proof}
Define $a_n := \sup \{ x_k : k \geq n \}$.
Write
$x := \limsup \, x_n = \lim\, a_n$.  Define the subsequence as follows.
Pick $n_1 := 1$ and work inductively.  Suppose we have
defined the subsequence until $n_k$ for some $k$.  Now pick some $m > n_k$
such that
\begin{equation*}
a_{(n_k+1)} - x_m < \frac{1}{k+1} .
\end{equation*}
We can do this as $a_{(n_k+1)}$ is a supremum of the
set $\{ x_n : n \geq n_k + 1 \}$ and hence there are elements
of the sequence arbitrarily close (or even possibly equal) to the supremum.
Set $n_{k+1} :=  m$.  The subsequence $\{ x_{n_k} \}$ is defined.  Next we
need to prove that it converges and has the right limit.

Note that
$a_{(n_{k-1}+1)} \geq a_{n_k}$ (why?) and that $a_{n_{k}} \geq x_{n_k}$.
Therefore, for every $k \geq 2$ we have
\begin{equation*}
\begin{split}
\abs{a_{n_k} - x_{n_k}} & = 
a_{n_k} - x_{n_k}
\\
& \leq
a_{(n_{k-1}+1)} - x_{n_k}
\\
& < \frac{1}{k} .
\end{split}
\end{equation*}

Let us show that $\{ x_{n_k} \}$ converges to $x$.
Note that the subsequence need not be monotone.  Let $\epsilon > 0$ be given.
As $\{ a_n \}$ converges to $x$, then the subsequence
$\{ a_{n_k} \}$ converges to $x$.
Thus there exists an $M_1 \in \N$
such that for all $k \geq M_1$ we have
\begin{equation*}
\abs{a_{n_k} - x} < \frac{\epsilon}{2} .
\end{equation*}
Find an $M_2 \in \N$ such that
\begin{equation*}
\frac{1}{M_2} \leq \frac{\epsilon}{2}.
\end{equation*}
Take $M := \max \{M_1 , M_2 , 2 \}$ and compute.  For all $k \geq M$
we have
\begin{equation*}
\begin{split}
\abs{x- x_{n_k}} & =
\abs{a_{n_k} - x_{n_k} + x - a_{n_k}}
\\
& \leq \abs{a_{n_k} - x_{n_k}} + \abs{x - a_{n_k}}
\\
& < \frac{1}{k} + \frac{\epsilon}{2}
\\
& \leq \frac{1}{M_2} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2} +
\frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}

We leave the statement for $\liminf$ as an exercise.
\end{proof}

\subsection{Using limit inferior and limit superior}

The advantage of $\liminf$ and $\limsup$ is that we can always write them
down for any (bounded) sequence.
If we could somehow compute them, we could also compute the limit of the
sequence if it exists, or show that the sequence diverges.
Working with $\liminf$ and $\limsup$ is a
little bit like working with limits, although there are subtle differences.

\begin{prop} \label{liminfsupconv:prop}
Let $\{ x_n \}$ be a bounded sequence.  Then $\{ x_n \}$ converges
if and only if
\begin{equation*}
\liminf_{n\to \infty} \, x_n = 
\limsup_{n\to \infty} \, x_n.
\end{equation*}
Furthermore, if $\{ x_n \}$ converges, then
\begin{equation*}
\lim_{n\to \infty} x_n = 
\liminf_{n\to \infty} \, x_n = 
\limsup_{n\to \infty} \, x_n.
\end{equation*}
\end{prop}

\begin{proof}
Define $a_n$ and $b_n$ as in \defnref{liminflimsup:def}.
Note that
\begin{equation*}
b_n \leq x_n \leq a_n .
\end{equation*}
If 
$\liminf \, x_n = \limsup \, x_n$, then we know that $\{ a_n \}$ and $\{ b_n \}$
have limits and that these two limits are the same.  By the squeeze lemma
(\lemmaref{squeeze:lemma}), $\{ x_n \}$ converges and
\begin{equation*}
\lim_{n\to \infty} b_n
=
\lim_{n\to \infty} x_n
=
\lim_{n\to \infty} a_n .
\end{equation*}

Now suppose $\{ x_n \}$ converges to $x$.
We know by
\thmref{subseqlimsupinf:thm}
that there exists a subsequence $\{ x_{n_k} \}$
that converges to $\limsup \, x_n$.
As $\{ x_n \}$ converges to $x$,
every subsequence converges to $x$ and
therefore $\limsup \, x_n = \lim\, x_{n_k} = x$.  Similarly $\liminf \, x_n = x$.
\end{proof}

Limit superior and limit inferior behave nicely
with subsequences.

\begin{prop} \label{prop:subseqslimsupinf}
Suppose $\{ x_n \}$ is a bounded sequence and
$\{ x_{n_k} \}$ is a subsequence.  Then
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\liminf_{k\to\infty} \, x_{n_k} \leq
\limsup_{k\to\infty} \, x_{n_k} \leq
\limsup_{n\to\infty} \, x_n .
\end{equation*}
\end{prop}

\begin{proof}
The middle inequality has been proved already.  We will prove the third
inequality, and leave the first inequality as an exercise.

We want to prove that
$\limsup \, x_{n_k} \leq \limsup \, x_n$.  Define
$a_j := \sup \{ x_k : k \geq j \}$ 
as usual.
Also define
$c_j := \sup \{ x_{n_k} : k \geq j \}$.
It is not true that $c_j$ is necessarily a subsequence of $a_j$.  However,
as $n_k \geq k$ for all $k$, we have that
$\{ x_{n_k} : k \geq j \} \subset \{ x_k : k \geq j \}$.
A supremum of a subset is less than or equal to the supremum of the
set and therefore
\begin{equation*}
c_j \leq a_j .
\end{equation*}
We apply \lemmaref{limandineq:lemma} to conclude 
\begin{equation*}
\lim_{j\to\infty} c_j \leq \lim_{j\to\infty} a_j ,
\end{equation*}
which is the desired conclusion.
\end{proof}

Limit superior and limit inferior
are the largest and smallest
subsequential limits.  If the subsequence in the previous
proposition is convergent, then we have that
$\liminf \, x_{n_k} = \lim\, x_{n_k} = \limsup \, x_{n_k}$.  Therefore,
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\lim_{k\to\infty} x_{n_k} \leq
\limsup_{n\to\infty} \, x_n .
\end{equation*}

Similarly we get the following useful test for convergence
of a bounded sequence.  We leave the proof as an exercise.

\begin{prop} \label{seqconvsubseqconv:prop}
A bounded sequence $\{ x_n \}$ is convergent and converges to $x$
if and only if
every convergent subsequence
$\{ x_{n_k} \}$ converges to $x$.
\end{prop}

\subsection{Bolzano--Weierstrass theorem}

While it is not true that a bounded sequence is convergent, the
Bolzano--Weierstrass theorem tells us that we can at least find a convergent
subsequence.
The version of Bolzano--Weierstrass 
that we present in this section is the Bolzano--Weierstrass for
sequences.

\begin{thm}[Bolzano--Weierstrass]\index{Bolzano--Weierstrass theorem}\label{thm:bwseq}
Suppose a sequence $\{ x_n \}$ of real numbers is bounded.
Then there exists a convergent subsequence $\{ x_{n_i} \}$.
\end{thm}

\begin{proof}
We use \thmref{subseqlimsupinf:thm}.  It says that there exists
a subsequence whose limit is $\limsup \, x_n$.
\end{proof}

The reader might complain right now that 
\thmref{subseqlimsupinf:thm} is strictly stronger than the
Bolzano--Weierstrass theorem as presented above.  That is true.
However, 
\thmref{subseqlimsupinf:thm} only applies to the real line, but
Bolzano--Weierstrass applies in more general contexts (that is, in $\R^n$)
with pretty much the exact same statement.

As the theorem is so important to analysis, we present an explicit
proof.
The following proof generalizes more easily to different contexts.

\begin{proof}[Alternate proof of Bolzano--Weierstrass]
As the sequence is bounded, then there exist two numbers $a_1 < b_1$
such that $a_1 \leq x_n \leq b_1$ for all $n \in \N$.

We will define a subsequence $\{ x_{n_i} \}$ and two
sequences $\{ a_i \}$ and $\{ b_i \}$, such that
$\{ a_i \}$ is monotone increasing, $\{ b_i \}$ is monotone decreasing,
$a_i \leq x_{n_i} \leq b_i$ and such that $\lim\, a_i = \lim\, b_i$.   That
$x_{n_i}$ converges follows by the \hyperref[squeeze:lemma]{squeeze lemma}.

We define the sequences inductively.  We will always have that $a_i < b_i$,
and that $x_n \in [a_i,b_i]$ for infinitely many
$n \in \N$.
We have already defined $a_1$ and $b_1$.  We take $n_1 := 1$, that is
$x_{n_1} = x_1$.

Suppose that up to some $k \in \N$
we have defined the subsequence $x_{n_1}, x_{n_2}, \ldots,
x_{n_k}$, and the sequences $a_1,a_2,\ldots,a_k$
and $b_1,b_2,\ldots,b_k$.
Let $y := \frac{a_k+b_k}{2}$.
Clearly
$a_k < y < b_k$.  If there exist infinitely many $j \in \N$
such that $x_j \in [a_k,y]$, then set $a_{k+1} := a_k$, $b_{k+1} := y$,
and pick $n_{k+1} > n_{k}$
such that $x_{n_{k+1}} \in [a_k,y]$.  If there are not infinitely many 
$j$ such that 
$x_j \in [a_k,y]$, then it must be true that there are infinitely many $j \in
\N$ such that 
$x_j \in [y,b_k]$.  In this case pick $a_{k+1} := y$, $b_{k+1} := b_k$,
and pick $n_{k+1} > n_{k}$
such that $x_{n_{k+1}} \in [y,b_k]$.

We now have the sequences defined.  What is left to prove is that
$\lim\, a_i = \lim\, b_i$.  The limits exist as the sequences
are monotone.  In the construction,
$b_i - a_i$ is cut in half in each step.  Therefore
$b_{i+1} - a_{i+1} = \frac{b_i-a_i}{2}$.  By
\hyperref[induction:thm]{induction},
\begin{equation*}
b_i - a_i = \frac{b_1-a_1}{2^{i-1}} .
\end{equation*}

Let $x := \lim\, a_i$.  As $\{ a_i \}$ is monotone,
\begin{equation*}
x = \sup \{ a_i : i \in \N \} .
\end{equation*}
Let $y := \lim\, b_i = \inf \{ b_i : i \in \N \}$.  Since $a_i < b_i$ for
all $i$, then $y \leq x$.
As the sequences are monotone, then
for any $i$ we have (why?)
\begin{equation*}
y-x \leq b_i-a_i = \frac{b_1-a_1}{2^{i-1}} .
\end{equation*}
Because $\frac{b_1-a_1}{2^{i-1}}$ is arbitrarily small and $y-x \geq 0$,
we have $y-x = 0$.  Finish by the \hyperref[squeeze:lemma]{squeeze lemma}.
\end{proof}

Yet another proof of the Bolzano--Weierstrass theorem is to show the
following claim,
which is left as a challenging exercise.
\emph{Claim: Every sequence has a monotone subsequence}.

\subsection{Infinite limits}

Just as for infima and suprema, it is possible to allow certain
limits, be infinite.  That is, we write $\lim \, x_n = \infty$ or
$\lim \, x_n = -\infty$ for certain divergent sequences.

\begin{defn}
We say
$\{ x_n \}$ \emph{\myindex{diverges to infinity}}%
\footnote{Sometimes it is said that $\{ x_n \}$ \emph{converges to infinity}.}
if for every $K \in
\R$, there exists an $M \in \N$ such that for all $n \geq M$ we have $x_n >
K$.  In this case we write
\begin{equation*}
\lim_{n \to \infty} x_n := \infty .  
\end{equation*}
Similarly
if for every $K \in \R$ there exists an $M \in \N$ such that
for all $n \geq M$ we have $x_n < K$, we say $\{ x_n \}$
\emph{\myindex{diverges to minus infinity}} and we write
\begin{equation*}
\lim_{n \to \infty} x_n := -\infty .  
\end{equation*}
\end{defn}

With this definition and allowing $\infty$ and $-\infty$,
we can write $\lim \, x_n$ for any monotone sequence.

\begin{prop} \label{prop:unboundedmonotone}
Suppose $\{ x_n \}$ is a monotone unbounded sequence.  Then
\begin{equation*}
\lim_{n \to \infty} x_n =
\begin{cases}
\infty & \text{ if $\{x_n \}$ is increasing,} \\
-\infty & \text{ if $\{x_n \}$ is decreasing.}
\end{cases}
\end{equation*}
\end{prop}

\begin{proof}
The case of monotone increasing follows from
\exerciseref{exercise:infseqlimlims} part c).  Let us do
monotone decreasing.  Suppose $\{x_n\}$ is decreasing and unbounded,
that is,
for every $K \in \R$, there is an $M \in \N$ such that $x_M < K$.
By monotonicity $x_n \leq x_M < K$ for all $n \geq M$.   Therefore
$\lim \, x_n = -\infty$.
%Similarly when $\{ x_n \}$ is monotone decreasing and unbounded.
\end{proof}

\begin{example}
%With this definition,
\begin{equation*}
\lim_{n\to \infty} n = \infty,
\qquad 
\lim_{n\to \infty} n^2 = \infty,
\qquad 
\lim_{n\to \infty} -n = -\infty.
\end{equation*}
We leave verification to the reader.
\end{example}

We may also allow $\liminf$ and $\limsup$ to take on
the values $\infty$ and $-\infty$, so that
we can apply $\liminf$ and $\limsup$
to absolutely any sequence, not just
a bounded one.   Unfortunately the sequences $\{ a_n \}$ and $\{ b_n \}$
are not sequences of real numbers but of extended real numbers.  In
particular, $a_n$ can equal $\infty$ for some $n$, and $b_n$ can equal
$-\infty$.  So we have no definition for the limits.
But since the extended real numbers are still an ordered set, we
can take suprema and infima.

\begin{defn}
Let $\{ x_n \}$ be an unbounded sequence of real numbers.  Define 
sequences of extended real numbers by
$a_n := \sup \{ x_k : k \geq n \}$ and
$b_n := \inf \{ x_k : k \geq n \}$.  Define
\begin{equation*}
\limsup_{n \to \infty} \, x_n := \inf \{ a_n : n \in \N \}, \qquad \text{and} \qquad
\liminf_{n \to \infty} \, x_n := \sup \{ b_n : n \in \N \}.
\end{equation*}
\end{defn}

This definition agrees with the definition for bounded
sequences whenever $\lim a_n$ or $\lim b_n$ makes sense including
possibly $\infty$ and $-\infty$.

\begin{prop}
Let $\{ x_n \}$ be an unbounded sequence.  Define 
$\{ a_n \}$ and $\{ b_n \}$ as above.
Then $\{ a_n \}$ is decreasing, and $\{ b_n \}$ is increasing.
If $a_n$ is a real number for every $n$, then
$\limsup \, x_n = \lim \, a_n$. 
If $b_n$ is a real number for every $n$, then
$\liminf \, x_n = \lim \, b_n$.
\end{prop}

\begin{proof}
As before,
$a_n = \sup \{ x_k : k \geq n \} \geq \sup \{ x_k : k \geq n+1 \} =
a_{n+1}$.  So $\{ a_n \}$ is decreasing. Similarly, $\{ b_n \}$ is increasing.

If the sequence $\{ a_n \}$ is a sequence of real numbers then
$\lim a_n = \inf \{ a_n : n \in \N \}$.  This follows from
\propref{prop:monotoneconv} if $\{ a_n \}$ is bounded and \propref{prop:unboundedmonotone} if $\{a_n \}$
is unbounded.  And similarly for $\{ b_n \}$.
\end{proof}

The definition behaves as expected with
$\limsup$ and $\liminf$, see exercises \ref{exercise:infseqlimex}
and \ref{exercise:infseqlimlims}.

\begin{example}
Suppose 
$x_n := 0$ for odd $n$ and $x_n := n$ for even $n$.
Then $a_n = \infty$ for every $n$, since for any $M$,
there exists an even $k$ such that $x_k = k \geq M$.
On the other hand, $b_n = 0$ for all $n$, as
for any $n$,
$\{ b_k : k \geq n \}$ consists of $0$ and nonnegative numbers.
So,
\begin{equation*}
\lim_{n\to \infty} x_n \quad \text{does not exist},
\qquad 
\limsup_{n\to \infty} \, x_n = \infty ,
\qquad 
\liminf_{n\to \infty} \, x_n = 0.
\end{equation*}
\end{example}

\subsection{Exercises}

\begin{exercise}
Suppose $\{ x_n \}$ is a bounded sequence.  Define $a_n$ and
$b_n$ as in \defnref{liminflimsup:def}.  Show that $\{ a_n \}$
and $\{ b_n \}$ are bounded.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a bounded sequence.
Define $b_n$ as in \defnref{liminflimsup:def}.  Show that
$\{ b_n \}$ is an increasing sequence.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:subseqslimsupinf}.  That is,
suppose $\{ x_n \}$ is a bounded sequence and
$\{ x_{n_k} \}$ is a subsequence.  Prove
$\displaystyle \liminf_{n\to\infty}\, x_n \leq
\liminf_{k\to\infty}\, x_{n_k}$.
\end{exercise}

\begin{exercise}
Prove \propref{seqconvsubseqconv:prop}.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
\item
Let $x_n := \dfrac{{(-1)}^n}{n}$, find $\limsup \, x_n$ and $\liminf \, x_n$.
\item
Let $x_n := \dfrac{(n-1){(-1)}^n}{n}$, find $\limsup \, x_n$ and $\liminf \, x_n$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences such that
$x_n \leq y_n$ for all $n$.  Then show that
\begin{equation*}
\limsup_{n\to\infty} \, x_n \leq
\limsup_{n\to\infty} \, y_n
\end{equation*}
and
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\liminf_{n\to\infty} \, y_n .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences.
\begin{enumerate}[a)]
\item
Show that $\{ x_n + y_n \}$ is bounded.
\item
Show that
\begin{equation*}
(\liminf_{n\to \infty}\, x_n)
+
(\liminf_{n\to \infty}\, y_n)
\leq
\liminf_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: Find a subsequence $\{ x_{n_i}+y_{n_i} \}$ of $\{ x_n + y_n \}$
that converges.
Then find a subsequence $\{ x_{n_{m_i}} \}$ of $\{ x_{n_i} \}$ that converges.
Then apply what you know about limits.
\item
Find an explicit $\{ x_n \}$ and $\{ y_n \}$ such that
\begin{equation*}
(\liminf_{n\to \infty}\, x_n)
+
(\liminf_{n\to \infty}\, y_n)
<
\liminf_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: Look for examples that do not have a limit.
\end{enumerate}
\end{exercise}

\begin{samepage}
\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences (from the previous
exercise we know that $\{ x_n + y_n \}$ is bounded).
\begin{enumerate}[a)]
\item
Show that
\begin{equation*}
(\limsup_{n\to \infty}\, x_n)
+
(\limsup_{n\to \infty}\, y_n)
\geq
\limsup_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: See previous exercise.
\item
Find an explicit $\{ x_n \}$ and $\{ y_n \}$ such that
\begin{equation*}
(\limsup_{n\to \infty}\, x_n)
+
(\limsup_{n\to \infty}\, y_n)
>
\limsup_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: See previous exercise.
\end{enumerate}
\end{exercise}
\end{samepage}

\begin{exercise}
If $S \subset \R$ is a set, then $x \in \R$ is a \emph{\myindex{cluster
point}}
if for every $\epsilon > 0$, the set $(x-\epsilon,x+\epsilon) \cap S
\setminus \{ x \}$ is not empty.  That is, if there are points of $S$
arbitrarily close to $x$.
For example, $S := \{ \nicefrac{1}{n} : n \in \N \}$ has a unique (only
one) cluster point $0$, but $0 \notin S$.
Prove the following version of the Bolzano--Weierstrass theorem:

\medskip

\noindent
\emph{\textbf{Theorem.} Let $S \subset \R$ be a bounded infinite set,
then there exists at least one cluster point of $S$}.

\medskip

Hint: If $S$ is infinite, then $S$ contains a countably infinite subset.
That is, there is a sequence $\{ x_n \}$ of distinct numbers in $S$.
\end{exercise}

\begin{exercise}[Challenging]
a)
Prove that any sequence contains a monotone subsequence.
Hint: Call $n \in \N$ a \emph{peak} if $a_m \leq a_n$ for all $m \geq n$.  
There are two possibilities: Either the sequence has at most finitely many
peaks,
or it has infinitely many peaks.
\\ \nopagebreak
b)~Conclude the Bolzano--Weierstrass theorem.
\end{exercise}

\begin{exercise}
Prove a stronger version of \propref{seqconvsubseqconv:prop}.
Suppose $\{ x_n \}$ is a sequence such that every subsequence $\{
x_{n_i} \}$ has a subsequence
$\{ x_{n_{m_i}} \}$ that converges to $x$.\\
a)~First show that $\{ x_n \}$ is
bounded.\\
b)~Now show that $\{ x_n \}$ converges to $x$.
\end{exercise}

\begin{exercise}
Let $\{x_n\}$ be a bounded sequence.
\\
a) Prove that there exists an $s$ such that for any $r > s$ there exists 
an $M \in \N$ such that for all $n \geq M$ we have
$x_n < r$.
\\
b) If $s$ is a number as in a), then prove $\limsup \, x_n \leq s$.
\\
c) Show that if $S$ is the set of all $s$ as in a), then
$\limsup \, x_n = \inf \, S$.
\end{exercise}

\begin{exercise}[Easy] \label{exercise:infseqlimex}
Suppose $\{ x_n \}$ is such that $\liminf \, x_n = -\infty$, $\limsup \, x_n
= \infty$.\\
a) Show that $\{ x_n \}$ is not convergent, and also
that neither $\lim \, x_n = \infty$ nor $\lim \, x_n =
-\infty$
is true.\\
b) Find an example of such a sequence.
\end{exercise}

\begin{exercise} \label{exercise:infseqlimlims}
Let $\{ x_n \}$ be a sequence.\\
a) Show that
$\lim \, x_n = \infty$ if and only if $\liminf \, x_n = \infty$.\\
b) Then show that $\lim \, x_n = - \infty$ if and only if $\limsup \, x_n =
-\infty$.\\
c) If $\{ x_n \}$ is monotone increasing, show that either
$\lim \, x_n$ exists and is finite or $\lim \, x_n = \infty$.  In either
case, $\lim \, x_n = \sup \{ x_n : n \in \N \}$.
\end{exercise}

\begin{exercise} \label{exercise:strongerratiotest2}
Prove the following stronger version of \lemmaref{seq:ratiotest}, the ratio
test.
Suppose $\{ x_n \}$ is a sequence such that $x_n \not= 0$ for all
$n$.
\\
a) Prove that if
\begin{equation*}
\limsup_{n \to \infty} \frac{\abs{x_{n+1}}}{\abs{x_n}} < 1 ,
\end{equation*}
then $\{ x_n \}$ converges to $0$.
\\
b) Prove that if
\begin{equation*}
\liminf_{n \to \infty} \frac{\abs{x_{n+1}}}{\abs{x_n}} > 1 ,
\end{equation*}
then $\{ x_n \}$ is unbounded.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a bounded sequence, $a_n := \sup \{ x_k : k \geq n \}$
as before.  Suppose that for some $\ell \in \N$, $a_\ell \notin \{ x_n : n \in
\N \}$.  Then show that $a_j = a_\ell$ for all $j \leq \ell$, and hence
$\limsup x_n = a_\ell$.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a sequence,
and $a_n := \sup \{ x_k : k \geq n \}$ and
$b_n := \sup \{ x_k : k \geq n \}$ as before.
\\
a) Prove that if $a_\ell = \infty$ for some $\ell \in \N$, then 
$\limsup x_n = \infty$.
\\
b) Prove that if $b_\ell = -\infty$ for some $\ell \in \N$, then 
$\liminf x_n = -\infty$.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a sequence
such that both $\liminf x_n$ and
$\limsup x_n$  are finite.  Prove that $\{ x_n \}$ is bounded.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a bounded sequence, and $\epsilon > 0$ is given.
Prove that there exists an $M$ such that for all $n \geq M$ we have
\begin{equation*}
x_n - \limsup x_n < \epsilon \qquad \text{and} \qquad
\liminf x_n - x_n < \epsilon .
\end{equation*}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Cauchy sequences}
\label{sec:cauchy}

\sectionnotes{0.5--1 lecture}

Often we wish to describe a certain number 
by a sequence that converges to it.  In this case, it is impossible to use the
number itself in the proof that the sequence converges.
It would be nice if we could check for convergence without knowing
the limit.

\begin{defn}
A sequence $\{ x_n \}$ is a \emph{\myindex{Cauchy sequence}}%
\footnote{%
Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Cauchy}{Augustin-Louis Cauchy} (1789--1857).} if
for every $\epsilon > 0$ there exists an $M \in \N$ such that
for all $n \geq M$ and all $k \geq M$ we have
\begin{equation*}
\abs{x_n - x_k} < \epsilon .
\end{equation*}
\end{defn}

Intuitively what it means is that the terms of the sequence are eventually
arbitrarily close to each other.  We would expect such a sequence to be
convergent.  It turns out that is true because $\R$ has the
\hyperref[defn:lub]{least-upper-bound property}.  First, let us look at some examples.

\begin{example}
The sequence $\{ \nicefrac{1}{n} \}$ is a Cauchy sequence.

Proof:  Given $\epsilon > 0$, find $M$ such that
$M > \nicefrac{2}{\epsilon}$.  Then for $n,k \geq M$
we have that $\nicefrac{1}{n} < \nicefrac{\epsilon}{2}$
and
$\nicefrac{1}{k} < \nicefrac{\epsilon}{2}$.  Therefore for $n, k \geq M$
we have
\begin{equation*}
\abs{\frac{1}{n} - \frac{1}{k}}
\leq
\abs{\frac{1}{n}} + \abs{\frac{1}{k}}
< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
\end{equation*}
\end{example}

\begin{example}
The sequence $\{ \frac{n+1}{n} \}$ is a Cauchy sequence.

Proof:  Given $\epsilon > 0$, find $M$ such that
$M > \nicefrac{2}{\epsilon}$.  Then for $n,k \geq M$
we have that $\nicefrac{1}{n} < \nicefrac{\epsilon}{2}$
and
$\nicefrac{1}{k} < \nicefrac{\epsilon}{2}$.  Therefore for $n, k \geq M$
we have
\begin{equation*}
\begin{split}
\abs{\frac{n+1}{n} - \frac{k+1}{k}}
& =
\abs{\frac{k(n+1)-n(k+1)}{nk}}
\\
& =
\abs{\frac{kn+k-nk-n}{nk}}
\\
& =
\abs{\frac{k-n}{nk}}
\\
& \leq
\abs{\frac{k}{nk}}
+
\abs{\frac{-n}{nk}}
\\
& = \frac{1}{n} + \frac{1}{k}
< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
\end{example}

\begin{prop}
A Cauchy sequence is bounded.
\end{prop}

\begin{proof}
Suppose $\{ x_n \}$ is Cauchy.  Pick $M$ such that for all
$n,k \geq M$ we have $\abs{x_n-x_k} < 1$.  In particular, we have
that for all $n \geq M$
\begin{equation*}
\abs{x_n - x_M} < 1 .
\end{equation*}
Or by the reverse triangle inequality,
$\abs{x_n} - \abs{x_M} \leq \abs{x_n - x_M} < 1$.  Hence for $n \geq M$
we have
\begin{equation*}
\abs{x_n} < 1 + \abs{x_M}.
\end{equation*}
Let
\begin{equation*}
B := \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}}, 1+ \abs{x_M} \} .
\end{equation*}
Then $\abs{x_n} \leq B$ for all $n \in \N$.
\end{proof}

\begin{thm}
A sequence of real numbers is Cauchy if and only if it converges.
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given and
suppose $\{ x_n \}$ converges to $x$.  Then there 
exists an $M$ such that for $n \geq M$ we have
\begin{equation*}
\abs{x_n - x} < \frac{\epsilon}{2} .
\end{equation*}
Hence for $n \geq M$ and $k \geq M$ we have
\begin{equation*}
\abs{x_n - x_k} = 
\abs{x_n - x + x - x_k}
\leq \abs{x_n-x} + \abs{x-x_k} < \frac{\epsilon}{2} + \frac{\epsilon}{2} =
\epsilon .
\end{equation*}

Alright, that direction was easy.  Now suppose $\{ x_n \}$ is Cauchy.
We have shown that $\{ x_n \}$ is bounded.
For a bounded sequence, liminf and limsup exist, and this is
where we use the
\hyperref[defn:lub]{least-upper-bound property}.
If we show that
\begin{equation*}
\liminf_{n\to \infty} \, x_n = \limsup_{n\to\infty} \, x_n ,
\end{equation*}
then $\{ x_n \}$ must be convergent by \propref{liminfsupconv:prop}.


Define $a := \limsup \, x_n$ and
$b := \liminf \, x_n$.
By \propref{seqconvsubseqconv:prop}, there exist subsequences
$\{ x_{n_i} \}$ and
$\{ x_{m_i} \}$, such that
\begin{equation*}
\lim_{i\to\infty} x_{n_i} = a
\qquad \text{and} \qquad
\lim_{i\to\infty} x_{m_i} = b.
\end{equation*}
Given an $\epsilon > 0$,
there exists an $M_1$ such that for all $i \geq M_1$
we have $\abs{x_{n_i} - a} < \nicefrac{\epsilon}{3}$ and
an $M_2$ such that for all $i \geq M_2$ we have
$\abs{x_{m_i} - b} < \nicefrac{\epsilon}{3}$.  There also exists an $M_3$
such that for all $n,k \geq M_3$ we have
$\abs{x_n-x_k} < \nicefrac{\epsilon}{3}$.  Let $M := \max \{ M_1, M_2, M_3 \}$.
Note that if $i \geq M$, then $n_i \geq M$ and $m_i \geq M$.  Hence
\begin{equation*}
\begin{split}
\abs{a-b} & =
\abs{a-x_{n_i}+x_{n_i}
-x_{m_i}+x_{m_i}
-b} \\
& \leq
\abs{a-x_{n_i}}
+ \abs{x_{n_i} -x_{m_i}}
+ \abs{x_{m_i} -b} \\
& <
\frac{\epsilon}{3}
+
\frac{\epsilon}{3}
+
\frac{\epsilon}{3}
= \epsilon .
\end{split}
\end{equation*}
As $\abs{a-b} < \epsilon$ for all $\epsilon > 0$, then $a=b$ and 
the sequence converges.
\end{proof}

\begin{remark}
The statement of this proposition is sometimes used to define the
completeness property of the real numbers.  We say a set is
\emph{\myindex{Cauchy-complete}} (or sometimes just \emph{\myindex{complete}})
if every Cauchy sequence converges.
Above we proved that
as $\R$ has the \hyperref[defn:lub]{least-upper-bound property}, then $\R$ is 
Cauchy-complete.
We can ``complete'' $\Q$ by ``throwing in'' just enough points to make all
Cauchy sequences converge (we omit the details).
The resulting field has the
least-upper-bound property.
The advantage of using Cauchy
sequences to define completeness is that this idea generalizes to
more abstract settings.
\end{remark}

The Cauchy criterion is stronger than 
$\abs{x_{n+1}-x_n}$ (or $\abs{x_{n+j}-x_n}$ for a fixed $j$) going to zero as
$n$ goes to
infinity.  When we get to the partial sums of the harmonic series
(see \exampleref{example:harmonicseries} in the next section), we will have
a sequence such that $x_{n+1}-x_n = \nicefrac{1}{n}$, yet $\{ x_n \}$ is
divergent.  In fact, for that sequence,
$\lim_{n\to\infty} \abs{x_{n+j}-x_n} = 0$ for
any $j \in \N$ (confer \exerciseref{exercise:badnocauchy}).
The key point in the definition of Cauchy is that $n$ and $k$
vary independently and can be arbitrarily far apart.

%The Cauchy criterion for convergence becomes very useful for series, which
%we will discuss in the next section.

\subsection{Exercises}

\begin{exercise}
Prove that $\{ \frac{n^2-1}{n^2} \}$ is Cauchy using directly the definition
of Cauchy sequences.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence such that
there exists a $0 < C < 1$ such that
\begin{equation*}
\abs{x_{n+1} - x_n} \leq C \abs{x_{n}-x_{n-1}} .
\end{equation*}
Prove that $\{ x_n \}$ is Cauchy.
Hint:  You can freely use the formula (for $C \not= 1$)
\begin{equation*}
1+ C+ C^2 + \cdots + C^n = \frac{1-C^{n+1}}{1-C}.
\end{equation*}
\end{exercise}

\begin{exercise}[Challenging]
Suppose $F$ is an ordered field that contains the rational numbers
$\Q$, such that $\Q$ is dense,
that is: Whenever $x,y \in F$ are such that $x < y$,
then there exists a $q \in \Q$ such that $x < q < y$.
Say a sequence $\{ x_n \}_{n=1}^\infty$ of rational numbers is Cauchy
if given any $\epsilon \in \Q$ with $\epsilon > 0$, there exists
an $M$ such that for all $n,k \geq M$ we have $\abs{x_n-x_k} < \epsilon$.
Suppose any Cauchy sequence of rational numbers has a limit in $F$.
Prove that $F$ has the \hyperref[defn:lub]{least-upper-bound property}.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be sequences such
that $\lim\, y_n =0$.  Suppose that for all $k \in \N$
and
for all $m \geq k$ we have
\begin{equation*}
\abs{x_m-x_k} \leq y_k .
\end{equation*}
Show that $\{ x_n \}$ is Cauchy.
\end{exercise}

\begin{exercise}
Suppose a Cauchy sequence $\{ x_n \}$ is such that for every $M \in \N$,
there exists a $k \geq M$ and an $n \geq M$ such that
$x_k < 0$ and $x_n > 0$.  Using simply the definition of a Cauchy sequence
and of a convergent sequence, show that
the sequence converges to $0$.
\end{exercise}

\begin{exercise}
Suppose $\abs{x_n-x_k} \leq \nicefrac{n}{k^2}$ for all $n$ and $k$.
Show that $\{ x_n \}$ is Cauchy.
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a Cauchy sequence such that for infinitely many
$n$, $x_n = c$.  Using only the definition of Cauchy sequence prove 
that $\lim\, x_n = c$.
\end{exercise}

\begin{exercise}
True/False prove or find a counterexample:  If $\{ x_n \}$ is a Cauchy sequence then there exists an $M$
such that for all $n \geq M$ we have
$\abs{x_{n+1}-x_n}
\leq
\abs{x_{n}-x_{n-1}}$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Series}
\label{sec:series}

\sectionnotes{2 lectures}

A fundamental object in mathematics is that of a series.  In fact, when
foundations of analysis were being developed, the motivation was to
understand series.  Understanding series is very important in applications
of analysis.  For example, solving differential equations often includes
series, and differential equations are the basis for understanding
almost all of modern science.

\subsection{Definition}

\begin{defn}
Given a sequence $\{ x_n \}$, we write the formal object
\begin{equation*}
\sum_{n=1}^\infty x_n
\qquad
\text{or sometimes just}
\qquad
\sum x_n
\end{equation*}
and call it a \emph{\myindex{series}}.  A series
\emph{converges}\index{convergent series}, if the sequence $\{ s_k \}$
defined by
\begin{equation*}
s_k := \sum_{n=1}^k x_n = x_1 + x_2 + \cdots + x_k ,
\end{equation*}
converges.
The numbers $s_k$ are called
\emph{\myindex{partial sums}}.
If $x := \lim\, s_k$, we write
\begin{equation*}
\sum_{n=1}^\infty x_n =  x .
\end{equation*}
In this case, we cheat a little and treat
$\sum_{n=1}^\infty x_n$ as a number.

If the sequence $\{ s_k \}$ diverges,
we say the series is \emph{divergent}\index{divergent series}.
In this case, $\sum x_n$ is simply a formal object and not a number.
\end{defn}

In other words, for a convergent series we have
\begin{equation*}
\sum_{n=1}^\infty x_n
=
\lim_{k\to\infty} 
\sum_{n=1}^k x_n .
\end{equation*}
We only have this equality if the limit on
the right actually exists.  That is, the right-hand side does not make
sense (the limit does not exist) if the series does not converge.
Therefore, be careful as 
$\sum x_n$ means two different things (a notation for the series itself 
or the limit of the partial sum), and you must use context
to distinguish.

\begin{remark}
Before going further, let us remark that it is sometimes convenient to start
the series at an index different from 1.  That is, for example we can write
\begin{equation*}
\sum_{n=0}^\infty r^n = \sum_{n=1}^\infty r^{n-1} .
\end{equation*}
The left-hand side is more convenient to write.
%The idea for the notation is the same as
%the notation for the tail of a sequence.
\end{remark}

\begin{remark}
It is common to write the series $\sum x_n$ as
\begin{equation*}
x_1 + x_2 + x_3 + \cdots
\end{equation*}
with the understanding that the ellipsis indicates a series and
not a simple sum.  We do not use this notation as it often leads to 
mistakes in proofs.
\end{remark}

\begin{example}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2^n}
\end{equation*}
converges and the limit is 1.  That is,
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2^n} = 
\lim_{k\to\infty} \sum_{n=1}^k \frac{1}{2^n} = 
1 .
\end{equation*}

Proof: First we prove the following equality
\begin{equation*}
\left( \sum_{n=1}^k \frac{1}{2^n} \right)
+ \frac{1}{2^k}
= 1 .
\end{equation*}
The equality is easy to see when $k=1$.  The proof for general $k$
follows by \hyperref[induction:thm]{induction}, which we leave to the
reader.  See \figureref{figcutseries} for an illustration.
\begin{myfigureht}
\subimport*{figures/}{figcutseries.pdf_t}
\caption{The equality 
$\left( \sum_{n=1}^k \frac{1}{2^n} \right)
+ \frac{1}{2^k}
= 1$ illustrated for $k=3$.\label{figcutseries}}
\end{myfigureht}

Let $s_k$ be the partial sum.  We write
\begin{equation*}
\abs{
1 - s_k 
}
=
\abs{
1 - 
\sum_{n=1}^k \frac{1}{2^n}
}
=
\abs{\frac{1}{2^k}} = 
\frac{1}{2^k} .
\end{equation*}
The sequence $\{ \frac{1}{2^k} \}$ and therefore $\{ \abs{1-s_k} \}$
converges to zero.  So, $\{ s_k \}$ converges to 1.
\end{example}

\begin{prop} \label{geometric:prop}
Suppose
$-1 < r < 1$.  Then the \emph{\myindex{geometric series}}
$\sum_{n=0}^\infty r^n$ converges, and
\begin{equation*}
\sum_{n=0}^\infty r^n = \frac{1}{1-r} .
\end{equation*}
\end{prop}

Details of the proof are left as an exercise.
The proof consists of showing 
\begin{equation*}
\sum_{n=0}^{k-1} r^n = \frac{1-r^k}{1-r} ,
\end{equation*}
and then taking the limit as $k$ goes to $\infty$.
Geometric series is one of the most important series, and in fact it is
one of the few series for which we can so explicitly find the limit.
%You will find it plays a central role in the theory.

\medskip

A fact we often use is the following analogue of looking at the tail of
a sequence.

\begin{prop}
Let $\sum x_n$ be a series.  Let $M \in \N$.  Then
\begin{equation*}
\sum_{n=1}^\infty x_n \quad \text{converges if and only if} \quad
\sum_{n=M}^\infty x_n \quad \text{converges.}
\end{equation*}
\end{prop}

\begin{proof}
We look at partial sums of the two series (for $k \geq M$)
\begin{equation*}
\sum_{n=1}^{k} x_n
=
\left(
\sum_{n=1}^{M-1} x_n
\right)
+
\sum_{n=M}^{k} x_n .
\end{equation*}
Note that 
$\sum_{n=1}^{M-1} x_n$ is a fixed number.  Now use
\propref{prop:contalg} to finish the proof.
\end{proof}

\subsection{Cauchy series}

\begin{defn}
A series $\sum x_n$ is said to be \emph{Cauchy} or a
\emph{Cauchy series}\index{Cauchy series},
if the sequence of partial sums $\{ s_n \}$ is a Cauchy sequence.
\end{defn}

A sequence of real numbers converges if and only if it is
Cauchy.  Therefore a series is convergent if and only if it is Cauchy.
The series $\sum x_n$ is Cauchy if for every $\epsilon > 0$,
there exists an $M \in \N$, such that for every $n \geq M$
and $k \geq M$ we have
\begin{equation*}
\abs{ \left( \sum_{j=1}^k x_j \right) - \left( \sum_{j=1}^n x_j \right) }
< \epsilon .
\end{equation*}
Without loss of generality we assume $n < k$.  Then we write
\begin{equation*}
\abs{ \left( \sum_{j=1}^k x_j \right) - \left( \sum_{j=1}^n x_j \right) }
=
\abs{ \sum_{j={n+1}}^k x_j }
< \epsilon .
\end{equation*}
We have proved the following simple proposition.

\begin{prop} \label{prop:cachyser}
The series $\sum x_n$ is Cauchy if for every $\epsilon > 0$, 
there exists an $M \in \N$ such that for every $n \geq M$
and every $k > n$ we have
\begin{equation*}
\abs{ \sum_{j={n+1}}^k x_j }
< \epsilon .
\end{equation*}
\end{prop}

\subsection{Basic properties}

%A sequence is convergent if and only if it is Cauchy, and therefore
%the same statement is true for series.
%Proposition~\ref{prop:cachyser} has the following simple consequence.

\begin{prop}
Let $\sum x_n$ be a convergent series.  Then
the sequence $\{ x_n \}$ is convergent and
\begin{equation*}
\lim_{n\to\infty} x_n = 0.
\end{equation*}
\end{prop}

\begin{proof}
Let $\epsilon > 0$ be given.  As $\sum x_n$ is convergent, it is Cauchy.
Thus we find an $M$ such that for every $n \geq M$ we have
\begin{equation*}
\epsilon > 
\abs{ \sum_{j={n+1}}^{n+1} x_j }
=
\abs{ x_{n+1} } .
\end{equation*}
Hence for every $n \geq M+1$ we have $\abs{x_{n}} < \epsilon$.
\end{proof}

\begin{example}
If $r \geq 1$ or $r \leq -1$, then the geometric series $\sum_{n=0}^\infty r^n$
diverges.

Proof: $\abs{r^n} = \abs{r}^n \geq 1^n = 1$.  So the terms do not go to zero
and the series cannot converge.
\end{example}

So if a series converges, the terms of the series go to zero.
The implication, however, goes only one way.
Let us give an example.

\begin{example} \label{example:harmonicseries}
The series $\sum \frac{1}{n}$ diverges (despite the fact that $\lim
\frac{1}{n} = 0$).  This is the famous \emph{\myindex{harmonic series}}%
\footnote{The divergence of the harmonic series was known 
before the theory of series was made rigorous.  In fact the proof we
give is the earliest proof and was given by
\href{http://en.wikipedia.org/wiki/Oresme}{Nicole Oresme}
(1323?--1382).}.

Proof: We will show that the sequence of partial sums is unbounded, and hence
cannot converge.
Write the partial sums $s_n$ for $n = 2^k$ as:
\begin{align*}
 s_1 & = 1 , \\
 s_2 & = \left( 1 \right) + \left( \frac{1}{2} \right) , \\
 s_4 & = \left( 1 \right) + \left( \frac{1}{2} \right) +
        \left( \frac{1}{3} + \frac{1}{4} \right) , \\
 s_8 & = \left( 1 \right) + \left( \frac{1}{2} \right) +
        \left( \frac{1}{3} + \frac{1}{4} \right) +
        \left( \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} \right) , \\
& ~~ \vdots \\
 s_{2^k} & = 
1 + 
\sum_{j=1}^k
\left(
\sum_{m=2^{j-1}+1}^{2^j} \frac{1}{m}
\right) .
\end{align*}
Notice $\nicefrac{1}{3} + \nicefrac{1}{4} \geq \nicefrac{1}{4} + \nicefrac{1}{4} =
\nicefrac{1}{2}$ and
$\nicefrac{1}{5} + \nicefrac{1}{6} + \nicefrac{1}{7} + \nicefrac{1}{8}
\geq \nicefrac{1}{8} + \nicefrac{1}{8} + \nicefrac{1}{8} + \nicefrac{1}{8} =
\nicefrac{1}{2}$.  More generally
\begin{equation*}
\sum_{m=2^{k-1}+1}^{2^k} \frac{1}{m}
\geq
\sum_{m=2^{k-1}+1}^{2^k} \frac{1}{2^k}
=
(2^{k-1}) \frac{1}{2^k} = \frac{1}{2} .
\end{equation*}
Therefore
\begin{equation*}
s_{2^k} = 
1 + 
\sum_{j=1}^k
\left(
\sum_{m=2^{j-1}+1}^{2^j} \frac{1}{m}
\right) 
\geq
1 + \sum_{j=1}^k \frac{1}{2} = 1 + \frac{k}{2} .
\end{equation*}
As $\{ \frac{k}{2} \}$ is unbounded by the
\hyperref[thm:arch:i]{Archimedean property}, that means that
$\{ s_{2^k} \}$ is unbounded, and therefore $\{ s_n \}$ is unbounded.
Hence $\{ s_n \}$ diverges, and consequently $\sum \frac{1}{n}$ diverges.
\end{example}

Convergent series are linear.  That is, we can multiply them by constants
and add them and these operations are done term by term.

\begin{prop}[Linearity of series]\index{linearity of series}
Let $\alpha \in \R$ and $\sum x_n$ and $\sum y_n$ be
convergent series.  Then
\begin{enumerate}[(i)]
\item
$\sum \alpha x_n$ is a convergent series and
\begin{equation*}
\sum_{n=1}^\infty \alpha x_n
=
\alpha \sum_{n=1}^\infty x_n .
\end{equation*}
\item
$\sum ( x_n + y_n )$ is a convergent series and
\begin{equation*}
\sum_{n=1}^\infty ( x_n + y_n ) 
=
\left( \sum_{n=1}^\infty x_n \right)
+
\left( \sum_{n=1}^\infty y_n \right) .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
For the first item,
we simply write the $k$th partial sum
\begin{equation*}
\sum_{n=1}^k \alpha x_n
=
\alpha \left( \sum_{n=1}^k x_n \right) .
\end{equation*}
We look at the right-hand side and note that the constant multiple of
a convergent sequence
is convergent.  Hence, we take the limit of both sides to obtain
the result.

For the second item we also look at the
$k$th partial sum
\begin{equation*}
\sum_{n=1}^k ( x_n + y_n ) 
=
\left( \sum_{n=1}^k x_n \right)
+
\left( \sum_{n=1}^k y_n \right) .
\end{equation*}
We look at the right-hand side and note that the sum of convergent sequences
is convergent.  Hence, we take the limit of both sides to obtain
the proposition.
\end{proof}

An example of a useful application of the first item is the following
formula.  Suppose $\abs{r} < 1$ and $j \in \N$, then
\begin{equation*}
\sum_{n=j}^\infty r^n = \frac{r^j}{1-r} .
\end{equation*}
The formula follows by using the geometric series and multiplying by
$r^j$:
\begin{equation*}
r^j \sum_{n=0}^\infty r^n =
\sum_{n=0}^\infty r^{n+j}
=
\sum_{n=j}^\infty r^n .
\end{equation*}

Multiplying series is not as simple as adding, see the next
section.
It is not true, of course, that we multiply
term by term.  That strategy does not work even for finite sums:
$(a+b)(c+d) \not= ac+bd$.

\subsection{Absolute convergence}

As monotone sequences are easier to work with than arbitrary sequences, it
is usually easier to work with series $\sum x_n$, where $x_n \geq 0$ for
all $n$.  The sequence of partial sums is then monotone increasing
and converges if it is bounded from above.
Let us formalize this statement as a proposition.

\begin{prop}
If $x_n \geq 0$ for all $n$, then $\sum x_n$ converges if and only if
the sequence of partial sums is bounded from above.
\end{prop}

As the limit of a monotone increasing sequence is the supremum, then
when $x_n \geq 0$ for all $n$, we have the
inequality
\begin{equation*}
\sum_{n=1}^k x_n \leq
\sum_{n=1}^\infty x_n .
\end{equation*}
If we allow infinite limits, the inequality still
holds even when the series divergent to infinity, although in that case it is not
terribly useful.

We will see that the following common criterion for convergence of series 
has big implications for how the series can be manipulated.

\begin{defn}
A series $\sum x_n$
\emph{\myindex{converges absolutely}}\index{absolute convergence} if
the series $\sum \abs{x_n}$ converges.
If a series converges, but does not converge absolutely, we say
it is \emph{\myindex{conditionally convergent}}.
\end{defn}

\begin{prop}
If the series $\sum x_n$ converges absolutely, then it converges.
\end{prop}

\begin{proof}
A series is convergent if and only if it is Cauchy.  Hence
suppose $\sum \abs{x_n}$ is Cauchy.  That is, for every $\epsilon > 0$,
there exists an $M$ such that for all $k \geq M$ and all $n > k$ we have 
\begin{equation*}
\sum_{j=k+1}^n \abs{x_j} 
=
\abs{ \sum_{j=k+1}^n \abs{x_j} }
<
\epsilon .
\end{equation*}
We apply the triangle inequality for a finite sum to obtain
\begin{equation*}
\abs{ \sum_{j=k+1}^n x_j }
\leq
\sum_{j=k+1}^n \abs{x_j}
<
\epsilon .
\end{equation*}
Hence $\sum x_n$ is Cauchy and therefore it converges.
\end{proof}

Of course, if $\sum x_n$ converges absolutely, the limits of
$\sum x_n$ and $\sum \abs{x_n}$ are different.  Computing one
does not help us compute the other.  However the computation above leads to
a useful inequality for absolutely convergent series,
a series version of the triangle inequality,
a proof of which we leave as an exercise:
\begin{equation*}
\abs{ \sum_{j=1}^\infty x_j }
\leq
\sum_{j=1}^\infty \abs{x_j} .
\end{equation*}

Absolutely convergent series have many wonderful properties.
For example, absolutely convergent
series can be rearranged arbitrarily, or we can multiply such
series together easily.  Conditionally convergent series on the other hand
often do not behave as one would expect.  See the next section.

We leave as an exercise to show that
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^n}{n}
\end{equation*}
converges, although the reader should finish this section before trying.
On the other hand we proved
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n}
\end{equation*}
diverges.  Therefore 
$\sum \frac{{(-1)}^n}{n}$ is a conditionally convergent subsequence.

\subsection{Comparison test and the \texorpdfstring{$p$}{p}-series}

We have noted above that for a series to converge
the terms not only have to go to zero, but they have to go to zero ``fast
enough.''  If we know about convergence of a certain series
we can use the following comparison test to see if the terms of another
series go to zero ``fast enough.''

\begin{samepage}
\begin{prop}[Comparison test]\index{comparison test for series}
Let $\sum x_n$ and $\sum y_n$ be series such that $0 \leq x_n \leq y_n$
for all $n \in \N$.
\begin{enumerate}[(i)]
\item If $\sum y_n$ converges, then so does $\sum x_n$.
\item If $\sum x_n$ diverges, then so does $\sum y_n$.
\end{enumerate}
\end{prop}
\end{samepage}

\begin{proof}
As the terms of the series are all nonnegative, the sequences of
partial sums are both monotone increasing.
Since $x_n \leq y_n$ for all $n$, the partial sums
satisfy for all $k$
\begin{equation} \label{comptest:eq}
\sum_{n=1}^k x_n \leq \sum_{n=1}^k y_n .
\end{equation}
If the series $\sum y_n$ converges the partial sums for the series
are bounded.  Therefore the right-hand side of \eqref{comptest:eq}
is bounded for all $k$; there exists some $B \in \R$ such that
$\sum_{n=1}^k y_n \leq B$ for all $k$, and so
\begin{equation*}
\sum_{n=1}^k x_n \leq \sum_{n=1}^k y_n \leq B.
\end{equation*}
Hence the partial sums for $\sum x_n$
are also bounded.  Since the partial sums are a monotone increasing sequence
they are convergent.  The first item is thus proved.

On the other hand if $\sum x_n$ diverges, the sequence of partial sums
must be unbounded since it is monotone increasing.  That is, the partial
sums for $\sum x_n$ are eventually bigger than any real number.  Putting this
together with \eqref{comptest:eq} we see that for any $B \in
\R$, there is a $k$ such that 
\begin{equation*}
B \leq \sum_{n=1}^k x_n \leq \sum_{n=1}^k y_n .
\end{equation*}
Hence the partial sums for $\sum y_n$ are also unbounded, and $\sum
y_n$ also diverges.
\end{proof}

A useful series to use with the comparison test is the
$p$-series%
\footnote{We have not really yet defined $x^p$ for $x > 0$ and
an arbitrary $p \in \R$, the definition is $x^p := \exp ( p \ln x )$.
We will define the logarithm and the exponential in \sectionref{sec:logandexp}.
The proposition of course works for rational $p$
where $x^{k/m} = {(x^{1/m})}^{k}$.  See also \exerciseref{exercise:realpower}.}.

\begin{prop}[$p$-series or the $p$-test]\index{p-series}\index{p-test}
For $p \in \R$, 
the series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n^p}
\end{equation*}
converges if and only if $p > 1$.
\end{prop}

\begin{proof}
First suppose $p \leq 1$.
As $n \geq 1$, we have
$\frac{1}{n^p} \geq \frac{1}{n}$.  Since
$\sum \frac{1}{n}$ diverges, we see that the 
$\sum \frac{1}{n^p}$ must diverge for all $p \leq 1$ by the comparison test.

Now suppose $p > 1$.
We proceed as we did for the
harmonic series, but instead of showing that the sequence
of partial sums is unbounded, we show that it is bounded.
The terms of the series are positive, so the sequence of partial sums
is monotone increasing and converges if it is bounded
above.
Let $s_n$ denote the $n$th partial sum.
\begin{align*}
 s_1 & = 1 , \\
 s_3 & = \left( 1 \right) + \left( \frac{1}{2^p} + \frac{1}{3^p} \right) , \\
 s_7 & = \left( 1 \right) + \left( \frac{1}{2^p} + \frac{1}{3^p} \right) +
        \left( \frac{1}{4^p} + \frac{1}{5^p} + \frac{1}{6^p} + \frac{1}{7^p} \right) , \\
& ~~ \vdots \\
 s_{2^k - 1} &= 
1 + 
\sum_{j=1}^{k-1}
\left(
\sum_{m=2^j}^{2^{j+1}-1} \frac{1}{m^p}
\right) .
\end{align*}
Instead of estimating from below, we estimate from above.  In particular,
as $p$ is positive, then $2^p < 3^p$, and hence
$\frac{1}{2^p} + \frac{1}{3^p} <
\frac{1}{2^p} + \frac{1}{2^p}$.  Similarly
$\frac{1}{4^p} + \frac{1}{5^p} +
\frac{1}{6^p} + \frac{1}{7^p} <
\frac{1}{4^p} + \frac{1}{4^p} +
\frac{1}{4^p} + \frac{1}{4^p}$.  Therefore
\begin{equation*}
\begin{split}
s_{2^k-1}
& =
1+
\sum_{j=1}^k
\left(
\sum_{m=2^{j}}^{2^{j+1}-1} \frac{1}{m^p}
\right) 
\\
& <
1+
\sum_{j=1}^k
\left(
\sum_{m=2^{j}}^{2^{j+1}-1} \frac{1}{{(2^j)}^p}
\right) 
\\
& =
1+
\sum_{j=1}^k
\left(
\frac{2^j}{{(2^j)}^p}
\right) 
\\
& =
1+
\sum_{j=1}^k
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{split}
\end{equation*}
As $p > 1$, then $\frac{1}{2^{p-1}} < 1$.
\propref{geometric:prop} says that
\begin{equation*}
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j
\end{equation*}
converges.  Therefore
\begin{equation*}
s_{2^k-1} < 
1+
\sum_{j=1}^k
{\left(
\frac{1}{2^{p-1}}
\right)}^j 
\leq 
1+
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{equation*}
As $\{ s_n \}$ is a monotone sequence, then $s_n \leq s_{2^k-1}$
for all $n \leq 2^k-1$.  Thus for all $n$,
\begin{equation*}
s_n < 
1+
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{equation*}
The sequence of partial sums is bounded and hence converges.
\end{proof}

Note that neither the $p$-series test nor the comparison test 
tell us what the sum converges to.  They only tell us that a limit
of the partial sums exists.  For example, while we know that
$\sum \nicefrac{1}{n^2}$ converges it is far harder to
find\footnote{Demonstration of this fact is
what made the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Paul Euler} (1707 -- 1783)
famous.}
that the limit is $\nicefrac{\pi^2}{6}$.
If we treat $\sum \nicefrac{1}{n^p}$ as a function of $p$,
we get the so-called Riemann $\zeta$ function.  Understanding the
behavior of this function contains
one of the most famous unsolved problems in mathematics today and has applications
in seemingly unrelated areas such as modern cryptography.

\begin{example}
The series $\sum \frac{1}{n^2+1}$ converges.

Proof:  First, $\frac{1}{n^2+1} < \frac{1}{n^2}$ for all $n \in \N$.
The series $\sum \frac{1}{n^2}$ converges by the $p$-series test.
Therefore, by the comparison test, $\sum \frac{1}{n^2+1}$ converges.
\end{example}

\subsection{Ratio test}

Suppose $r > 0$.  The ratio of two subsequent terms in the geometric series $\sum
r^n$ is $\frac{r^{n+1}}{r^n} = r$, and the series converges
whenever $r < 1$.  Just as for sequences, this fact
can be generalized to more arbitrary series
as long as we have such a ratio ``in the limit''.  We then compare
the tail of a series to the geometric series.


\begin{prop}[Ratio test]\index{ratio test for series}
Let $\sum x_n$ be a series, $x_n \not= 0$ for all $n$, and such that
\begin{equation*}
L := \lim_{n\to\infty} \frac{\abs{x_{n+1}}}{\abs{x_n}}
\end{equation*}
exists.  Then
\begin{enumerate}[(i)]
\item
If $L < 1$, then $\sum x_n$ converges absolutely.
\item
If $L > 1$, then $\sum x_n$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
From \lemmaref{seq:ratiotest} we note that if $L > 1$, then $x_n$
diverges.  Since it is a necessary condition for the convergence of series
that the terms go to zero, we know that $\sum x_n$ must diverge.

Thus suppose $L < 1$.
We will argue that $\sum \abs{x_n}$ must converge.
The proof is similar to that of \lemmaref{seq:ratiotest}.  Of course $L \geq
0$.  
Pick
$r$ such that $L < r < 1$.  As $r-L > 0$, there exists an $M \in \N$ such that for
all $n \geq M$
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < r-L .
\end{equation*}
Therefore,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} < r .
\end{equation*}
For $n > M$ (that is for $n \geq M+1$)
write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
<
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
For $k > M$ we write the partial sum as
\begin{equation*}
\begin{split}
\sum_{n=1}^k \abs{x_n}
& =
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
\left(\sum_{n=M+1}^{k} \abs{x_n} \right)
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
\left(\sum_{n=M+1}^{k} 
(\abs{x_M} r^{-M}) r^n
\right)
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{k} r^n \right) .
\end{split}
\end{equation*}
As $0 < r < 1$ the geometric series
$\sum_{n=0}^{\infty} r^n$ converges, so
$\sum_{n=M+1}^{\infty} r^n$ converges as well.  We take the
limit as $k$ goes to infinity on the right-hand side above to obtain
\begin{equation*}
\begin{split}
\sum_{n=1}^k \abs{x_n}
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{k} r^n \right) 
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{\infty} r^n \right) .
\end{split}
\end{equation*}
The right-hand side is a number that does not depend on $k$.
Hence the sequence of partial sums of $\sum \abs{x_n}$ is bounded
and $\sum \abs{x_n}$ is convergent.  Thus $\sum x_n$ is
absolutely convergent.
\end{proof}

\begin{example}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{2^n}{n!}
\end{equation*}
converges absolutely.

Proof:  We write
\begin{equation*}
\lim_{n\to\infty} \frac{2^{(n+1)}/(n+1)!}{2^n / n!} =
\lim_{n\to\infty} \frac{2}{n+1} = 0 .
\end{equation*}
Therefore, the series converges absolutely by the ratio test.
\end{example}

\subsection{Exercises}

\begin{exercise}
Suppose the $k$th partial sum of $\sum_{n=1}^\infty x_n$ is $s_k = \frac{k}{k+1}$.
Find the series, that is find $x_n$, prove that the series converges, and
then find the limit.
\end{exercise}

\begin{exercise} \label{geometric:exr}
Prove \propref{geometric:prop}, that is for $-1 < r < 1$ prove
\begin{equation*}
\sum_{n=0}^\infty r^n = \frac{1}{1-r} .
\end{equation*}
Hint:  See \exampleref{example:geometricsum}.
\end{exercise}

\begin{exercise}
Decide the convergence or divergence of the following series.

\medskip

\noindent
\begin{tabular}{llllll}
a)
$\displaystyle \sum_{n=1}^\infty \frac{3}{9n+1}$
& &
b)
$\displaystyle \sum_{n=1}^\infty \frac{1}{2n-1}$
& &
c)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n^2}$
\\
d)
$\displaystyle \sum_{n=1}^\infty \frac{1}{n(n+1)}$
& &
e)
$\displaystyle \sum_{n=1}^\infty n e^{-n^2}$
\end{tabular}
\end{exercise}

\begin{samepage}
\begin{exercise}
{\ }
\begin{enumerate}[a)]
\item Prove that if
$\displaystyle
\sum_{n=1}^\infty x_n
$
converges, then
$\displaystyle
\sum_{n=1}^\infty ( x_{2n} + x_{2n+1} )
$
also converges.
\item
Find an explicit example where the converse does not hold.
\end{enumerate}
\end{exercise}
\end{samepage}

\begin{exercise}
For $j=1,2,\ldots,n$, let $\{ x_{j,k} \}_{k=1}^\infty$ denote $n$
sequences.  Suppose that for each $j$
\begin{equation*}
\sum_{k=1}^\infty x_{j,k}
\end{equation*}
is convergent.  Then show
\begin{equation*}
\sum_{j=1}^n \left( \sum_{k=1}^\infty x_{j,k} \right)
=
\sum_{k=1}^\infty \left( \sum_{j=1}^n x_{j,k} \right) .
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the following stronger version of the ratio test:
Let $\sum x_n$ be a series.
\begin{enumerate}[a)]
\item
If there is an $N$ and a $\rho < 1$ such that for
all $n \geq N$ we have
$\frac{\abs{x_{n+1}}}{\abs{x_n}} < \rho$, then
the series converges absolutely.
%FIXME how to add this in without changing the exercise much?
%(Equivalently the condition can be stated as
%$\limsup \frac{\abs{x_{n+1}}}{\abs{x_n}} < 1$).
\item
If there is an $N$ such that for
all $n \geq N$ we have
$\frac{\abs{x_{n+1}}}{\abs{x_n}} \geq 1$, then
the series diverges. 
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenging]
Let $\{ x_n \}$ be a decreasing sequence such that $\sum x_n$ converges.  Show
that $\displaystyle \lim_{n\to\infty} n x_n = 0$.
\end{exercise}

\begin{exercise}
Show that $\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n}$ converges.
Hint: Consider the sum of two subsequent entries.
\end{exercise}

\begin{exercise}
{\ }
\begin{enumerate}[a)]
\item Prove that if $\sum x_n$ and $\sum y_n$ converge absolutely, then
$\sum x_ny_n$ converges absolutely.
\item Find an explicit example where the converse does not hold.
\item Find an explicit example where all three series are absolutely convergent,
are not just finite sums,
and $(\sum x_n)(\sum y_n) \not= \sum x_ny_n$.  That is, show that series are
not multiplied term-by-term.
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove the triangle inequality for series, that is
if $\sum x_n$ converges absolutely then
\begin{equation*}
\abs{\sum_{n=1}^\infty x_n} \leq
\sum_{n=1}^\infty \abs{x_n} .
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{limit comparison test}}.  That is, prove that if
$a_n > 0$ and $b_n > 0$ for all $n$, and
\begin{equation*}
0 < \lim_{n\to\infty} \frac{a_n}{b_n} < \infty ,
\end{equation*}
then either $\sum a_n$ and $\sum b_n$ both converge or both diverge.
\end{exercise}

\begin{exercise} \label{exercise:badnocauchy}
Let $x_n = \sum_{j=1}^n \nicefrac{1}{j}$.  Show that for every $k$
we have
$\displaystyle \lim_{n\to\infty} \abs{x_{n+k}-x_n} = 0$, yet $\{ x_n \}$ is not Cauchy.
\end{exercise}

\begin{exercise}
Let $s_k$ be the $k$th partial sum of $\sum x_n$.\\
a) Suppose that there exists a $m \in \N$ such that $\displaystyle \lim_{k\to\infty}
s_{mk}$ exists and $\lim\, x_n = 0$.  Show that $\sum x_n$ converges.\\
b) Find an example where $\displaystyle \lim_{k\to\infty} s_{2k}$ exists and
$\lim\, x_n \not= 0$ (and therefore $\sum x_n$ diverges).\\
c) (Challenging) Find an example where $\lim\, x_n = 0$, and there exists
a subsequence $\{ s_{k_j} \}$ such that $\displaystyle \lim_{j\to\infty} s_{k_j}$ exists,
but $\sum x_n$ still diverges.
\end{exercise}

\begin{exercise} \label{exercise:squareseriesconv}
Suppose $\sum x_n$ converges and $x_n \geq 0$ for all $n$.  Prove that $\sum x_n^2$ converges.
\end{exercise}

\begin{exercise}[Challenging]
Suppose $\{ x_n\}$ is a decreasing sequence of positive numbers.
The proof of convergence/divergence for the $p$-series generalizes.
Prove the so-called 
\emph{\myindex{Cauchy Condensation Principle}}:
\begin{equation*}
\sum_{n=1}^\infty x_n \quad \text{ converges if and only if } \quad
\sum_{n=1}^\infty 2^n x_{2^n} .
\end{equation*}
\end{exercise}

\begin{exercise}
Use the Cauchy Condensation Principle (see exercise above)
to decide on the convergence of
\\
a) $\displaystyle \sum \frac{\ln n}{n^2}$
\qquad
b) $\displaystyle \sum \frac{1}{n \ln n}$
\qquad
c) $\displaystyle \sum \frac{1}{n {(\ln n)}^2}$
\qquad
d) $\displaystyle \sum \frac{1}{n (\ln n ){(\ln \ln n)}^2}$
\\
Hint: Feel free to use the identity $\ln (2^n) = n \ln 2$.
\end{exercise}

\begin{exercise}[Challenging]
Prove \emph{\myindex{Abel's theorem}}:

\medskip

\noindent
\emph{\textbf{Theorem.} Suppose $\sum x_n$ is a series whose partial sums
are a bounded sequence, $\{ \lambda_n \}$ is a sequence with $\lim \lambda_n = 0$, and
$\sum \abs{ \lambda_{n+1} - \lambda_n }$ is convergent.
Then $\sum \lambda_n x_n$ is convergent.}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{More on series}
\label{sec:moreonseries}

\sectionnotes{up to 2--3 lectures (optional, can safely be skipped or covered partially)}

\subsection{Root test}

A test similar to the ratio test is the so-called
\emph{\myindex{root test}}.  In fact, the 
proof of this test is similar and somewhat easier.
Again, the idea is to generalize what happens for the geometric series.

\begin{prop}[Root test]
Let $\sum x_n$ be a series and let
\begin{equation*}
L := \limsup_{n\to\infty} \, {\abs{x_n}}^{1/n} .
\end{equation*}
Then
\begin{enumerate}[(i)]
\item If $L < 1$ then $\sum x_n$ converges absolutely.
\item If $L > 1$ then $\sum x_n$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
If $L > 1$, then there exists a subsequence $\{ x_{n_k} \}$ such that
$L = \lim_{k\to\infty} \, {\abs{x_{n_k}}}^{1/n_k}$.  Let
$r$ be such that $L > r > 1$.  There exists an $M$ such
that for all $k \geq M$, we have 
${\abs{x_{n_k}}}^{1/n_k} > r > 1$, or in other words
$\abs{x_{n_k}} > r^{n_k} > 1$.  The
subsequence 
$\{ \abs{x_{n_k}} \}$, and therefore also
$\{ \abs{x_{n}} \}$, cannot possibly converge to zero, and so the series
diverges.

Now suppose $L < 1$.  Pick $r$ such that $L < r < 1$.
By definition of limit supremum,
pick $M$ such that for all $n \geq M$ we have 
\begin{equation*}
\sup \{ {\abs{x_k}}^{1/k} : k \geq n \} < r .
\end{equation*}
Therefore, for all $n \geq M$ we have
\begin{equation*}
{\abs{x_n}}^{1/n} < r , \qquad \text{or in other words} \qquad \abs{x_n} < r^n .
\end{equation*}
Let $k > M$, and let us estimate the $k$th partial sum
\begin{equation*}
\sum_{n=1}^k \abs{x_n} = 
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\left( \sum_{n=M+1}^k \abs{x_n} \right)
\leq
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\left( \sum_{n=M+1}^k r^n \right) .
\end{equation*}
As $0 < r < 1$,
the geometric series $\sum_{n=M+1}^\infty r^n$ converges to
$\frac{r^{M+1}}{1-r}$.  As everything is positive we have
\begin{equation*}
\sum_{n=1}^k \abs{x_n} 
\leq
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\frac{r^{M+1}}{1-r} .
\end{equation*}
Thus the sequence of partial sums of $\sum \abs{x_n}$ is bounded, and
the series converges.  Therefore $\sum x_n$ converges absolutely.
\end{proof}

\subsection{Alternating series test}

The tests we have so far only addressed absolute convergence.  The
following test gives a large supply of conditionally convergent series.

\begin{prop}[Alternating series]
Let $\{ x_n \}$ be a monotone decreasing sequence of positive real numbers such
that $\lim\, x_n = 0$.  Then
\begin{equation*}
\sum_{n=1}^\infty {(-1)}^n x_n
\end{equation*}
converges.
\end{prop}

\begin{proof}
Let $s_m := \sum_{k=1}^m {(-1)}^k x_k$ be the $m$th partial sum.  Then write
\begin{equation*}
s_{2n} =
\sum_{k=1}^{2n} {(-1)}^k x_k
=
(-x_1 + x_2) + \cdots + (-x_{2n-1} + x_{2n})
=
\sum_{k=1}^{n} (-x_{2k-1} + x_{2k}) .
\end{equation*}
The sequence $\{ x_k \}$ is decreasing and so $(-x_{2k-1}+x_{2k}) \leq 0$
for all $k$.
Therefore the subsequence $\{ s_{2n} \}$ of partial sums
is a decreasing sequence.  Similarly, $(x_{2k}-x_{2k+1}) \geq 0$, and so
\begin{equation*}
s_{2n} = - x_1 + ( x_2 - x_3 ) + \cdots + ( x_{2n-2} - x_{2n-1} ) + x_{2n}
\geq -x_1 .
\end{equation*}
The sequence $\{ s_{2n} \}$ is decreasing and bounded below, so it converges.
Let $a := \lim\, s_{2n}$.

We wish to show that $\lim\, s_m = a$ (and not just for the subsequence).
Notice
\begin{equation*}
s_{2n+1} = s_{2n} + x_{2n+1} .
\end{equation*}
Given $\epsilon > 0$, pick $M$ such that $\abs{s_{2n}-a} <
\nicefrac{\epsilon}{2}$ whenever $2n \geq M$.
Since $\lim\, x_n = 0$, we also
make $M$ possibly larger
to obtain
$x_{2n+1} < \nicefrac{\epsilon}{2}$ whenever $2n \geq M$.  
If $2n \geq M$, we have
$\abs{s_{2n}-a} < \nicefrac{\epsilon}{2} < \epsilon$, so we just need
to check the situation for $s_{2n+1}$:
\begin{equation*}
\abs{s_{2n+1}-a} = 
\abs{s_{2n}-a + x_{2n+1}} \leq
\abs{s_{2n}-a} + x_{2n+1} < 
\nicefrac{\epsilon}{2}+ \nicefrac{\epsilon}{2} = \epsilon .  \qedhere
\end{equation*}
\end{proof}

In particular, there exist conditionally convergent series
where the absolute values of the terms go to zero arbitrarily slowly.
For example,
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^n}{n^p}
\end{equation*}
converges for arbitrarily small $p > 0$, but it does not converge
absolutely when $p \leq 1$.

\subsection{Rearrangements}

Generally,
absolutely convergent series behave as we imagine they should.  For example,
absolutely convergent series can be summed in any order whatsoever.  Nothing
of the sort holds for conditionally convergent series
(see \exampleref{example:harmonsumanything}
and \exerciseref{exercise:seriesconvergestoanything}).

Take a series
\begin{equation*}
\sum_{n=1}^\infty x_n .
\end{equation*}
Given a bijective function $\sigma \colon \N \to \N$, the corresponding
rearrangement\index{rearrangement of a series} is the following
series:
\begin{equation*}
\sum_{k=1}^\infty x_{\sigma(k)} .
\end{equation*}
We simply sum the series in a different order.

\begin{prop}
Let $\sum x_n$ be an absolutely convergent series converging to a number
$x$.  Let $\sigma \colon \N \to \N$ be a bijection.  Then
$\sum x_{\sigma(n)}$ is absolutely convergent and converges to $x$.
\end{prop}

In other words,
a rearrangement of an absolutely convergent series converges (absolutely)
to the same number.

\begin{proof}
Let $\epsilon > 0$ be given.  Then take $M$ to be such that
\begin{equation*}
\abs{\left(\sum_{n=1}^M x_n \right) - x} < \frac{\epsilon}{2}
\qquad \text{and} \qquad
\sum_{n=M+1}^\infty \abs{x_n} < \frac{\epsilon}{2} .
\end{equation*}
As $\sigma$ is a bijection,
there exists a number $K$ such that for each
$n \leq M$, there exists $k \leq K$ such that $\sigma(k) = n$.
In other words
$\{ 1,2,\ldots,M \} \subset \sigma\bigl(\{ 1,2,\ldots,K \} \bigr)$.

Then for any $N \geq K$, let $Q := \max \sigma(\{ 1,2,\ldots,K \})$
and compute
\begin{equation*}
\begin{split}
\abs{\left( \sum_{n=1}^N x_{\sigma(n)} \right) - x}
& =
\abs{ \left( \sum_{n=1}^M x_n
+
\sum_{\substack{n=1\\\sigma(n) > M}}^N x_{\sigma(n)} \right) - x}
\\
& \leq
\abs{ \left( \sum_{n=1}^M x_n \right) - x}
+
\sum_{\substack{n=1\\\sigma(n) > M}}^N \abs{x_{\sigma(n)}}
\\
& \leq
\abs{ \left( \sum_{n=1}^M x_n \right) - x}
+
\sum_{n=M+1}^Q \abs{x_{n}}
\\
& < \nicefrac{\epsilon}{2} + \nicefrac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
So 
$\sum x_{\sigma(n)}$ converges to $x$.  To see that the convergence
is absolute, we apply the above argument to $\sum \abs{x_n}$ to show
that $\sum \abs{x_{\sigma(n)}}$ converges.
\end{proof}

\begin{example} \label{example:harmonsumanything}
Let us show that the alternating harmonic series $\sum
\frac{{(-1)}^{n+1}}{n}$, which does not converge absolutely, can be
rearranged to converge to anything.
The odd terms and the even terms both diverge to
infinity (prove this!):
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2n-1} = \infty, \qquad \text{and} \qquad
\sum_{n=1}^\infty \frac{1}{2n} = \infty .
\end{equation*}
Let $a_n := \frac{{(-1)}^{n+1}}{n}$ for simplicity, 
let an arbitrary number $L \in \R$ be given, and set $\sigma(1) := 1$.
Suppose we have
defined $\sigma(n)$ for all $n \leq N$.  If
\begin{equation*}
\sum_{n=1}^N a_{\sigma(n)} \leq L ,
\end{equation*}
then let $\sigma(N+1) := k$ be the smallest odd $k \in \N$
that we have not used yet,
that is $\sigma(n) \not= k$ for all $n \leq N$.
Otherwise let $\sigma(N+1) := k$ 
be the smallest even $k$ that we have not yet used.

By construction $\sigma \colon \N \to \N$ is one-to-one.
It is also onto, because if we keep adding either odd (resp.\ even) terms,
eventually we pass $L$ and switch
to the evens (resp.\ odds).  So we switch infinitely many times.

Finally, let $N$ be the $N$ where we just pass $L$ and switch.
For example suppose we have just switched from odd to even (so we start
subtracting),
and let $N' > N$ be where we first switch back from even to odd.  Then
\begin{equation*}
L + \frac{1}{\sigma(N)} \geq \sum_{n=1}^{N-1} a_{\sigma(n)}
> \sum_{n=1}^{N'-1} a_{\sigma(n)} > L- \frac{1}{\sigma(N')}.
\end{equation*}
And similarly for switching in the other direction.  Therefore,
the sum up to $N'-1$ is within $\frac{1}{\min \{ \sigma(N), \sigma(N') \}}$
of $L$.  As
we switch infinitely many times we obtain that $\sigma(N) \to \infty$
and $\sigma(N') \to \infty$, and hence
\begin{equation*}
\sum_{n=1}^\infty a_{\sigma(n)} = 
\sum_{n=1}^\infty \frac{{(-1)}^{\sigma(n)+1}}{\sigma(n)} = L .
\end{equation*}

Here is an example to illustrate the proof.  Suppose $L=1.2$, then the order
is
\begin{equation*}
1+\nicefrac{1}{3}-\nicefrac{1}{2}+\nicefrac{1}{5}+\nicefrac{1}{7}+\nicefrac{1}{9}-\nicefrac{1}{4}+\nicefrac{1}{11}+\nicefrac{1}{13}-\nicefrac{1}{6}
+\nicefrac{1}{15}+\nicefrac{1}{17}+\nicefrac{1}{19} - \nicefrac{1}{8} + \cdots .
\end{equation*}
At this point we are no more than $\nicefrac{1}{8}$ from the limit.
\end{example}

\subsection{Multiplication of series}

As we have
already mentioned,
multiplication of series is somewhat harder than addition.
If we have that at least one of the series converges
absolutely, than we can use the following theorem.  For this result it is
convenient to start the series at 0, rather than at 1.

\begin{thm}[\myindex{Mertens' theorem}\footnote{Proved by
the German mathematician
\href{http://en.wikipedia.org/wiki/Franz_Mertens}{Franz Mertens}
(1840 -- 1927).}]
Suppose $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$ are two convergent series, converging
to $A$ and $B$ respectively.  If at least one of the series
converges absolutely, then the series $\sum_{n=0}^\infty c_n$ where
\begin{equation*}
c_n = a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0 = \sum_{j=0}^n a_j b_{n-j} ,
\end{equation*}
converges to $AB$.
\end{thm}

The series $\sum c_n$ is called the \emph{\myindex{Cauchy product}} of
$\sum a_n$ and $\sum b_n$.

\begin{proof}
Suppose $\sum a_n$ converges absolutely, and let $\epsilon > 0$ be
given.
In this proof instead of picking complicated estimates just to make
the final estimate come out as less than $\epsilon$,
let us simply obtain an estimate that depends on $\epsilon$
and can be made arbitrarily small.

Write
\begin{equation*}
A_m := \sum_{n=0}^m a_n , \qquad B_m := \sum_{n=0}^m b_n .
\end{equation*}
We rearrange the $m$th partial sum of $\sum c_n$:
\begin{equation*}
\begin{split}
\abs{\left(\sum_{n=0}^m c_n \right) - AB}
& =
\abs{\left( \sum_{n=0}^m \sum_{j=0}^n a_j b_{n-j} \right) - AB}
\\
& =
\abs{\left( \sum_{n=0}^m
  B_n a_{m-n} \right) - AB}
\\
& =
\abs{\left( \sum_{n=0}^m
  ( B_n -  B ) a_{m-n} \right)
    + B A_m - AB}
\\
& \leq
\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\abs{B}\abs{A_m - A}
\end{split}
\end{equation*}
We can surely make the second term on the right hand side go to zero.
The trick is to handle the first term.
Pick $K$ such that for all $m \geq K$ we have 
$\abs{A_m - A} < \epsilon$ and
also
$\abs{B_m - B} < \epsilon$.  Finally,
as $\sum a_n$ converges absolutely,
make sure that $K$ is large enough such that
for all $m \geq K$, % we have
%Take $K$ large enough such that for all $n \geq K$
%we have
\begin{equation*}
\sum_{n=K}^m \abs{a_n} < \epsilon .
\end{equation*}
As $\sum b_n$ converges, then
we have that
$B_{\text{max}} := \sup \{ \abs{ B_n - B } : n = 0,1,2,\ldots \}$
is finite.  Take $m \geq 2K$, then in particular $m-K+1 > K$.  So
\begin{equation*}
\begin{split}
%\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
%\right)
& =
\left(
\sum_{n=0}^{m-K}
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\left(
\sum_{n=m-K+1}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
\\
& <
\left(
\sum_{n=K}^m
\abs{a_{n}}
\right)
B_{\text{max}}
+
\left(
\sum_{n=0}^{K-1}
  \epsilon \abs{a_{n}}
\right)
\\
& <
\epsilon
B_{\text{max}}
+
\epsilon
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right) .
\end{split}
\end{equation*}
Therefore, for $m \geq 2K$ we have
\begin{equation*}
\begin{split}
\abs{\left(\sum_{n=0}^m c_n \right) - AB}
& \leq
\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\abs{B}\abs{A_m - A}
\\
& <
\epsilon
B_{\text{max}}
+
\epsilon
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right)
+
\abs{B}\epsilon
=
\epsilon 
\left(
B_{\text{max}}
+
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right)
+
\abs{B}
\right) .
\end{split}
\end{equation*}
The expression in the parenthesis on the right hand side
is a fixed number.
Hence,
we can make the right hand side arbitrarily small by picking a small enough
$\epsilon> 0$.  So $\sum_{n=0}^\infty c_n$ converges to $AB$.
\end{proof}

\begin{example}
If both series are only conditionally convergent, the Cauchy product series
need not even converge.
Suppose we take $a_n = b_n = {(-1)}^n \frac{1}{\sqrt{n+1}}$.
The series $\sum_{n=0}^\infty a_n = \sum_{n=0}^\infty b_n$
converges
by the alternating series test, however, it does not converge
absolutely as can be seen from the $p$-test.  Let us look
at the Cauchy product.
\begin{equation*}
c_n = 
{(-1)}^n
\left(
\frac{1}{\sqrt{n+1}} + 
\frac{1}{\sqrt{2n}} + 
\frac{1}{\sqrt{3(n-1)}} + \cdots +
%\frac{1}{\sqrt{2n}} + 
\frac{1}{\sqrt{n+1}}
\right)
=
{(-1)}^n
\sum_{j=0}^n \frac{1}{\sqrt{(j+1)(n-j+1)}} .
\end{equation*}
Therefore
\begin{equation*}
\abs{c_n} 
=
\sum_{j=0}^n \frac{1}{\sqrt{(j+1)(n-j+1)}} 
\geq
\sum_{j=0}^n \frac{1}{\sqrt{(n+1)(n+1)}} 
= 1 .
\end{equation*}
The terms do not go to zero and hence $\sum c_n$ cannot converge.
\end{example}

\subsection{Power series}

Fix $x_0 \in \R$.
A \emph{\myindex{power series}} about $x_0$
is a series of the form
\begin{equation*}
\sum_{n=0}^\infty a_n {(x-x_0)}^n .
\end{equation*}
A power series is really a function of $x$, and
many important functions in analysis can be written
as a power series.
%That is, given $x$, $f(x)$ is given by a series of the form
%Of course if we wish to define $f \colon U \subset \R \to \R$ using
%such a series, we must have that the series converges for all $x \in U$.

We say that a power series is
\emph{convergent}\index{convergent power series} if
there is at least one $x \not= x_0$ that makes the series converge.
Note that it is trivial to see that if $x=x_0$ then the series always
converges since all terms except the first are zero.
If the series does not converge for any point $x \not= x_0$, we say that
the series is \emph{divergent}\index{divergent power series}.

\begin{example} \label{ps:expex}
The series
\begin{equation*}
\sum_{n=0}^\infty \frac{1}{n!} x^n
\end{equation*}
is absolutely convergent for all $x \in \R$.  This can be seen using the ratio test:
For any $x$ notice that
\begin{equation*}
\lim_{n \to \infty}
\frac{\bigl(1/(n+1)!\bigr) \, x^{n+1}}{(1/n!) \, x^{n}}
=
\lim_{n \to \infty}
\frac{x}{n+1}
=
0.
\end{equation*}
In fact, you may recall from calculus that this series converges to $e^x$.
\end{example}

\begin{example} \label{ps:1kex}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n} x^n
\end{equation*}
converges absolutely for all $x \in (-1,1)$ via the ratio test:
\begin{equation*}
\lim_{n \to \infty}
\abs{
\frac{\bigl(1/(n+1) \bigr) \, x^{n+1}}{(1/n) \, x^{n}}
}
=
\lim_{n \to \infty}
\abs{x} \frac{n}{n+1}
=
\abs{x} < 1 .
\end{equation*}
It converges at $x=-1$,
as
$\sum_{n=1}^\infty \frac{{(-1)}^n}{n}$ converges
by the alternating series
test.
But the power series does not converge absolutely at $x=-1$, because
$\sum_{n=1}^\infty \frac{1}{n}$ does not converge.
The series
diverges at $x=1$.
When $\abs{x} > 1$, then the series diverges via the ratio test.
\end{example}

\begin{example} \label{ps:divergeex}
The series
\begin{equation*}
\sum_{n=1}^\infty n^n x^n
\end{equation*}
diverges for all $x \not= 0$.  Let us apply the root test
\begin{equation*}
\limsup_{n\to\infty}
\,
\abs{n^n x^n}^{1/n}
=
\limsup_{n\to\infty}
\,
n \abs{x}
= \infty .
\end{equation*}
Therefore the series diverges for all $x \not= 0$.
\end{example}

In fact, convergence of power series in general always works analogously to
one of the three examples above.
%The three examples show what happens for power series in general.

\begin{prop}
Let $\sum_{n=0}^\infty a_n {(x-x_0)}^n$ be a power series.
If the series is convergent, then either it converges at
all $x \in \R$, or
there exists a number $\rho$, such that
the series converges absolutely on the interval
$(x_0-\rho,x_0+\rho)$ and diverges when $x < x_0-\rho$ or $x > x_0+\rho$.
\end{prop}

The number $\rho$ is called the \emph{\myindex{radius of convergence}} of the
power series.  We write $\rho = \infty$ if the series converges for
all $x$, and we write $\rho = 0$ if the series is divergent.
See \figureref{ps:convfig}.
In \exampleref{ps:1kex}
the radius of convergence is $\rho=1$.
In \exampleref{ps:expex} the radius of convergence is $\rho=\infty$,
and in \exampleref{ps:divergeex} the radius of convergence is $\rho=0$.

\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{ps-conv.pdf_t}
\caption{Convergence of a power series.\label{ps:convfig}}
%\end{center}
\end{myfigureht}

\begin{proof}
Write
\begin{equation*}
R := \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n} .
\end{equation*}
We use the root test to prove the proposition:
\begin{equation*}
L = \limsup_{n\to\infty} \, {\abs{a_n {(x-x_0)}^n}}^{1/n} 
=
\abs{x-x_0} \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n}
=
\abs{x-x_0} R .
\end{equation*}
In particular if $R = \infty$, then $L=\infty$ for any $x \not= x_0$, and
the series diverges by the root test.  On the
other hand if $R = 0$, then $L=0$ for any $x$, and the series 
converges absolutely for all $x$.

Suppose $0 < R < \infty$.
The series
converges absolutely if
$1 > L = R \abs{x-x_0}$,
or in other words when
\begin{equation*}
\abs{x-x_0} < \nicefrac{1}{R} .
\end{equation*}
The series diverges when
$1 < L = R \abs{x-x_0}$,
or
\begin{equation*}
\abs{x-x_0} > \nicefrac{1}{R} .
\end{equation*}
Letting $\rho = \nicefrac{1}{R}$ completes the proof.
\end{proof}

It may be useful to restate what we have learned in the proof
as a separate proposition.

\begin{prop}
Let $\sum_{n=0}^\infty a_n {(x-x_0)}^n$ be a power series, and let
\begin{equation*}
R := \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n} .
\end{equation*}
If $R = \infty$, the power series is divergent.  If
$R=0$, then the power series converges everywhere.   Otherwise
the radius of convergence $\rho = \nicefrac{1}{R}$.
\end{prop}

Often, radius of convergence is written as $\rho = \nicefrac{1}{R}$ in all
three cases, with
the understanding of what $\rho$ should be if $R = 0$ or $R =
\infty$.

Convergent power series can be added and multiplied together, and multiplied
by constants.
The proposition has an easy proof using what we know about series
in general, and power series in particular.  We leave the proof to the reader.

\begin{prop}
Let $\sum_{n=0}^\infty a_n {(x-x_0)}^n$ and
$\sum_{n=0}^\infty b_n {(x-x_0)}^n$ be two convergent power series
with radius of convergence at least $\rho > 0$ and $\alpha \in \R$.  Then
for all $x$ such that $\abs{x-x_0} < \rho$, we have 
\begin{equation*}
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
+
\left(\sum_{n=0}^\infty b_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty (a_n+b_n) {(x-x_0)}^n ,
\end{equation*}
\begin{equation*}
\alpha
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty \alpha a_n {(x-x_0)}^n ,
\end{equation*}
and
\begin{equation*}
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
\,
\left(\sum_{n=0}^\infty b_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty c_n {(x-x_0)}^n ,
\end{equation*}
where
$c_n = a_0b_n + a_1 b_{n-1} + \cdots + a_n b_0$.
\end{prop}

That is, after performing the algebraic operations, the
radius of convergence of the resulting series is at least $\rho$.
For all $x$ with $\abs{x-x_0} < \rho$, we have two convergent series so
their term by term addition and multiplication by constants
follows by what we learned in the last section.
For multiplication of two power series,
the series are absolutely convergent inside
the radius of convergence and that is why for those $x$
we can apply Mertens' theorem.
Note that after applying an algebraic operation the radius of convergence
could increase.  See the exercises.

Let us look at some examples of power series.
Polynomials are simply finite power series.  That is, a polynomial
is a power series where
the $a_n$ are zero for all $n$ large enough.  We expand
a polynomial as a power series about any point $x_0$ by writing
the polynomial as a polynomial in $(x-x_0)$.  For example,
$2x^2-3x+4$ as a power series around $x_0 = 1$ is
\begin{equation*}
2x^2-3x+4 = 3 + (x-1) + 2{(x-1)}^2 .
\end{equation*}

We can also expand
\emph{\myindex{rational functions}}, that is, ratios of polynomials
as power series, although we will not completely prove this fact here.
Notice that a series for a rational function only defines the function
on an interval even if the function is defined elsewhere.  For example, for
the \emph{\myindex{geometric series}} we have that for
$x \in (-1,1)$
\begin{equation*}
\frac{1}{1-x} =
\sum_{n=0}^\infty x^n .
\end{equation*}
The series diverges when $\abs{x} > 1$, even though $\frac{1}{1-x}$ is
defined for all $x \not= 1$.

We can use the geometric series together with rules for addition and
multiplication of power series to expand rational functions as power
series around $x_0$,
as long as the denominator is not zero at $x_0$.  We state without
proof that this is always possible, and we give an example of such
a computation using the geometric series.

\begin{example}
Let us expand $\frac{x}{1+2x+x^2}$ as a power series around the origin ($x_0 = 0$) and
find the radius of convergence.

Write $1+2x+x^2 = {(1+x)}^2 = {\bigl(1-(-x)\bigr)}^2$, and suppose
$\abs{x} < 1$.  Compute
\begin{equation*}
\begin{split}
\frac{x}{1+2x+x^2}
&=
x \,
{\left(
\frac{1}{1-(-x)}
\right)}^2
\\
&=
x \,
{\left( 
\sum_{n=0}^\infty {(-1)}^n x^n 
\right)}^2
\\
&=
x \,
\left(
\sum_{n=0}^\infty c_n x^n 
\right)
\\
&=
\sum_{n=0}^\infty c_n x^{n+1} ,
\end{split}
\end{equation*}
where using the formula for the product of series
we obtain, $c_0 = 1$, $c_1 = -1 -1 = -2$, $c_2 = 1+1+1 = 3$, etc\ldots.
Therefore we get that for $\abs{x} < 1$, 
\begin{equation*}
\frac{x}{1+2x+x^2}
=
\sum_{n=1}^\infty {(-1)}^{n+1} n x^n .
\end{equation*}
The radius of convergence is at least 1.  We leave it to the reader to
verify that the radius of convergence is exactly equal to 1.
%We use the ratio test
%\begin{equation*}
%\lim_{n\to\infty}
%\left\lvert \frac{a_{k+1}}{a_k} \right\rvert
%=
%\lim_{k\to\infty}
%\left\lvert \frac{{(-1)}^{k+2} (k+1)}{{(-1)}^{k+1}k} \right\rvert
%=
%\lim_{k\to\infty}
%\frac{k+1}{k}
%= 1 .
%\end{equation*}
%So the radius of convergence is actually equal to 1.
\end{example}

You can use the method of partial fractions you know from calculus.
For example, to find the power series for $\frac{x^3+x}{x^2-1}$ at 0, write
\begin{equation*}
\frac{x^3+x}{x^2-1}
=
x + \frac{1}{1+x} - \frac{1}{1-x}
=
x + \sum_{n=0}^\infty {(-1)}^n x^n - \sum_{n=0}^\infty x^n .
\end{equation*}

\subsection{Exercises}

%This kind of sucks as an exercise ....
%\begin{exercise}
%Strengthen the root test.  Suppose that
%$\limsup_{n\to\infty} \, {\abs{x_n}}^{1/n} = 1$.
%Find an easy condition
%
%but
%that ${\abs{x_n}}^{1/n} \geq 1$ infinitely often, then show that
%the series diverges.
%\end{exercise}

\begin{exercise}
Decide the convergence or divergence of the following series.

\medskip

\noindent
\begin{tabular}{lllllll}
a)
$\displaystyle \sum_{n=1}^\infty \frac{1}{2^{2n+1}}$
& &
b)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^{n}(n-1)}{n}$
& &
c)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n^{1/10}}$
& &
d)
$\displaystyle \sum_{n=1}^\infty \frac{n^n}{{(n+1)}^{2n}}$
\end{tabular}
\end{exercise}

\begin{exercise}
Suppose both $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$ 
converge absolutely.
Show that the product series, $\sum_{n=0}^\infty c_n$ where
$c_n = a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0$, also converges absolutely.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:seriesconvergestoanything}
Let $\sum a_n$ be conditionally convergent.
Show that given any number $x$
there exists a rearrangement of $\sum a_n$
such that the rearranged series converges to $x$.
Hint: See \exampleref{example:harmonsumanything}.
\end{exercise}

\begin{exercise}
a) Show that the alternating harmonic series $\sum
\frac{{(-1)}^{n+1}}{n}$ has a rearrangement
such that for any $x < y$, there exists a partial sum $s_n$
of the rearranged series such that $x < s_n < y$.
b) Show that the rearrangement you found does not converge.
See \exampleref{example:harmonsumanything}.
c) Show that for any $x \in \R$, there exists a subsequence of
partial sums $\{ s_{n_k} \}$ of your rearrangement such that 
$\lim \, s_{n_k} = x$.
\end{exercise}

\begin{exercise}
For the following power series, find if they are convergent or not, and
if so find their radius of convergence.

\medskip

\noindent
\begin{tabular}{llllll}
a)
$\displaystyle \sum_{n=0}^\infty 2^n x^n$
&
b) $\displaystyle \sum_{n=0}^\infty n x^n$
& 
c) 
$\displaystyle \sum_{n=0}^\infty n! \, x^n$
&
d) $\displaystyle \sum_{n=0}^\infty \frac{1}{(2k)!} {(x-10)}^n$
&
e) $\displaystyle \sum_{n=0}^\infty x^{2n}$
&
f) $\displaystyle \sum_{n=0}^\infty n! \, x^{n!}$
\end{tabular}
\end{exercise}

\begin{exercise}
Suppose $\sum a_n x^n$ converges for $x=1$.
a) What can you say about the radius of convergence?
b) If you further know that at $x=1$ the convergence is not absolute,
what can you say?
\end{exercise}

\begin{exercise}
Expand
$\dfrac{x}{4-x^2}$ as a power series around $x_0 = 0$ and compute its radius
of convergence.
\end{exercise}

\begin{exercise}
a) Find an example where the radius of convergence of $\sum a_n x^n$ and
$\sum b_n x^n$ are 1, but the radius of convergence of
the sum of the two series is infinite.
b) (Trickier)
Find an example where the radius of convergence of $\sum a_n x^n$ and
$\sum b_n x^n$ are 1, but the radius of convergence of
the product of the two series is infinite.
\end{exercise}

\begin{exercise}
Figure out how to compute the radius of convergence using the ratio test.
That is, suppose $\sum a_n x^n$ is a power series and
$R := \lim \, \frac{\abs{a_{n+1}}}{\abs{a_n}}$ exists or is $\infty$.
Find the radius of convergence and prove your claim.
\end{exercise}

\begin{exercise}
a) Prove that $\lim \, n^{1/n} = 1$.  Hint:  Write $n^{1/n} = 1+b_n$ and
note $b_n > 0$.  Then show that ${(1+b_n)}^n \geq 
\frac{n(n-1)}{2}b_n^2$ and use this to show that $\lim \, b_n = 0$. b) Use
the result of part a) to show that if $\sum a_n x^n$ is a convergent power series with
radius of convergence $R$, then $\sum n a_n x^n$ is also convergent with the
same radius of convergence.
\end{exercise}

\begin{exnote}
There are different notions of summability (convergence)
of a series
than just the one we have seen.
A common one is \emph{\myindex{Ces{\`a}ro summability}}%
\footnote{Named for the Italian mathematician
\href{http://en.wikipedia.org/wiki/Ernesto_Ces\%C3\%A0ro}{Ernesto Ces{\`a}ro}
(1859 -- 1906).}.  Let $\sum a_n$ be a series
and let $s_n$ be the $n$th partial sum.  The series is said to
be Ces{\`a}ro summable to $a$ if
\begin{equation*}
a = \lim_{n\to \infty} \frac{s_1 + s_2 + \cdots + s_n}{n} .
\end{equation*}
\end{exnote}

\begin{exercise}[Challenging]
a) If $\sum a_n$ is convergent to $a$ (in the usual sense), show that
$\sum a_n$ is Ces{\`a}ro summable to $a$.
b) Show that in the sense of Ces{\`a}ro $\sum {(-1)}^n$ is summable to
$\nicefrac{1}{2}$.
c) Let $a_n := k$ when $n = k^3$ for some $k \in \N$,
$a_n := -k$ when $n = k^3+1$ for some $k \in \N$,
otherwise
let $a_n := 0$.  Show that $\sum a_n$ diverges in the usual sense,
(partial sums are unbounded), but it is
Ces{\`a}ro summable to 0 (seems a little paradoxical at first sight).
\end{exercise}

\begin{exercise}[Challenging]
Show that the monotonicity in the alternating series test
is necessary.  That is, find a sequence of positive real numbers
$\{ x_n \}$ with $\lim\, x_n = 0$ but such that
$\sum {(-1)}^n x_n$ diverges.
\end{exercise}

\begin{exercise}
Find a series such that $\sum x_n$ converges but $\sum x_n^2$ diverges.
Hint: Compare with \exerciseref{exercise:squareseriesconv}.
\end{exercise}

\begin{exercise}
Suppose $\{ c_n \}$ is any sequence.  Prove that for any $r \in (0,1)$
there exists a strictly increasing sequence $\{ n_k \}$ of natural numbers ($n_{k+1} > n_k$) such that
\begin{equation*}
\sum_{k=1}^\infty c_k x^{n_k}
\end{equation*}
converges absolutely for all $x \in [-r,r]$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Continuous Functions} \label{lim:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limits of functions}
\label{sec:limoffunc}

\sectionnotes{2--3 lectures}

Before we define continuity of functions, we need to visit a somewhat
more general notion of a limit.  That is, given a function $f \colon S \to
\R$, we want to see how $f(x)$ behaves as $x$ tends to a certain point.

\subsection{Cluster points}

First,
let us return to a concept we have previously seen in an exercise.

\begin{defn}
Let $S \subset \R$ be a set.  A number $x \in \R$ is called
a \emph{\myindex{cluster point}} of $S$
if for every $\epsilon > 0$, the set $(x-\epsilon,x+\epsilon) \cap S
\setminus \{ x \}$ is not empty.
\end{defn}

That is, $x$ is a cluster point of $S$ if there are points of $S$
arbitrarily close to $x$.  Another way of phrasing the definition is to say
that $x$ is a cluster point of $S$ if for every $\epsilon > 0$, there
exists a $y \in S$ such that $y \not= x$ and $\abs{x - y} < \epsilon$.
Note that a cluster point of $S$ need not lie in $S$.

Let us see some examples.
\begin{enumerate}[(i)]
\item The set
$\{ \nicefrac{1}{n} : n \in \N \}$ has a unique cluster point zero.
\item The cluster points of the open interval $(0,1)$ are
all points in the closed interval $[0,1]$.
\item For the set $\Q$, the set of
cluster points is the whole real line $\R$.
\item For the set $[0,1) \cup \{ 2 \}$,
the set of cluster points is the interval $[0,1]$.
\item The set $\N$ has no cluster points in $\R$.
\end{enumerate}

\begin{prop}
Let $S \subset \R$.  Then $x \in \R$ is a cluster point of $S$
if and only if
there exists a convergent sequence of numbers $\{ x_n \}$ such that
$x_n \not= x$ and $x_n \in S$ for all $n$, and $\lim\, x_n = x$.
\end{prop}

\begin{proof}
First suppose $x$ is a cluster point of $S$.
For any $n \in \N$, we pick $x_n$ to be an arbitrary point of
$(x-\nicefrac{1}{n},x+\nicefrac{1}{n}) \cap S \setminus \{x\}$, which
we know is nonempty because $x$ is a cluster point of $S$.
Then
$x_n$ is within $\nicefrac{1}{n}$ of $x$, that is,
\begin{equation*}
\abs{x-x_n} < \nicefrac{1}{n} .
\end{equation*}
As $\{ \nicefrac{1}{n} \}$ converges to zero, $\{ x_n \}$ converges to $x$.

On the other hand, if we start with a sequence of numbers $\{ x_n \}$ in $S$
converging to $x$ such that $x_n \not= x$ for all $n$, then for every
$\epsilon > 0$ there is an $M$ such that in particular $\abs{x_M - x} <
\epsilon$.  That is, $x_M \in (x-\epsilon,x+\epsilon) \cap S \setminus \{x\}$.
\end{proof}

\subsection{Limits of functions}

If a function $f$ is defined on a set $S$ and $c$ is a cluster point of $S$,
then we define the limit of $f(x)$ as $x$ gets close to $c$.  
It is irrelevant for the definition if $f$ is defined at $c$ or not.
Furthermore, even if the function is defined at $c$, the limit of the
function as $x$ goes to $c$ can very well be different
from $f(c)$.

\begin{defn}
\index{limit of a function}%
Let $f \colon S \to \R$ be a function and $c$ a cluster point of $S$.
Suppose there exists an $L \in \R$ and for every $\epsilon > 0$,
there exists a $\delta > 0$ such that whenever $x \in S \setminus \{ c \}$
and $\abs{x - c} < \delta$, then
\begin{equation*}
\abs{f(x) - L} < \epsilon .
\end{equation*}
In this case we say $f(x)$ \emph{\myindex{converges}} to $L$ as $x$ goes
to $c$.  We say $L$ is the \emph{\myindex{limit}} of $f(x)$ as $x$
goes to $c$.  We write
\begin{equation*}
\lim_{x \to c} f(x) := L ,
\end{equation*}
or 
\begin{equation*}
f(x) \to L \quad\text{as}\quad x \to c .
\end{equation*}
If no such $L$ exists, then we say that the limit does not exist or
that $f$ \emph{\myindex{diverges}} at $c$.
\end{defn}

Again the notation and language we are using above assumes the limit
is unique even though we have not yet proved that.
Let us do that now.

\begin{prop}
Let $c$ be a cluster point of $S \subset \R$ and let $f \colon S \to \R$
be a function such that $f(x)$ converges as $x$ goes to $c$.  Then
the limit of $f(x)$ as $x$ goes to $c$ is unique.
\end{prop}

\begin{proof}
Let $L_1$ and $L_2$ be two numbers that both satisfy the definition.
Take an $\epsilon > 0$ and find a $\delta_1 > 0$ such that
$\abs{f(x)-L_1} < \nicefrac{\epsilon}{2}$ 
for all $x \in S \setminus \{c\}$ with $\abs{x-c} < \delta_1$.
Also find $\delta_2 > 0$ such that
$\abs{f(x)-L_2} < \nicefrac{\epsilon}{2}$
for all $x \in S \setminus \{c\}$ with $\abs{x-c} < \delta_2$.
Put $\delta := \min \{ \delta_1, \delta_2 \}$.  Suppose $x \in S$,
$\abs{x-c} < \delta$, and $x \not= c$.  As $\delta > 0$ and $c$ is a cluster
point, such an $x$ exists.  Then
\begin{equation*}
\abs{L_1 - L_2} =
\abs{L_1 - f(x) + f(x) - L_2} \leq
\abs{L_1 - f(x)} + \abs{f(x) - L_2} < \frac{\epsilon}{2} + \frac{\epsilon}{2}
= \epsilon.
\end{equation*}
As $\abs{L_1-L_2} < \epsilon$ for arbitrary $\epsilon > 0$, then
$L_1 = L_2$.
\end{proof}

\begin{example}
Let $f \colon \R \to \R$ be defined as $f(x) := x^2$.  Then
\begin{equation*}
\lim_{x\to c} f(x) = \lim_{x\to c} x^2 = c^2 .
\end{equation*}

Proof: First let $c$ be fixed.  Let $\epsilon > 0$ be given.  Take
\begin{equation*}
\delta := \min \left\{ 1 , \, \frac{\epsilon}{2\abs{c}+1} \right\} .
\end{equation*}
Take $x \not= c$ such that $\abs{x-c} < \delta$.  In particular,
$\abs{x-c} < 1$.  By reverse triangle inequality we get
\begin{equation*}
\abs{x}-\abs{c} \leq \abs{x-c} < 1 .
\end{equation*}
Adding $2\abs{c}$ to both sides we obtain
$\abs{x} + \abs{c} < 2\abs{c} + 1$.  We compute
\begin{equation*}
\begin{split}
\abs{f(x) - c^2} &= \abs{x^2-c^2} \\
&= \abs{(x+c)(x-c)} \\
&= \abs{x+c}\abs{x-c} \\
&\leq (\abs{x}+\abs{c})\abs{x-c} \\
&< (2\abs{c}+1)\abs{x-c} \\
&< (2\abs{c}+1)\frac{\epsilon}{2\abs{c}+1} = \epsilon .
\end{split}
\end{equation*}
\end{example}

\begin{example}
Define $f \colon [0,1) \to \R$ by
\begin{equation*}
f(x) := 
\begin{cases}
x & \text{if $x > 0$} , \\
1 & \text{if $x = 0$} .
\end{cases}
\end{equation*}
Then
\begin{equation*}
\lim_{x\to 0} f(x) = 0 ,
\end{equation*}
even though $f(0) = 1$.

Proof:  Let $\epsilon > 0$ be given.  Let $\delta := \epsilon$.
Then for $x \in [0,1)$, $x \not= 0$, and $\abs{x-0} < \delta$ we get
\begin{equation*}
\abs{f(x) - 0} = \abs{x} < \delta = \epsilon .
\end{equation*}
\end{example}

\subsection{Sequential limits} \label{subseq:sequentiallimits}

Let us connect the limit as defined above with limits of sequences.

\begin{lemma}\label{seqflimit:lemma}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$ be a function.

Then
$f(x) \to L$ as $x \to c$, if and only if for every sequence $\{ x_n \}$
of numbers such that $x_n \in S \setminus \{c\}$ for all $n$,
and such that $\lim\, x_n = c$,
we have that the sequence $\{ f(x_n) \}$ converges to $L$.
\end{lemma}

\begin{proof}
Suppose 
$f(x) \to L$ as $x \to c$, and $\{ x_n \}$ is a sequence
such that
$x_n \in S \setminus \{c\}$ and
$\lim\, x_n = c$.
We wish to show that $\{ f(x_n) \}$ converges to $L$.
Let $\epsilon > 0$ be given.  Find a $\delta > 0$ such that
if $x \in S \setminus \{c\}$ and $\abs{x-c} < \delta$, then
$\abs{f(x) - L} < \epsilon$.  As
$\{ x_n \}$  converges to $c$, find an $M$ such that for $n \geq M$
we have that $\abs{x_n - c} < \delta$.  Therefore, for $n \geq M$,
\begin{equation*}
\abs{f(x_n) - L} < \epsilon .
\end{equation*}
Thus $\{ f(x_n) \}$ converges to $L$.

For the other direction, we use proof by contrapositive.  Suppose 
it is not true that $f(x) \to L$ as $x \to c$.  The negation of the
definition is that there exists an $\epsilon > 0$ such that for every
$\delta > 0$ there exists an $x \in S \setminus \{c\}$, where
$\abs{x-c} < \delta$
and $\abs{f(x)-L} \geq \epsilon$.

Let us use $\nicefrac{1}{n}$ for $\delta$ in the above statement to
construct a sequence $\{ x_n \}$.  We have
that there exists an $\epsilon > 0$ such that for every $n$,
there exists a point $x_n \in S \setminus \{c\}$, where
$\abs{x_n-c} < \nicefrac{1}{n}$
and $\abs{f(x_n)-L} \geq \epsilon$.
The sequence $\{ x_n \}$ just constructed converges to $c$, but
the sequence $\{ f(x_n) \}$ does not converge to $L$.
And we are done.
\end{proof}

It is possible to strengthen the reverse direction of
the lemma by simply stating that
$\{ f(x_n) \}$ converges without requiring a specific limit.
See \exerciseref{exercise:seqflimitalt}.

\begin{example}
$\displaystyle \lim_{x \to 0} \, \sin( \nicefrac{1}{x} )$
does not exist, but 
$\displaystyle \lim_{x \to 0} \, x\sin( \nicefrac{1}{x} ) = 0$.
See \figureref{figsin1x}.

\begin{myfigureht}
%\begin{center}
%\includegraphics[width=3in]{figures/sin1x}
\includegraphics{figures/sin1xfig}
\qquad
%\includegraphics[width=3in]{figures/xsin1x}
\includegraphics{figures/xsin1xfig}
\caption{Graphs of $\sin(\nicefrac{1}{x})$ and $x \sin(\nicefrac{1}{x})$.
Note that the computer cannot properly graph $\sin(\nicefrac{1}{x})$
near zero as it oscillates too fast.\label{figsin1x}}
%\end{center}
\end{myfigureht}

Proof:
We start with $\sin(\nicefrac{1}{x})$.  Define the sequence
$x_n := \frac{1}{\pi n + \nicefrac{\pi}{2}}$.  It is not hard to see
that $\lim\, x_n = 0$.  Furthermore,
\begin{equation*}
\sin ( \nicefrac{1}{x_n} )
=
\sin (\pi n + \nicefrac{\pi}{2})
= {(-1)}^n .
\end{equation*}
Therefore, $\{ \sin ( \nicefrac{1}{x_n} ) \}$ does not converge.
Thus, by
\lemmaref{seqflimit:lemma}, 
$\lim_{x \to 0} \, \sin( \nicefrac{1}{x} )$ does not exist.

Now let us look at $x\sin(\nicefrac{1}{x})$.  Let $x_n$ be a sequence
such that $x_n \not= 0$ for all $n$, and such that $\lim\, x_n = 0$.
Notice that $\abs{\sin(t)} \leq 1$ for any $t \in \R$.  Therefore,
\begin{equation*}
\abs{x_n\sin(\nicefrac{1}{x_n})-0}
=
\abs{x_n}\abs{\sin(\nicefrac{1}{x_n})}
\leq
\abs{x_n} .
\end{equation*}
As $x_n$ goes to 0, then $\abs{x_n}$ goes to zero, and hence
$\{ x_n\sin(\nicefrac{1}{x_n}) \}$ converges to zero.  By
\lemmaref{seqflimit:lemma}, 
$\displaystyle \lim_{x \to 0} \, x\sin( \nicefrac{1}{x} ) = 0$.
\end{example}

Keep in mind the phrase ``for every sequence'' in the lemma.
For example, take $\sin(\nicefrac{1}{x})$ and the sequence $x_n = \nicefrac{1}{\pi n}$.
Then $\{ \sin (\nicefrac{1}{x_n}) \}$ is the constant zero sequence, and
therefore converges to zero.

Using \lemmaref{seqflimit:lemma}, 
we can start applying everything we know about
sequential limits to limits of functions.  Let us give a few important
examples.

\begin{cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$ and $g \colon S \to \R$ be functions.
Suppose the limits of $f(x)$ and $g(x)$ as $x$ goes to $c$ both exist,
and that
\begin{equation*}
f(x) \leq g(x) \qquad \text{for all $x \in S$}.
\end{equation*}
Then
\begin{equation*}
\lim_{x\to c} f(x) \leq \lim_{x\to c} g(x) .
\end{equation*}
\end{cor}

\begin{proof}
Take $\{ x_n \}$ be a sequence of numbers in $S \setminus \{ c \}$
that converges to $c$.  Let
\begin{equation*}
L_1 := \lim_{x\to c} f(x), \qquad \text{and} \qquad L_2 := \lim_{x\to c} g(x) .
\end{equation*}
By \lemmaref{seqflimit:lemma} we know $\{ f(x_n) \}$ converges to
$L_1$ and $\{ g(x_n) \}$ converges to $L_2$.  We also
have $f(x_n) \leq g(x_n)$.
We obtain $L_1 \leq L_2$ using
\lemmaref{limandineq:lemma}.
\end{proof}

By applying constant functions, we get the following corollary.  The
proof is left as an exercise.

\begin{cor} \label{fconstineq:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$ be a function.  And suppose the limit of $f(x)$ as $x$ goes to $c$
exists.
Suppose there are two real numbers $a$ and $b$ such that
\begin{equation*}
a \leq f(x) \leq b \qquad \text{for all $x \in S$}.
\end{equation*}
Then
\begin{equation*}
a \leq \lim_{x\to c} f(x) \leq b .
\end{equation*}
\end{cor}

Using \lemmaref{seqflimit:lemma} in the same way as above we also get
the following corollaries, whose proofs are again left as an exercise.

\begin{cor} \label{fsqueeze:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$,
$g \colon S \to \R$, and $h \colon S \to \R$ be functions.  Suppose 
\begin{equation*}
f(x) \leq g(x) \leq h(x) \qquad \text{for all $x \in S$},
\end{equation*}
and the limits of $f(x)$ and $h(x)$ as $x$ goes to $c$ both exist, and
\begin{equation*}
\lim_{x\to c} f(x) = \lim_{x\to c} h(x) .
\end{equation*}
Then the limit of $g(x)$ as $x$ goes to $c$ exists and
\begin{equation*}
\lim_{x\to c} g(x) =
\lim_{x\to c} f(x) = \lim_{x\to c} h(x) .
\end{equation*}
\end{cor}

\begin{cor} \label{falg:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$ and
$g \colon S \to \R$ be functions. 
Suppose limits of $f(x)$ and $g(x)$ as $x$ goes to $c$ both exist.
Then
\begin{enumerate}[(i)]
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)+g(x)\bigr) = \left(\lim_{x\to c} f(x)\right) + 
\left(\lim_{x\to c} g(x)\right) .
$
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)-g(x)\bigr) = \left(\lim_{x\to c} f(x)\right) -
\left(\lim_{x\to c} g(x)\right) .
$
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)g(x)\bigr) = \left(\lim_{x\to c} f(x)\right)
\left(\lim_{x\to c} g(x)\right) .
$
\item \label{falg:cor:iv} If
$\displaystyle \lim_{x\to c} g(x) \not= 0$,
and $g(x) \not= 0$ for all $x \in S \setminus \{ c \}$, then
\begin{equation*}
\lim_{x\to c} \frac{f(x)}{g(x)} =
\frac{\lim_{x\to c} f(x)}{\lim_{x\to c} g(x)} .
\end{equation*}
\end{enumerate}
\end{cor}

\begin{cor} \label{fabs:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.  Let $f \colon S \to
\R$ be a function and suppose the limit of $f(x)$ as $x$ goes to $c$ exists.
Then
\begin{equation*}
\lim_{x\to c} \abs{f(x)} =
\abs{\lim_{x\to c} f(x)}.
\end{equation*}
\end{cor}

\subsection{Limits of restrictions and one-sided limits}

%It is not necessary to always consider all of $S$.
Sometimes we work with the function defined on a subset.

\begin{defn}
Let $f \colon S \to \R$ be a function.  Let $A \subset S$.  Define the
function $f|_A \colon A \to \R$ by
\begin{equation*}
f|_A (x) := f(x)  \qquad \text{for $x \in A$}.
\end{equation*}
The function
$f|_A$ is called the \emph{\myindex{restriction}} of $f$ to $A$.
\end{defn}

The function $f|_A$ is simply the function $f$ taken on a smaller domain.
The following proposition is the analogue of taking a tail of a sequence.

\begin{prop} \label{prop:limrest}
Let $S \subset \R$, $c \in \R$, and
let $f \colon S
\to \R$ be a function.
Suppose
$A \subset S$ is such that there is some $\alpha > 0$ such that
$(A \setminus \{ c \}) \cap (c-\alpha,c+\alpha) = (S \setminus \{ c \}) \cap (c-\alpha,c+\alpha)$.
\begin{enumerate}[(i)]
\item The point $c$ is a cluster point of $A$ if and only if $c$ is a cluster point
of $S$.
\item Supposing $c$ is a cluster point of $S$, then $f(x) \to L$ as $x \to c$ if and only if
$f|_A(x) \to L$ as $x \to c$.
\end{enumerate}
\end{prop}

\begin{proof}
First, let $c$ be a cluster point of $A$.
Since $A \subset S$, then if $( A \setminus \{ c\} ) \cap
(c-\epsilon,c+\epsilon)$ is nonempty for every $\epsilon > 0$,
then $( S \setminus \{ c\} ) \cap
(c-\epsilon,c+\epsilon)$ is nonempty for every $\epsilon > 0$.
Thus $c$ is a cluster point of $S$.
Second, suppose $c$ is a cluster
point of $S$.  Then for $\epsilon > 0$ such that $\epsilon < \alpha$
we get that $( A \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon) =
( S \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon)$, which is nonempty.  This is true for all
$\epsilon < \alpha$ and hence 
$( A \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon)$ must be nonempty for all
$\epsilon > 0$.  Thus $c$ is a cluster point of $A$.

Now suppose $f(x) \to L$ as $x \to c$.  That is, for every $\epsilon > 0$
there is a $\delta > 0$ such that if $x \in S \setminus \{ c \}$
and $\abs{x-c} < \delta$, then $\abs{f(x)-L} < \epsilon$.  Because $A \subset S$,
if $x$ is in $A \setminus \{ c \}$, then $x$ is in $S \setminus \{ c
\}$, and hence $f|_A(x) \to L$ as $x \to c$.

Finally suppose $f|_A(x) \to L$ as $x \to c$.
For every $\epsilon > 0$
there is a $\delta' > 0$ such that if $x \in A \setminus \{ c \}$
and $\abs{x-c} < \delta'$, then $\bigl\lvert f|_A(x)-L \bigr\rvert < \epsilon$.
Take $\delta := \min \{ \delta', \alpha \}$.
%It is still true if $\delta$ is smaller, so ensure $\delta \leq \alpha$.
%Without loss of generality assume $\delta \leq \alpha$.
%If 
%$\delta > \alpha$, then set $\delta := \alpha$.
Now suppose $x \in S \setminus \{ c \}$ and
$\abs{x-c} < \delta$.  As $\abs{x-c} < \alpha$, then $x \in A \setminus \{ c \}$,
and as $\abs{x-c} < \delta'$, 
we have $\abs{f(x)-L} = \bigl\lvert f|_A(x)-L
\bigr\rvert < \epsilon$.
\end{proof}

The hypothesis of the proposition is necessary.  For an arbitrary
restriction we generally only get implication in only one direction,
see \exerciseref{exercise:restrictionlimitexercise}.  

The usual notation for the limit is
\begin{equation*}
\lim_{\substack{x \to c\\x \in A}} f(x) := \lim_{x \to c} f|_A(x) .
\end{equation*}
The most common use of restriction with respect to limits
are the \emph{\myindex{one-sided limits}}%
\footnote{%
There are a plethora of notations for one sided limits.  E.g.\ for
$\lim\limits_{x \to c^-} f(x)$ one sees
$\lim\limits_{\substack{x \to c\\x < c}} f(x)$,
$\lim\limits_{x \uparrow c} f(x)$, or
$\lim\limits_{x \nearrow c} f(x)$.}.

\begin{defn} \label{defn:onesidedlimits}
Let $f \colon S \to \R$ be function and let $c$ be a cluster point of
$S \cap (c,\infty)$.  Then if the limit
of the restriction of $f$ to $S \cap (c,\infty)$ 
 as $x \to c$ exists, define
\begin{equation*}
\lim_{x \to c^+} f(x) := \lim_{x\to c} f|_{S \cap (c,\infty)}(x) .
\end{equation*}
Similarly if $c$ is a cluster point of 
$S \cap (-\infty,c)$ and the limit of the restriction as $x \to c$
exists, define
\begin{equation*}
\lim_{x \to c^-} f(x) := \lim_{x\to c} f|_{S \cap (-\infty,c)}(x) .
\end{equation*}
\end{defn}

The proposition above does not apply to one-sided limits.
It is possible to have one-sided limits, but no limit at a point.  For
example, define $f \colon \R \to \R$ by $f(x) := 1$ for $x < 0$ and
$f(x) :=
0$ for $x \geq 0$.  We leave it to the reader to verify that
\begin{equation*}
\lim_{x \to 0^-} f(x) = 1, \qquad
\lim_{x \to 0^+} f(x) = 0, \qquad
\lim_{x \to 0} f(x) \quad \text{does not exist.}
\end{equation*}
We have the following replacement.


\begin{prop} \label{prop:onesidedlimits}
Let $S \subset \R$ be a set such that $c$ is a cluster point
of both $S \cap (-\infty,c)$ and $S \cap (c,\infty)$, and let
$f \colon S \to \R$ be a function.  Then $c$ is a cluster point of $S$ and
\begin{equation*}
\lim_{x \to c} f(x) = L
\qquad \text{if and only if} \qquad
\lim_{x \to c^-} f(x) =
\lim_{x \to c^+} f(x) =
L .
\end{equation*}
\end{prop}

That is, a limit exists if both one-sided limits exist and are equal, and
vice-versa.  The
proof is a straightforward application of the definition of limit
and is left as an exercise.  The key point is that
$\bigl( S \cap (-\infty,c) \bigr) \cup \bigl( S \cap (c,\infty) \bigr)
= S \setminus \{ c \}$.

\subsection{Exercises}

\begin{exercise}
Find the limit or prove that the limit does not exist

\medskip

\noindent
\begin{tabular}{lllll}
a)
$\displaystyle
\lim_{x\to c} \sqrt{x}
$, for $c \geq 0$
& &
b)
$\displaystyle
\lim_{x\to c} x^2+x+1
$, for any $c \in \R$
& &
c)
$\displaystyle
\lim_{x\to 0} x^2 \cos (\nicefrac{1}{x})
$
\\
d)
$\displaystyle
\lim_{x\to 0}\, \sin(\nicefrac{1}{x}) \cos (\nicefrac{1}{x})
$
& &
e)
$\displaystyle
\lim_{x\to 0}\, \sin(x) \cos (\nicefrac{1}{x})
$ & 
\end{tabular}
\end{exercise}

\begin{exercise}
Prove \corref{fconstineq:cor}.
\end{exercise}

\begin{exercise}
Prove \corref{fsqueeze:cor}.
\end{exercise}

\begin{exercise}
Prove \corref{falg:cor}.
\end{exercise}

\begin{exercise}
Let $A \subset S$.  Show that if $c$ is a cluster point of $A$, then $c$
is a cluster point of $S$.  Note the difference from
\propref{prop:limrest}.
\end{exercise}

\begin{exercise} \label{exercise:restrictionlimitexercise}
Let $A \subset S$.  Suppose $c$ is a cluster point of $A$ and
it is also a cluster point of $S$.
Let $f \colon S \to \R$ be a function.  Show that if
$f(x) \to L$ as $x \to c$, then
$f|_A(x) \to L$ as $x \to c$.
Note the difference from
\propref{prop:limrest}.
\end{exercise}

\begin{exercise}
Find an example of a function $f \colon [-1,1] \to \R$ such that
for $A:=[0,1]$, the restriction
$f|_A(x) \to 0$ as $x \to 0$, but the limit of $f(x)$ as $x \to 0$
does not exist.  Note why you cannot apply
\propref{prop:limrest}.
\end{exercise}

\begin{exercise}
Find example functions $f$ and $g$ such that the limit of neither $f(x)$
nor $g(x)$ exists as $x \to 0$, but such that the limit of $f(x)+g(x)$ exists
as $x \to 0$.
\end{exercise}

\begin{exercise} \label{exercise:contlimitcomposition}
Let $c_1$ be a cluster point of $A \subset \R$ and $c_2$ be
a cluster point of $B \subset \R$.  Suppose 
$f \colon A \to B$ and $g \colon B \to \R$ are functions
such that
$f(x) \to c_2$ as $x \to c_1$ and
$g(y) \to L$ as $y \to c_2$.  If $c_2 \in B$ also suppose that $g(c_2) = L$.  Let $h(x) := g\bigl(f(x)\bigr)$ and show
$h(x) \to L$ as $x \to c_1$.
Hint: Note that $f(x)$ could equal $c_2$ for many $x \in A$,
see also
\exerciseref{exercise:contlimitbadcomposition}.
\end{exercise}

\begin{exercise}
Let $c$ be a cluster point of $A \subset \R$, and $f \colon A \to \R$
be a function.  Suppose for every sequence $\{x_n\}$ in $A$,
such that $\lim\, x_n = c$,
the sequence $\{ f(x_n) \}_{n=1}^\infty$ is Cauchy.  Prove that
$\lim_{x\to c} f(x)$ exists.
\end{exercise}

\begin{exercise} \label{exercise:seqflimitalt}
Prove the following stronger version of one direction of
\lemmaref{seqflimit:lemma}:
Let $S \subset \R$, $c$ be a cluster point of $S$, and $f \colon S \to
\R$ be a function.
Suppose that for every sequence $\{x_n\}$ in $S \setminus \{c\}$ such that
$\lim\, x_n = c$ the sequence $\{ f(x_n) \}$ is convergent.
Then show $f(x) \to L$ as $x \to c$ for some $L \in \R$.
\end{exercise}

\begin{exercise}
Prove \propref{prop:onesidedlimits}.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$ and $c$ is a cluster point of $S$.  Suppose $f \colon
S \to \R$ is bounded.  Show that there exists a sequence $\{ x_n \}$
with $x_n \in S \setminus \{ c \}$ and $\lim\, x_n = c$ such that
$\{ f(x_n) \}$ converges.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:contlimitbadcomposition}
Show that the hypothesis that $g(c_2) = L$ in
\exerciseref{exercise:contlimitcomposition} is necessary.  That is, find $f$
and $g$ such that $f(x) \to c_2$ as $x \to c_1$ and
$g(y) \to L$ as $y \to c_2$, but $g\bigl(f(x)\bigr)$ does not go to $L$
as $x \to c_1$.
\end{exercise}

\begin{exercise}
Show that the condition of being a cluster point is necessary to have a
reasonable definition of a limit.  That is, suppose $c$ is not a cluster
point of $S \subset \R$, and $f \colon S \to \R$ is a function.  Show that
every $L$ would satisfy the definition of limit at $c$ without the condition
on $c$ being a cluster point.
\end{exercise}

\begin{exercise}
a) Prove \corref{fabs:cor}.  b) Find an example showing that the converse of
the corollary does not hold.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Continuous functions}
\label{sec:cont}

\sectionnotes{2--2.5 lectures}

You undoubtedly heard of continuous functions in your schooling.  A
high-school criterion for this concept is that a function is continuous if
we can draw its graph without lifting the pen from the paper.  While that
intuitive concept may be useful in simple situations, we require
rigor.  The following definition took three great mathematicians
(Bolzano, Cauchy, and finally Weierstrass) to get correctly and its final
form dates only to the late 1800s.

\subsection{Definition and basic properties}

\begin{defn}
Let $S \subset \R$, $c \in S$, and let $f \colon S \to \R$ be a function.
We say
that $f$ is \emph{continuous at $c$}\index{continuous at $c$}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x \in S$ and $\abs{x-c} <
\delta$, then
$\abs{f(x)-f(c)} < \epsilon$.

%\medskip

When $f \colon S \to \R$ is continuous at all $c \in S$, then we simply say
$f$ is a \emph{\myindex{continuous function}}.
\end{defn}
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{contigr.pdf_t}
\caption{For $\abs{x-c} < \delta$, $f(x)$ should be within the gray region.\label{fig:contigr}}
%\end{center}
\end{myfigureht}

If $f$ is continuous for all $c \in A$, we say
$f$ is continuous on $A \subset S$.  It is left as an easy exercise to
show that this implies that $f|_A$ is continuous, although
the converse does not hold.

Continuity may be the most important definition to understand in analysis,
and it is not an easy one.  See \figureref{fig:contigr}.  Note that $\delta$ not only
depends on $\epsilon$, but also on $c$;  we need not pick
one $\delta$ for all $c \in S$.
It is no accident 
that the definition of continuity is similar to the definition of a
limit of a function.  The main feature of continuous functions
is that these are precisely the functions that behave nicely with limits.

\enlargethispage{\baselineskip}
\begin{prop} \label{contbasic:prop}
Let $S \subset \R$, let $f \colon S \to \R$ be a function, and let $c \in S$
be a point.
Then
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
\begin{enumerate}[(i)]
\item If $c$ is not a cluster point of $S$, then $f$ is continuous at $c$.
\item If $c$ is a cluster point of $S$, then $f$ is continuous at $c$
if and only if the limit of $f(x)$ as $x \to c$ exists and
\begin{equation*}
\lim_{x\to c} f(x) = f(c) .
\end{equation*}
\item $f$ is continuous at $c$ if and only if for every sequence $\{ x_n \}$
where $x_n \in S$ and $\lim\, x_n = c$, the sequence $\{ f(x_n) \}$ converges
to $f(c)$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.  Suppose $c$ is not a cluster point of
$S$.  Then there exists a $\delta > 0$
such that $S \cap (c-\delta,c+\delta) = \{
c \}$.  Therefore, for any $\epsilon > 0$, simply pick this given delta.
The only $x \in S$ such that $\abs{x-c} < \delta$ is $x=c$.  Then
$\abs{f(x)-f(c)} = \abs{f(c)-f(c)} = 0 < \epsilon$.

Let us move to the second item.
Suppose $c$ is a cluster point of $S$.  Let us first suppose
that $\lim_{x\to c} f(x) = f(c)$.  Then for every $\epsilon > 0$
there is a $\delta > 0$ such that if $x \in S \setminus \{ c \}$
and $\abs{x-c} < \delta$, then $\abs{f(x)-f(c)} < \epsilon$.
Also $\abs{f(c)-f(c)} = 0 < \epsilon$, so the definition of continuity at
$c$ is satisfied.  On the other hand, suppose $f$ is continuous
at $c$.  For every $\epsilon > 0$, there exists a $\delta > 0$
such that for $x \in S$ where $\abs{x-c} < \delta$ we have
$\abs{f(x)-f(c)} < \epsilon$.  Then the statement is, of course, still true if
$x \in S \setminus \{ c \} \subset S$.  Therefore $\lim_{x\to c} f(x) =
f(c)$.

For the third item, first suppose $f$ is continuous at $c$.  Let $\{ x_n \}$
be a sequence such that $x_n \in S$ and $\lim\, x_n = c$.  Let $\epsilon > 0$
be given.  Find a $\delta > 0$ such that $\abs{f(x)-f(c)} < \epsilon$
for all $x \in S$ where $\abs{x-c} < \delta$.  Find an $M \in \N$
such that for $n \geq M$ we have $\abs{x_n-c} < \delta$.  Then for
$n \geq M$ we have that $\abs{f(x_n)-f(c)} < \epsilon$, so $\{ f(x_n) \}$
converges to $f(c)$.

Let us prove the other direction of the third item by contrapositive.
Suppose $f$ is not
continuous at $c$.  Then there exists an $\epsilon > 0$
such that for all $\delta > 0$, there exists an $x \in S$
such that $\abs{x-c} < \delta$ and $\abs{f(x)-f(c)} \geq \epsilon$.
Let us define a sequence $\{ x_n \}$ as follows.
Let $x_n \in S$ be such that $\abs{x_n-c} < \nicefrac{1}{n}$
and $\abs{f(x_n)-f(c)} \geq \epsilon$.
%As $f$ is not continuous at $c$,
%we can do this.
Now $\{ x_n \}$ is
a sequence of numbers in $S$ such that
$\lim\, x_n = c$ and such that
$\abs{f(x_n)-f(c)} \geq \epsilon$ for all $n \in \N$.  Thus $\{ f(x_n) \}$
does not converge to $f(c)$.  It may or may not converge, but it definitely
does not converge to $f(c)$.  
\end{proof}

The last item in the proposition is particularly powerful.  It allows us to
quickly apply what we know about limits of sequences to continuous functions
and even to prove that certain functions are continuous.
It can also be strengthened, see \exerciseref{exercise:contseqalt}.

\begin{example}
$f \colon (0,\infty) \to \R$ defined by
$f(x) := \nicefrac{1}{x}$ is continuous.

Proof: Fix $c \in (0,\infty)$.  
Let $\{ x_n \}$ be a sequence in $(0,\infty)$ such that
$\lim\, x_n = c$.  Then we know that
\begin{equation*}
f(c) = \frac{1}{c}
=
\frac{1}{\lim\, x_n}
=
\lim_{n \to \infty} \frac{1}{x_n}
=
\lim_{n \to \infty} f(x_n) .
\end{equation*}
Thus $f$ is continuous at $c$.  As $f$ is continuous at all $c \in
(0,\infty)$, $f$ is continuous.
\end{example}

We have previously shown $\lim_{x \to c} x^2 = c^2$ directly.  Therefore
the function $x^2$ is continuous.  We can use the continuity of
algebraic operations with respect to limits of sequences, which we proved in
the previous chapter, to prove a much more general result.

\begin{prop}
Let $f \colon \R \to \R$ be a \emph{\myindex{polynomial}}.  That is
\begin{equation*}
f(x) = a_d x^d + a_{d-1} x^{d-1} + \cdots + a_1 x + a_0 ,
\end{equation*}
for some constants $a_0, a_1, \ldots, a_d$.
Then $f$ is continuous.
\end{prop}

\begin{proof}
Fix $c \in \R$.  
Let $\{ x_n \}$ be a sequence such that
$\lim\, x_n = c$.  Then
\begin{equation*}
\begin{split}
f(c) &=
a_d c^d + a_{d-1} c^{d-1} + \cdots + a_1 c + a_0 
\\
&= 
a_d {(\lim\, x_n)}^d + a_{d-1} {(\lim\, x_n)}^{d-1} + \cdots + a_1 (\lim\, x_n) + a_0 
\\
& =
\lim_{n \to \infty}
\left(
a_d x_n^d + a_{d-1} x_n^{d-1} + \cdots + a_1 x_n + a_0 
\right)
=
\lim_{n \to \infty}
f(x_n) .
\end{split}
\end{equation*}
Thus $f$ is continuous at $c$.  As $f$ is continuous at all $c \in \R$,
$f$ is continuous.
\end{proof}

By similar reasoning, or by appealing to \corref{falg:cor},
we can prove the following.  The details of the proof are left as an
exercise.

\begin{prop} \label{contalg:prop}
Let $f \colon S \to \R$ and $g \colon S \to \R$ be functions
continuous at $c \in S$.
\begin{enumerate}[(i)]
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)+g(x)$ is continuous at $c$.
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)-g(x)$ is continuous at $c$.
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)g(x)$ is continuous at $c$.
\item If $g(x)\not=0$ for all $x \in S$, the function $h \colon S \to \R$
defined by $h(x) := \frac{f(x)}{g(x)}$ is continuous at $c$.
\end{enumerate}
\end{prop}

\begin{example} \label{sincos:example}
The functions $\sin(x)$ and $\cos(x)$ are continuous.
In the following computations we use the sum-to-product
trigonometric identities.  We also use the simple facts that
$\abs{\sin(x)} \leq \abs{x}$, $\abs{\cos(x)} \leq 1$,
and $\abs{\sin(x)} \leq 1$.
\begin{equation*}
\begin{split}
\abs{\sin(x)-\sin(c)} & =
\abs{
2 \sin \left( \frac{x-c}{2} \right) \cos \left( \frac{x+c}{2} \right)
}
\\
& =
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\abs{ \cos \left( \frac{x+c}{2} \right) }
\\
& \leq
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\\
& \leq
2
\abs{ \frac{x-c}{2} }
= \abs{x-c}
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
\abs{\cos(x)-\cos(c)} & =
\abs{
-2 \sin \left( \frac{x-c}{2} \right) \sin \left( \frac{x+c}{2} \right)
}
\\
& =
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\abs{ \sin \left( \frac{x+c}{2} \right) }
\\
& \leq
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\\
& \leq
2
\abs{ \frac{x-c}{2} }
= \abs{x-c}
\end{split}
\end{equation*}

The claim that sin and cos are continuous follows by taking an
arbitrary sequence $\{ x_n \}$ converging to $c$, or by applying the
definition of continuity directly.  Details are left to the
reader.
\end{example}

\subsection{Composition of continuous functions}

You probably already realized that one of the basic tools in
constructing complicated functions out of simple ones is composition.
Recall that for two functions $f$ and $g$,
the composition $f \circ g$ is defined by
$(f \circ g)(x) := f\bigl(g(x)\bigr)$.
A composition of
continuous functions is again
continuous.

\begin{prop}
Let $A, B \subset \R$ and $f \colon B \to \R$ and $g \colon A \to B$ be
functions.  If $g$ is continuous at $c \in A$ and
$f$ is continuous at $g(c)$, then $f \circ g \colon A \to \R$ is continuous
at $c$.
\end{prop}

\begin{proof}
Let $\{ x_n \}$ be a sequence in $A$ such that $\lim\, x_n = c$.
As $g$ is continuous at $c$, then $\{ g(x_n) \}$ converges to $g(c)$.
As $f$ is continuous at $g(c)$, then $\{ f\bigl(g(x_n)\bigr) \}$ converges
to $f\bigl(g(c)\bigr)$.
Thus $f \circ g$ is continuous at $c$.
\end{proof}

\begin{example}
Claim: ${\bigl(\sin(\nicefrac{1}{x})\bigr)}^2$ is a continuous function on $(0,\infty)$.

Proof: First note that $\nicefrac{1}{x}$ is a continuous function on
$(0,\infty)$ and $\sin(x)$ is a continuous function on $(0,\infty)$ (actually
on all of $\R$, but $(0,\infty)$ is the range for $\nicefrac{1}{x}$).
Hence the composition $\sin(\nicefrac{1}{x})$ is continuous.  We also
know that $x^2$ is continuous on the interval $(-1,1)$ (the range of sin).  Thus
the composition
${\bigl(\sin(\nicefrac{1}{x})\bigr)}^2$ is also continuous on $(0,\infty)$.
\end{example}

\subsection{Discontinuous functions}

%Let us spend a bit of time on discontinuous functions.
When $f$ is not continuous at $c$, we
say $f$ is \emph{\myindex{discontinuous}} at $c$, or that it has a
\emph{\myindex{discontinuity}} at $c$.
The following proposition is a useful test and follows immediately
from third item of \propref{contbasic:prop}.
%which is an easy to use test for discontinuities.

\begin{prop}
Let $f \colon S \to \R$ be a function and $c \in S$.  Suppose 
there exists a sequence $\{ x_n \}$, $x_n \in S$, and $\lim\, x_n = c$
such that $\{ f(x_n) \}$ does not converge to $f(c)$.  Then $f$ is 
discontinuous at $c$.
\end{prop}

Again, 
$\{ f(x_n) \}$ may or may not converge, but
if it does, it definitely does not converge to
$f(c)$.

\begin{example} \label{example:jumpdiscont}
The function $f \colon \R \to \R$ defined by
\begin{equation*}
f(x) := 
\begin{cases}
-1 & \text{ if $x < 0$,} \\
1 & \text{ if $x \geq 0$,}
\end{cases}
\end{equation*}
is not continuous at 0.

Proof: Take the sequence $\{ - \nicefrac{1}{n} \}$, which converges to 0.  Then
$f(-\nicefrac{1}{n}) = -1$ for every $n$,
and so
$\lim\, f(-\nicefrac{1}{n}) = -1$, but $f(0) = 1$.  See
\figureref{fig:jumpdiscont}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{jumpdiscont.eepic}
\caption{Graph of the jump discontinuity.  The values of
$f(-\nicefrac{1}{n})$ and $f(0)$ are marked.\label{fig:jumpdiscont}}
%\end{center}
\end{myfigureht}

Also notice that $f(\nicefrac{1}{n}) = 1$ for every $n$,
so $\lim \, f(\nicefrac{1}{n}) = f(0) = 1$.  So
$\{ f(x_n) \}$ may converge to $f(0)$
for some specific
sequence $\{ x_n \}$ going to 0,
despite the function being discontinuous at 0.

Finally, consider $f\bigl(\frac{{(-1)}^n}{n}\bigr) = {(-1)}^n$,
and this sequence diverges.
\end{example}

\begin{example}
For an extreme example, take the so-called
\emph{\myindex{Dirichlet function}}.
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x$ is rational,} \\
0 & \text{ if $x$ is irrational.}
\end{cases}
\end{equation*}
The function $f$ is discontinuous at all $c \in \R$.

Proof:
Suppose $c$ is rational.  Take a sequence $\{ x_n \}$
of irrational numbers such that $\lim\, x_n = c$ (why can we?).  Then $f(x_n) = 0$
and so $\lim\, f(x_n) = 0$, but $f(c) = 1$.
If $c$ is irrational, take a sequence of rational numbers $\{ x_n \}$
that converges to $c$ (why can we?).  Then $\lim\, f(x_n) = 1$, but $f(c) = 0$.
\end{example}

Let us test the limits of our intuition.  Can
there exist a function continuous at all irrational numbers, but
discontinuous at all rational numbers?  There are rational numbers
arbitrarily close to any irrational number.  Perhaps strangely, the
answer is yes.  The following example is called the
\emph{\myindex{Thomae function}}\footnote{Named after the German
mathematician
\href{http://en.wikipedia.org/wiki/Thomae}{Johannes Karl Thomae}
(1840 -- 1921).} or the
\emph{\myindex{popcorn function}}.

\begin{example} \label{popcornfunction:example}
Let $f \colon (0,1) \to \R$ be defined by
\begin{equation*}
f(x) := 
\begin{cases}
\nicefrac{1}{k} & \text{ if $x=\nicefrac{m}{k}$, where $m,k \in \N$
and $m$ and $k$ have no common divisors,} \\
0 & \text{ if $x$ is irrational}.
\end{cases}
\end{equation*}
See the graph of $f$
in \figureref{popcornfig}.
We claim that
$f$ is continuous at all irrational $c$ and 
discontinuous at all rational $c$.
\begin{myfigureht}
%\begin{center}
\includegraphics{figures/popcornfig}
\caption{Graph of the ``popcorn function.''\label{popcornfig}}
%\end{center}
\end{myfigureht}

Proof:
Suppose $c = \nicefrac{m}{k}$ is rational.  Take a sequence of
irrational numbers $\{ x_n \}$ such that $\lim\, x_n = c$.  Then
$\lim\, f(x_n) = \lim \, 0 = 0$, but $f(c) = \nicefrac{1}{k} \not= 0$.  So $f$
is discontinuous at $c$.

Now let $c$ be irrational, so $f(c) = 0$.  Take a sequence 
$\{ x_n \}$ in $(0,1)$ such that $\lim\, x_n = c$.
Given $\epsilon > 0$, find $K \in \N$ such
that $\nicefrac{1}{K} < \epsilon$
by the \hyperref[thm:arch:i]{Archimedean property}.
If $\nicefrac{m}{k} \in (0,1)$ is in lowest terms
(no common divisors), then $m < k$.
So there are only finitely many rational numbers in $(0,1)$
whose denominator $k$ in lowest terms is less than $K$.  Hence
there is an $M$ such that for $n \geq M$, all the numbers $x_n$
that are rational
have a denominator larger than or equal to $K$.  Thus for $n \geq M$,
\begin{equation*}
\abs{f(x_n) - 0} = f(x_n) \leq \nicefrac{1}{K} < \epsilon .
\end{equation*}
Therefore $f$ is continuous at irrational $c$.
\end{example}

Let us end on an easier example.

\begin{example}
Define
$g \colon \R \to \R$ by $g(x) := 0$ if $x \not= 0$ and
$g(0) := 1$.  Then $g$ is not continuous at zero, but continuous everywhere else (why?).
The point $x=0$ is called a \emph{\myindex{removable discontinuity}}.  That
is because if we would change the definition of $g$, by insisting that
$g(0)$ be $0$, we would obtain a continuous function.  On the other hand
let $f$ be the function of example \exampleref{example:jumpdiscont}.
Then $f$ does not have a
removable discontinuity at $0$.  No matter how we would define $f(0)$ the function
will still fail to be continuous.  The difference is that 
$\lim_{x\to 0} g(x)$ exists while
$\lim_{x\to 0} f(x)$ does not.

Let us stay with this example but show another phenomenon.  Let $A = \{ 0
\}$, then $g|_A$ is continuous (why?), while $g$ is not continuous on $A$.
\end{example}

\subsection{Exercises}

\begin{exercise}
Using the definition of continuity directly prove that
$f \colon \R \to \R$ defined by
$f(x) := x^2$ is continuous.
\end{exercise}

\begin{exercise}
Using the definition of continuity directly prove that
$f \colon (0,\infty) \to \R$ defined by
$f(x) := \nicefrac{1}{x}$ is continuous.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be defined by
\begin{equation*}
f(x) :=
\begin{cases}
x & \text{ if $x$ is rational,} \\
x^2 & \text{ if $x$ is irrational.}
\end{cases}
\end{equation*}
Using the definition of continuity directly prove that
$f$ is continuous at $1$ and discontinuous at $2$.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be
defined by
\begin{equation*}
f(x) :=
\begin{cases}
\sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Is $f$ continuous?  Prove your assertion.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be
defined by
\begin{equation*}
f(x) :=
\begin{cases}
x \sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Is $f$ continuous?  Prove your assertion.
\end{exercise}

\begin{exercise}
Prove \propref{contalg:prop}.
\end{exercise}

\begin{exercise}
Prove the following statement.
Let $S \subset \R$ and $A \subset S$.  Let $f \colon S \to \R$
be a continuous function.
Then the restriction $f|_A$ is continuous.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$.  Suppose for some $c \in \R$
and $\alpha > 0$, we have $A=(c-\alpha,c+\alpha) \subset S$.
Let $f \colon S \to \R$ be a function.  Prove that
if $f|_A$ is continuous at $c$, then $f$ is continuous at $c$.
\end{exercise}

\begin{exercise}
Give an example of functions $f \colon \R \to \R$ and $g \colon \R \to \R$
such that the function $h$ defined by $h(x) := f(x) + g(x)$ is continuous,
but $f$ and $g$ are not continuous.  Can you find $f$ and $g$ that are nowhere
continuous, but $h$ is a continuous function?
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ and 
$g \colon \R \to \R$ be continuous functions.  Suppose that for
all rational numbers $r$, $f(r) = g(r)$.  Show that $f(x) = g(x)$ for all
$x$.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be continuous.  Suppose $f(c) > 0$.  Show that
there exists an $\alpha > 0$ such that for all $x \in (c-\alpha,c+\alpha)$
we have $f(x) > 0$.
\end{exercise}

\begin{exercise}
Let $f \colon \Z \to \R$ be a function.  Show that $f$ is continuous.
\end{exercise}

\begin{exercise} \label{exercise:contseqalt}
Let $f \colon S \to \R$ be a function and $c \in S$, such that for every
sequence $\{ x_n \}$ in $S$ with $\lim\, x_n = c$, the sequence
$\{ f(x_n) \}$ converges.  Show that $f$ is continuous at $c$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [-1,0] \to \R$ and $g \colon [0,1] \to \R$ are continuous
and $f(0) = g(0)$.  Define $h \colon [-1,1] \to \R$ by 
$h(x) := f(x)$ if $x \leq 0$ and $h(x) := g(x)$ if $x > 0$.  Show that
$h$ is continuous.
\end{exercise}

\begin{exercise}
Suppose $g \colon \R \to \R$ is a continuous function such that $g(0) = 0$,
and supppse $f \colon \R \to \R$ is such that
$\abs{f(x)-f(y)} \leq g(x-y)$ for all $x$ and $y$.  Show that $f$ is
continuous.
\end{exercise}

\begin{exercise}[Challenging]
Suppose $f(x+y) = f(x) + f(y)$ for some $f \colon \R \to \R$
such that $f$ is continuous at 0.
Show that $f(x) = ax$ for some $a \in \R$.
Hint: Show that $f(nx) = nf(x)$, then show $f$ is continuous on $\R$.
Then show that $\nicefrac{f(x)}{x} = f(1)$ for all rational $x$.
\end{exercise}

\begin{exercise} \label{exercise:minmaxcont}
Suppose $S \subset \R$ and
let $f \colon S \to \R$ and
$g \colon S \to \R$ be continuous functions.
Define $p \colon S \to \R$ by
$p(x) := \max \{ f(x) , g(x) \}$ and
$q \colon S \to \R$ by
$q(x) := \min \{ f(x) , g(x) \}$.  Prove that $p$ and $q$ are
continuous.
\end{exercise}

\begin{exercise}
Suppose $f \colon [-1,1] \to \R$ is a function continuous at all $x \in
[-1,1] \setminus \{ 0 \}$.  Show that for every $\epsilon$ such
that $0 < \epsilon < 1$, there exists
a function $g \colon [-1,1] \to \R$ continuous on all of $[-1,1]$, such that
$f(x) = g(x)$ for all $x \in [-1,-\epsilon] \cup [\epsilon,1]$, and 
$\abs{g(x)} \leq \abs{f(x)}$ for all $x \in [-1,1]$.
\end{exercise}

\begin{exercise}[Challenging]
A function $f \colon I \to \R$ is \emph{\myindex{convex}} if
whenever $a \leq x \leq b$ for $a,x,b$ in $I$, we have
$f(x) \leq f(a) \frac{b-x}{b-a} + f(b) \frac{x-a}{b-a}$.  In other words,
if the line drawn between $\bigl(a,f(a)\bigr)$ and $\bigl(b,f(b)\bigr)$ 
is above the graph of $f$.\\
a) If $I = (a,b)$ an open interval and $f \colon I \to \R$ is convex,
then prove that $f$ is continuous.
\\
b) Find an example of a convex $f \colon [0,1] \to \R$ which is
not continuous.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Min-max and intermediate value theorems}
\label{sec:minmaxint}

\sectionnotes{1.5 lectures}

Continuous functions on closed and bounded intervals
are quite well behaved.
%have some interesting and very useful properties.
%Let us now state and prove some very important results about continuous
%functions defined on closed and bounded intervals of the real line.
%In particular, on closed bounded
%intervals of
%the real line.

\subsection{Min-max theorem}

Recall a function $f \colon [a,b] \to \R$ is
\emph{bounded\index{bounded function}} if there exists a $B \in \R$ such that
$\abs{f(x)} \leq B$ for all $x \in [a,b]$.  We have the following lemma.

\begin{lemma}
Let $f \colon [a,b] \to \R$ be a continuous function.  Then $f$ is bounded.
\end{lemma}

\begin{proof}
Let us prove this claim by contrapositive.  Suppose $f$ is not bounded.
Then for each
$n \in \N$, there is an $x_n \in [a,b]$, such that
\begin{equation*}
\abs{f(x_n)} \geq n .
\end{equation*}
The sequenece $\{ x_n \}$ is bounded as $a \leq x_n \leq b$.
By the \hyperref[thm:bwseq]{Bolzano--Weierstrass theorem},
there is a convergent subsequence $\{ x_{n_i} \}$.  Let $x := \lim\, x_{n_i}$.
Since $a \leq x_{n_i} \leq b$ for all $i$, then $a \leq x \leq b$.
The sequence $\{ f(x_{n_i}) \}$ is not bounded 
%$\lim\, f(x_{n_i})$ does
as 
$\abs{f(x_{n_i})} \geq n_i \geq i$.
Thus $f$ is not continuous at $x$ as
%so $\lim\, f(x_{n_i})$ does not exist.
%On the other hand $f(x)$ is a finite number and
\begin{equation*}
f(x)
=
f\Bigl( \lim_{i\to\infty} x_{n_i} \Bigr) ,
\qquad\text{but}\qquad
\lim_{i\to\infty} f(x_{n_i}) ~~\text{does not exist.} \qedhere
\end{equation*}
\end{proof}

%In fact, for a continuous $f$, we will see that the minimum and
%the maximum are actually achieved.
Recall from calculus that $f \colon S \to \R$ achieves an
\emph{\myindex{absolute minimum}} at $c \in S$ if
\begin{equation*}
f(x) \geq f(c) \qquad \text{ for all $x \in S$.}
\end{equation*}
On the other hand, $f$ achieves an 
\emph{\myindex{absolute maximum}} at $c \in S$ if
\begin{equation*}
f(x) \leq f(c) \qquad \text{ for all $x \in S$.}
\end{equation*}
If such a $c \in S$ exists, then 
$f$ \emph{achieves an absolute minimum (resp.\ absolute maximum) on
$S$}.
%See \figureref{fig:minmax}.
\begin{myfigureht}
%\begin{center}
% to get the fonts right input eepic
\subimport*{figures/}{minmax.eepic}
\caption{$f \colon [a,b] \to \R$ achieves an absolute maximum $f(c)$ at
$c$, and an absolute minimum $f(d)$ at $d$.\label{fig:minmax}}
%\end{center}
\end{myfigureht}

If $S$ is a closed
and bounded interval, then a continuous $f$
must achieve an absolute minimum and an absolute
maximum on $S$.

\begin{thm}[Minimum-maximum theorem]
\index{Minimum-maximum theorem}
\index{Maximum-minimum theorem}
Let $f \colon [a,b] \to \R$ be a continuous function.  Then $f$
achieves both an absolute minimum and an absolute maximum on $[a,b]$.
\end{thm}

\begin{proof}
The lemma says that $f$ is bounded, and thus
the set $f([a,b]) = \{ f(x) : x \in [a,b] \}$ has a supremum and an infimum.
There exist sequences
in the set $f([a,b])$ that approach its supremum and its infimum.
That is, there are sequences
$\{ f(x_n) \}$ and $\{ f(y_n) \}$, where $x_n, y_n$ are in $[a,b]$,
such that
\begin{equation*}
\lim_{n\to\infty} f(x_n) = \inf f([a,b]) \qquad \text{and} \qquad
\lim_{n\to\infty} f(y_n) = \sup f([a,b]).
\end{equation*}
We are not done yet, we need to find where the minima and the maxima are.
The problem is that the sequences $\{ x_n \}$ and $\{ y_n \}$ need not
converge.
We know $\{ x_n \}$ and $\{ y_n \}$ are bounded (their elements
belong to 
a bounded interval $[a,b]$).
Apply the 
\hyperref[thm:bwseq]{Bolzano--Weierstrass theorem},
to find
convergent subsequences
$\{ x_{n_i} \}$ and 
$\{ y_{m_i} \}$.  Let
\begin{equation*}
x := \lim_{i\to\infty} x_{n_i}
\qquad \text{and} \qquad
y := \lim_{i\to\infty} y_{m_i}.
\end{equation*}
As $a \leq x_{n_i} \leq b$, we have $a \leq x \leq b$,
and similarly $a \leq y \leq b$.  So $x$ and $y$ are in $[a,b]$.
A limit of a subsequence is the same as the limit of the
sequence, and we can take a limit past the continuous function $f$:
\begin{equation*}
\inf f([a,b]) = \lim_{n\to\infty} f(x_n)
= \lim_{i\to\infty} f(x_{n_i}) = 
f \Bigl( \lim_{i\to\infty} x_{n_i} \Bigr) = f(x) .
\end{equation*}
Similarly,
\begin{equation*}
\sup f([a,b]) = \lim_{n\to\infty} f(m_n)
= \lim_{i\to\infty} f(y_{m_i}) = 
f \Bigl( \lim_{i\to\infty} y_{m_i} \Bigr) = f(y) .
\end{equation*}
Therefore, $f$ achieves an absolute minimum at $x$ and
$f$ achieves an absolute maximum at $y$.
\end{proof}

\begin{example}
The function $f(x) := x^2+1$ defined on the interval $[-1,2]$ achieves a minimum
at $x=0$ when $f(0) = 1$.  It achieves a maximum at $x=2$ where $f(2) = 5$.
Do note that the domain of definition matters.  If we instead took the domain
to be $[-10,10]$, then $x=2$ would no longer be a maximum of $f$.  Instead
the maximum would be achieved at either $x=10$ or $x=-10$.
\end{example}

Let us show by examples that the different hypotheses of the theorem are
truly needed.

\begin{example}
The function $f(x) := x$, defined on the whole real line,
achieves neither a minimum, nor a maximum.  So it is important that
we are looking at a bounded interval.
\end{example}

\begin{example}
The function $f(x) := \nicefrac{1}{x}$, defined on $(0,1)$ 
achieves neither a minimum, nor a maximum.  The values of the function are
unbounded as we approach 0.  Also as we approach $x=1$, the values of the
function approach 1, but $f(x) > 1$ for all $x \in (0,1)$.  There is
no $x \in (0,1)$ such that $f(x) = 1$.  So it is important that
we are looking at a closed interval.
\end{example}

\begin{example}
Continuity is important.
Define $f \colon [0,1] \to \R$ by 
$f(x) := \nicefrac{1}{x}$ for $x > 0$ and let $f(0) := 0$.
The function does not achieve a maximum.  The problem is that
the function is not continuous at 0.
\end{example}

\subsection{Bolzano's intermediate value theorem}

Bolzano's intermediate value theorem is one of the cornerstones of analysis.
It is sometimes called only intermediate value theorem, or just
Bolzano's theorem.  To prove Bolzano's theorem we prove the
following simpler lemma.

\begin{lemma} \label{IVT:lemma}
Let $f \colon [a,b] \to \R$ be a continuous function.
Suppose $f(a) < 0$ and $f(b) > 0$. 
Then there exists a number $c \in (a,b)$
such that $f(c) = 0$.
\end{lemma}

\begin{proof}
We define two sequences $\{ a_n \}$
and $\{ b_n \}$ inductively:
\begin{enumerate}[(i)]
\item Let $a_1 := a$ and $b_1 := b$.
\item If $f\left(\frac{a_n+b_n}{2}\right) \geq 0$, let $a_{n+1} := a_n$ and
$b_{n+1} := \frac{a_n+b_n}{2}$.
\item If $f\left(\frac{a_n+b_n}{2}\right) < 0$, let $a_{n+1} := \frac{a_n+b_n}{2}$ and
$b_{n+1} := b_n$.
\end{enumerate}
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{bisect.eepic}
\caption{Finding roots (bisection method).\label{bisectfig}}
%\end{center}
\end{myfigureht}
See \figureref{bisectfig} for an example defining the first five steps.
If $a_n < b_n$, then $a_n < \frac{a_n+b_n}{2} < b_n$.  So
$a_{n+1} < b_{n+1}$.
Thus by \hyperref[induction:thm]{induction} $a_n < b_n$ for all $n$.
Furthermore, $a_n \leq a_{n+1}$ and 
$b_n \geq b_{n+1}$ for all $n$, that is the sequences are monotone.
As $a_n < b_n \leq b_1 = b$ and 
$b_n > a_n \geq a_1 = a$ for all $n$,
the sequences are also bounded.  Therefore, the
sequences converge.  Let $c := \lim\, a_n$ and $d := \lim\, b_n$,
where also $a \leq c \leq d \leq b$.  We need
to show that $c=d$.
Notice
\begin{equation*}
b_{n+1} - a_{n+1} = \frac{b_n-a_n}{2}.
\end{equation*}
By \hyperref[induction:thm]{induction},
\begin{equation*}
b_n - a_n = \frac{b_1-a_1}{2^{n-1}} = 2^{1-n} (b-a) .
\end{equation*}
%  As $a_n < b_n$ for all
%$n$, then $c \leq d$.  Furthermore, as $a_n$ is increasing and $b_n$
%is decreasing, $c$ is the supremum of $a_n$ and $d$ is the infimum
%of the $b_n$.  Thus $d-c \leq b_n - a_n$ for all $n$.  So
%\begin{equation*}
%\abs{d-c} =
%d-c \leq b_n - a_n = 2^{1-n} (b-a) 
%\end{equation*}
%for all $n$.  As $2^{1-n}(b-a) \to 0$ as $n \to \infty$, we see that
%$c = d$.
As $2^{1-n}(b-a)$ converges to zero, we take the limit as $n$ goes to
infinity to get
\begin{equation*}
d-c = \lim_{n\to\infty} (b_n - a_n) =
\lim_{n\to\infty} 2^{1-n} (b-a) = 0.
\end{equation*}
In other words $d=c$.

By construction, for all $n$ we have
\begin{equation*}
f(a_n) < 0
\qquad \text{and} \qquad
f(b_n) \geq 0 .
\end{equation*}
Since
$\lim\, a_n = \lim\, b_n = c$
and as $f$ is continuous, we may take 
limits in those inequalities:
\begin{equation*}
f(c) = \lim\, f(a_n) \leq 0
\qquad \text{and} \qquad
f(c) = \lim\, f(b_n) \geq 0 .
\end{equation*}
As $f(c) \geq 0$ and 
$f(c) \leq 0$, we conclude $f(c) = 0$.
Thus also $c \not=a$ and $c \not= b$, so
$a < c < b$.
\end{proof}

\begin{thm}[Bolzano's intermediate value theorem] \label{IVT:thm}
\index{Bolzano's theorem}
\index{Bolzano's intermediate value theorem}
\index{intermediate value theorem}
Let $f \colon [a,b] \to \R$ be a continuous function.
Suppose $y \in \R$ is such that $f(a) < y < f(b)$
or $f(a) > y > f(b)$.  Then there exists a $c \in (a,b)$
such that $f(c) = y$.
\end{thm}

The theorem says that a continuous function on a closed interval
achieves all the values between the values at the endpoints.

\begin{proof}
If $f(a) < y < f(b)$, then define $g(x) := f(x)-y$.  Then we see
that $g(a) < 0$ and $g(b) > 0$ and we apply \lemmaref{IVT:lemma}
to $g$ to find $c$.  If $g(c) = 0$, then $f(c) = y$.

Similarly if $f(a) > y > f(b)$, then define $g(x) := y-f(x)$.  Then
again $g(a) < 0$ and $g(b) > 0$ and we apply \lemmaref{IVT:lemma} to
find $c$.
Again if $g(c) = 0$, then $f(c) = y$.
\end{proof}

If a function is continuous, then the restriction
to a subset is continuous.  So if $f \colon S \to \R$ is continuous and
$[a,b] \subset S$, then $f|_{[a,b]}$ is also continuous.  Hence, we generally
apply the theorem to a function continuous on some large set $S$,
but we restrict attention to an interval.


The proof of the lemma tells us how to find the root $c$.  The
proof is not only useful for us pure mathematicians,
but it is a useful idea in applied mathematics,
where it is called the \emph{\myindex{bisection method}}.


\begin{example}[Bisection method] %\index{bisection method}
The polynomial $f(x) := x^3-2x^2+x-1$ has a real root in $(1,2)$.  We simply
notice that $f(1) = -1$ and $f(2) = 1$.  Hence there must exist a point $c
\in (1,2)$ such that $f(c) = 0$.  To find a better approximation of
the root we follow the proof of \lemmaref{IVT:lemma}.  
We look at 1.5 and find that $f(1.5) = -0.625$.  Therefore,
there is a root of the polynomial in $(1.5,2)$.  Next we look at 1.75
and note that $f(1.75) \approx -0.016$.  Hence there is a root of $f$ in
$(1.75,2)$.  Next we look at 1.875 and find that $f(1.875) \approx 0.44$,
thus there is a root in $(1.75,1.875)$.  We follow this procedure until we gain
sufficient precision.  In fact, the root is at $c \approx 1.7549$.
\end{example}

The technique above is the simplest method of finding roots of polynomials,
which is perhaps the most common problem in applied
mathematics.  In general it is hard to do quickly, precisely,
and automatically.
%We can use the
%technique
%intermediate value theorem
%to find roots for any continuous function, not just a polynomial.

There are better and faster methods of finding roots of equations, such
as Newton's method.  One advantage of the above method is its
simplicity.  The
moment we find an initial interval where the intermediate value theorem
applies, we are guaranteed to find a root up to a desired
precision in finitely many steps.  Furthermore, the bisection
method finds
roots of any
a continuous function, not just a polynomial.

The theorem guarantees at least one $c$ such that $f(c) = y$, but there
may be many different roots of the equation $f(c) = y$.  If we follow
the procedure of the proof, we are guaranteed to find approximations to
one such root.  We need to work harder to find any other roots.

\medskip

Polynomials of even degree may not have any real roots.  For example,
there is no real number $x$ such that $x^2+1 = 0$.  Odd polynomials, on the
other hand, always have at least one real root.

\begin{prop}
Let $f(x)$ be a polynomial of odd degree.  Then $f$ has a real root.
\end{prop}

\begin{proof}
Suppose $f$ is a polynomial of odd degree $d$.  We write
\begin{equation*}
f(x) = a_d x^d + a_{d-1} x^{d-1} + \cdots + a_1 x + a_0 ,
\end{equation*}
where $a_d \not= 0$.  We divide by $a_d$ to obtain a polynomial
\begin{equation*}
g(x) := x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
where $b_k = \nicefrac{a_k}{a_d}$.
Let us show that $g(n)$ is
positive for some large $n \in \N$.
We first compare the highest order term with the rest:
\begin{equation*}
\begin{split}
\abs{\frac{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}{n^d}}
& =
\frac{\abs{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}}{n^d}
\\
& \leq
\frac{\abs{b_{d-1}} n^{d-1} + \cdots + \abs{b_1} n + \abs{b_0}}{n^d}
\\
& \leq
\frac{\abs{b_{d-1}} n^{d-1} + \cdots + \abs{b_1} n^{d-1} + \abs{b_0} n^{d-1}}{n^d}
\\
& =
\frac{n^{d-1}\bigl(\abs{b_{d-1}} + \cdots + \abs{b_1} + \abs{b_0}\bigr)}{n^d}
\\
& =
\frac{1}{n}
\bigl(\abs{b_{d-1}} + \cdots + \abs{b_1} + \abs{b_0}\bigr) .
\end{split}
\end{equation*}
Therefore
\begin{equation*}
\lim_{n\to\infty} \frac{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}{n^d}
= 0 .
\end{equation*}
Thus there exists an $M \in \N$ such that 
\begin{equation*}
\abs{\frac{b_{d-1} M^{d-1} + \cdots + b_1 M + b_0}{M^d}} < 1 ,
\end{equation*}
which implies
\begin{equation*}
-(b_{d-1} M^{d-1} + \cdots + b_1 M + b_0) < M^d .
\end{equation*}
Therefore $g(M) > 0$.

Next we look at $g(-n)$ for $n \in \N$.  By a similar argument (exercise)
we find that there exists some $K \in \N$ such that
$b_{d-1} {(-K)}^{d-1} + \cdots + b_1 (-K) + b_0 < K^d$
and therefore $g(-K) < 0$ (why?).
In the
proof make sure you use the fact that $d$ is odd.  In particular, 
if $d$ is odd then ${(-n)}^d = -(n^d)$.

We appeal to the intermediate value theorem to find a
$c \in [-K,M]$, such that $g(c) = 0$.  As $g(x) = \frac{f(x)}{a_d}$,
then $f(c) = 0$, and the proof is done.
\end{proof}

\begin{example}
An interesting fact is that there do exist discontinuous functions that have
the intermediate value property.
The function
\begin{equation*}
f(x) :=
\begin{cases}
\sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$,}
\end{cases}
\end{equation*}
is not continuous at 0, however, it has the intermediate value property.
That is, for any $a < b$, and any $y$ such that $f(a) < y < f(b)$
or $f(a) > y > f(b)$,
there exists a $c$ such that $f(y) = c$.
Proof is left as an exercise.
\end{example}

The intermediate value theorem says that if $f \colon [a,b] \to \R$ is
continuous then $f([a,b])$ contains all the values between $f(a)$ and
$f(b)$.  In fact, more is true.  Combining all the results of this section
we can prove the following useful corollary whose proof is left as an exercise.

\begin{cor} \label{cor:imageofinterval}
If $f \colon [a,b] \to \R$ is continuous, then the direct image $f([a,b])$
is a closed and bounded interval or a single number.
\end{cor}

\subsection{Exercises}

\begin{exercise}
Find an example of a discontinuous function $f \colon [0,1] \to \R$
where the intermediate value theorem fails.
\end{exercise}

\begin{exercise}
Find an example of a \emph{bounded} discontinuous function $f \colon [0,1]
\to \R$ that has neither an absolute minimum nor an absolute maximum.
\end{exercise}

\begin{exercise}
Let $f \colon (0,1) \to \R$ be a continuous function such that
$\displaystyle \lim_{x\to 0} f(x) =
\displaystyle \lim_{x\to 1} f(x) = 0$.  Show that
$f$ achieves either an absolute minimum or an absolute maximum on $(0,1)$
(but perhaps not both).
\end{exercise}

\begin{exercise}
Let
\begin{equation*}
f(x) :=
\begin{cases}
\sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Show that $f$ has the intermediate value property.
That is, for any $a < b$, if there exists a $y$ such that $f(a) < y < f(b)$
or $f(a) > y > f(b)$, then
there exists a $c \in (a,b)$ such that $f(c) = y$.
\end{exercise}

\begin{exercise}
Suppose $g(x)$ is a polynomial of odd degree $d$ such that
\begin{equation*}
g(x) = x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
for some real numbers $b_{0}, b_1, \ldots, b_{d-1}$.  Show that there exists
a $K \in \N$ such that $g(-K) < 0$.  Hint: Make sure to use the fact that
$d$ is odd.  You will have to use that ${(-n)}^d = -(n^d)$.
\end{exercise}

\begin{exercise}
Suppose $g(x)$ is a polynomial of positive even degree $d$ such that
\begin{equation*}
g(x) = x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
for some real numbers $b_{0}, b_1, \ldots, b_{d-1}$.  Suppose 
$g(0) < 0$.  Show that $g$ has at least two distinct real roots.
\end{exercise}

\begin{exercise}
Prove \corref{cor:imageofinterval}:
Suppose $f \colon [a,b] \to \R$ is a continuous function.  Prove
that the direct image $f([a,b])$ is a closed and bounded interval or
a single number.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuous and periodic with period
$P > 0$.  That is, $f(x+P) = f(x)$ for all $x \in \R$.  Show that $f$
achieves an absolute minimum and an absolute maximum.
\end{exercise}

\begin{exercise}[Challenging]
Suppose $f(x)$ is a bounded polynomial,
in other words, there is an $M$ such that $\abs{f(x)} \leq M$
for all $x \in \R$.  Prove that $f$ must be a constant.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to [0,1]$ is continuous.  Show that $f$
has a fixed point, in other words, show that there exists an $x \in [0,1]$ such that
$f(x) = x$.
\end{exercise}

\begin{exercise}
Find an example of a continuous bounded function $f \colon \R \to \R$ that does
not achieve an absolute minimum nor an absolute maximum on $\R$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a continuous function such that
$x \leq f(x) \leq x+1$ for all $x \in \R$.  Find $f(\R)$.
\end{exercise}

\begin{exercise}
True/False, prove or find a counterexample.  If $f \colon \R \to
\R$ is a continuous function such that $f|_{\Z}$ is bounded, then $f$
is bounded.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to (0,1)$ is a bijection.  Prove that $f$ is not
continuous.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuous.
a)~If there is a $c$ such that $f(c)f(-c) < 0$,
then there is a $d$ such that $f(d) = 0$.
b)~Find a continuous function $f$ such that
$f(\R) = \R$, but $f(x)f(-x) \geq 0$ for all $x \in \R$.
\end{exercise}

\begin{exercise}
Suppose $g(x)$ is a polynomial of even degree $d$ such that
\begin{equation*}
g(x) = x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
Show that $g$ achieves an absolute minimum on $\R$.
\end{exercise}

\begin{exercise}
Suppose $f(x)$ is a polynomial of degree $d$ and 
$f(\R) = \R$.  Show that $d$ is odd.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Uniform continuity}
\label{sec:unifcont}

\sectionnotes{1.5--2 lectures (Continuous extension can be
optional)}

\subsection{Uniform continuity}

We made a fuss of saying that the $\delta$ in the definition of
continuity depended on the point $c$.  There are situations when it is
advantageous to have a $\delta$ independent of any point.  Let
us give a name to this concept.

\begin{defn}
Let $S \subset \R$, and let $f \colon S \to \R$ be a function.
Suppose for any $\epsilon > 0$ there exists a $\delta > 0$
such that whenever $x, c \in S$ and
$\abs{x-c} < \delta$, then $\abs{f(x)-f(c)} < \epsilon$.
Then we say $f$ is \emph{\myindex{uniformly continuous}}.
\end{defn}

It is not hard to see that a uniformly continuous function
must be continuous.
The only difference in the definitions
is that in uniform continuity, for a given $\epsilon > 0$ we pick a $\delta > 0$ that
works for all $c \in S$.  That is, $\delta$ can no longer depend on $c$,
it only depends on $\epsilon$.  The domain of definition
of the function makes a difference now.  A function that is not uniformly
continuous on a larger set, may be uniformly continuous when restricted to a
smaller set.

\begin{example}
$f \colon [0,1] \to \R$, defined by $f(x) := x^2$ is uniformly continuous.

Proof: Note that $0 \leq x,c \leq 1$.  Then
\begin{equation*}
\abs{x^2-c^2} = \abs{x+c}\abs{x-c}
\leq (\abs{x}+\abs{c}) \abs{x-c}
\leq (1+1)\abs{x-c} .
\end{equation*}
Therefore given $\epsilon > 0$, let $\delta := \nicefrac{\epsilon}{2}$.
If $\abs{x-c} < \delta$, then $\abs{x^2-c^2} < \epsilon$.

\medskip

On the other hand, $g \colon \R \to \R$, defined by $g(x) := x^2$ is not uniformly
continuous.

Proof: Suppose it is uniformly continuous, then for all $\epsilon > 0$,
there would exist a $\delta > 0$ such that
if $\abs{x-c} < \delta$, then $\abs{x^2 -c^2} < \epsilon$.
Take $x > 0$ and let
$c := x+\nicefrac{\delta}{2}$.  Write
\begin{equation*}
\epsilon >
\abs{x^2-c^2} = \abs{x+c}\abs{x-c}
=
(2x+\nicefrac{\delta}{2})\nicefrac{\delta}{2} 
\geq 
\delta x .
\end{equation*}
Therefore $x < \nicefrac{\epsilon}{\delta}$ for all $x > 0$, which is a
contradiction.
\end{example}


\begin{example}
The function $f \colon (0,1) \to \R$, defined by $f(x) := \nicefrac{1}{x}$ is not
uniformly continuous.

Proof: Given $\epsilon > 0$, then $\epsilon >
\abs{\nicefrac{1}{x}-\nicefrac{1}{y}}$ holds if and only if
\begin{equation*}
\epsilon >
\abs{\nicefrac{1}{x}-\nicefrac{1}{y}}
=
\frac{\abs{y-x}}{\abs{xy}} 
=
\frac{\abs{y-x}}{xy} ,
\end{equation*}
or
\begin{equation*}
\abs{x-y} < xy \epsilon .
\end{equation*}
Therefore, to satisfy the definition of uniform continuity we would have to
have $\delta \leq xy \epsilon$ for all $x,y$ in $(0,1)$, but that would mean
that $\delta \leq 0$.  Therefore there is no single $\delta > 0$.
\end{example}

We have seen that if $f$ is defined on an interval that is either not closed
or not bounded, then $f$ can be continuous, but not uniformly continuous.
For a closed and bounded interval $[a,b]$, we can, however,
make the following statement.

\begin{thm} \label{unifcont:thm}
Let $f \colon [a,b] \to \R$ be a continuous function.  Then $f$
is uniformly continuous.
\end{thm}

\begin{proof}
We prove the statement by contrapositive.
Suppose $f$ is not uniformly continuous.  We will prove
that there is some
$c \in [a,b]$ where $f$ is not continuous.  Let us negate
the definition of uniformly continuous.
There exists an $\epsilon > 0$
such that for every $\delta > 0$, there exist points $x, y$ in $S$ with
$\abs{x-y} < \delta$ and $\abs{f(x)-f(y)} \geq \epsilon$.

So for the $\epsilon > 0$ above,
we find sequences $\{ x_n \}$ and $\{ y_n \}$ such that
$\abs{x_n-y_n} < \nicefrac{1}{n}$ and such that $\abs{f(x_n)-f(y_n)} \geq
\epsilon$.  By
\hyperref[thm:bwseq]{Bolzano--Weierstrass},
there exists a convergent subsequence
$\{ x_{n_k} \}$.  Let $c := \lim\, x_{n_k}$.
As $a \leq x_{n_k} \leq b$, then $a \leq c \leq b$.  Write
\begin{equation*}
\abs{y_{n_k} - c} =
\abs{y_{n_k} - x_{n_k} + x_{n_k} - c} \leq
\abs{y_{n_k} - x_{n_k}}
+
\abs{x_{n_k}-c}
<
\nicefrac{1}{n_k} 
+
\abs{x_{n_k}-c} .
\end{equation*}
As $\nicefrac{1}{n_k}$ and $\abs{x_{n_k}-c}$ both go to zero when
$k$ goes to infinity, $\{ y_{n_k} \}$ converges and the limit
is $c$.  We now show that $f$ is not continuous at $c$.  We
estimate
\begin{equation*}
\begin{split}
\abs{f(x_{n_k}) - f(c)} & =
\abs{f(x_{n_k}) - f(y_{n_k}) + f(y_{n_k}) - f(c)} \\
& \geq
\abs{f(x_{n_k}) - f(y_{n_k})} - \abs{f(y_{n_k}) - f(c)} \\
& \geq
\epsilon - \abs{f(y_{n_k})-f(c)} .
\end{split}
\end{equation*}
Or in other words
\begin{equation*}
\abs{f(x_{n_k})-f(c)} 
+
\abs{f(y_{n_k})-f(c)}  \geq
\epsilon .
\end{equation*}
At least one of the sequences $\{ f(x_{n_k}) \}$  or
$\{ f(y_{n_k}) \}$ cannot converge to $f(c)$, otherwise the left
hand side of the inequality would go to zero while the right-hand side is positive.
Thus $f$ cannot be continuous at $c$.
\end{proof}

\subsection{Continuous extension}

Before we get to continuous extension, we show the following useful lemma.
It says that uniformly continuous functions behave nicely with respect
to Cauchy sequences.  The new issue here is that for a Cauchy sequence
we no longer know where the limit ends up; it may not end up in the domain
of the function.

\begin{lemma} \label{unifcauchycauchy:lemma}
Let $f \colon S \to \R$ be a uniformly continuous function.  Let
$\{ x_n \}$ be a Cauchy sequence in $S$.  Then $\{ f(x_n) \}$ is Cauchy.
\end{lemma}

\begin{proof}
Let $\epsilon > 0$ be given.  There is a $\delta > 0$ such that
$\abs{f(x)-f(y)} < \epsilon$ whenever $\abs{x-y} < \delta$.  Find an $M
\in \N$ such that for all $n, k \geq M$ we have $\abs{x_n-x_k} < \delta$.
Then for all $n, k \geq M$ we have $\abs{f(x_n)-f(x_k)} < \epsilon$.
\end{proof}

An application of the above lemma is the following result.  It says that
a function on an open interval is uniformly continuous if and only if
it can be extended to a continuous function on the closed interval.

\begin{prop} \label{context:prop}
A function $f \colon (a,b) \to \R$ is uniformly continuous if and only if
the limits 
\begin{equation*}
L_a := \lim_{x \to a} f(x) \qquad \text{and} \qquad
L_b := \lim_{x \to b} f(x)
\end{equation*}
exist and the function $\widetilde{f} \colon [a,b] \to \R$
defined by
\begin{equation*}
\widetilde{f}(x) :=
\begin{cases}
f(x) & \text{ if $x \in (a,b)$,} \\
L_a & \text{ if $x = a$,} \\
L_b & \text{ if $x = b$,}
\end{cases}
\end{equation*}
is continuous.
\end{prop}

\begin{proof}
One direction is not difficult.  If $\widetilde{f}$ is continuous, then
it is uniformly continuous by \thmref{unifcont:thm}.  As $f$ is the
restriction of $\widetilde{f}$ to $(a,b)$, then $f$ is also uniformly continuous
(easy exercise).

Now suppose $f$ is uniformly continuous.  We must first show
that the limits $L_a$ and $L_b$ exist.  Let us concentrate on $L_a$.
Take a sequence $\{ x_n \}$ in $(a,b)$ such that $\lim\, x_n = a$.
The sequence $\{ x_n \}$ is Cauchy, so by
\lemmaref{unifcauchycauchy:lemma}
the sequence $\{ f(x_n) \}$ is Cauchy and thus convergent.
We have some number $L_1 := \lim\, f(x_n)$.  Take another sequence
$\{ y_n \}$ in $(a,b)$ such that $\lim\, y_n = a$.  By the same reasoning
we get $L_2 := \lim\, f(y_n)$.  If we show that $L_1 = L_2$, then
the limit $L_a = \lim_{x\to a} f(x)$ exists.  Let $\epsilon > 0$ be given.
Find $\delta > 0$ such that $\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} <
\nicefrac{\epsilon}{3}$.  Find $M \in \N$ such that for
$n \geq M$ we have $\abs{a-x_n} < \nicefrac{\delta}{2}$,
$\abs{a-y_n} < \nicefrac{\delta}{2}$,
$\abs{f(x_n)-L_1} < \nicefrac{\epsilon}{3}$, and
$\abs{f(y_n)-L_2} < \nicefrac{\epsilon}{3}$.  Then for $n \geq M$ we have
\begin{equation*}
\abs{x_n-y_n} = 
\abs{x_n-a+a-y_n} \leq
\abs{x_n-a}+\abs{a-y_n} < \nicefrac{\delta}{2} + \nicefrac{\delta}{2} =
\delta.
\end{equation*}
So
\begin{equation*}
\begin{split}
\abs{L_1-L_2} &=
\abs{L_1-f(x_n)+f(x_n)-f(y_n)+f(y_n)-L_2} \\
& \leq 
\abs{L_1-f(x_n)}+\abs{f(x_n)-f(y_n)}+\abs{f(y_n)-L_2} \\
& \leq
\nicefrac{\epsilon}{3} + \nicefrac{\epsilon}{3} + \nicefrac{\epsilon}{3}
=
\epsilon .
\end{split}
\end{equation*}
Therefore $L_1 = L_2$.
Thus $L_a$ exists.  To show that $L_b$ exists is left as an exercise.

Now that we know that the
limits $L_a$ and $L_b$ exist, we are done.  If $\lim_{x\to a} f(x)$
exists, then $\lim_{x\to a} \widetilde{f}(x)$ exists
(See \propref{prop:limrest}).  Similarly with $L_b$.
Hence $\widetilde{f}$ is continuous at $a$ and $b$.  
And since $f$ is continuous at $c \in (a,b)$, then
$\widetilde{f}$ is continuous at $c \in (a,b)$.
\end{proof}

A common application of this proposition (together with
\propref{prop:onesidedlimits})
is the following.
Suppose $f \colon (-1,0) \cup (0,1) \to \R$ is uniformly continuous,
then $\lim_{x\to 0} f(x)$ exists and the function
has what is called an \emph{\myindex{removable singularity}}, that is,
we can extend the function to a continuous function on $(-1,1)$.


\subsection{Lipschitz continuous functions}

\begin{defn}
A function $f \colon S \to \R$
is \emph{\myindex{Lipschitz continuous}}%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Rudolf_Lipschitz}{Rudolf Otto Sigismund Lipschitz}
(1832--1903).}, if there exists a $K \in \R$, such that
%be a function such that there exists a number $K$
%such that for all $x$ and $y$ in $S$ we have
\begin{equation*}
\abs{f(x)-f(y)} \leq K \abs{x-y} 
\qquad \text{for all $x$ and $y$ in $S$.}
\end{equation*}
\end{defn}

A large class of functions is Lipschitz continuous.  Be careful, just as
for uniformly continuous functions, the
domain of definition of the function is important.  See the examples below
and the exercises.  First we justify the use of the word \emph{continuous}.

\begin{prop}
A Lipschitz continuous function is uniformly continuous.
\end{prop}

\begin{proof}
Let $f \colon S \to \R$ be a function and let $K$ be a constant such that
for all $x, y$ in $S$ we have
$\abs{f(x)-f(y)} \leq K \abs{x-y}$.

Let $\epsilon > 0$ be given.  Take $\delta :=
\nicefrac{\epsilon}{K}$.
For any $x$ and $y$ in $S$ such that
$\abs{x-y} < \delta$
we have that
\begin{equation*}
\abs{f(x)-f(y)} \leq K \abs{x-y} < K \delta = K \frac{\epsilon}{K} =
\epsilon .
\end{equation*}
Therefore $f$ is uniformly continuous.
\end{proof}

We interpret Lipschitz continuity geometrically.  If $f$ is a Lipschitz
continuous function with some constant $K$.  We rewrite the inequality 
to say that for $x \not=y$ we have
\begin{equation*}
\abs{\frac{f(x)-f(y)}{x-y}} \leq K .
\end{equation*}
The quantity $\frac{f(x)-f(y)}{x-y}$ is the slope of the line
between the points $\bigl(x,f(x)\bigr)$
and $\bigl(y,f(y)\bigr)$, that is, a \emph{\myindex{secant line}}.  Therefore, $f$ is Lipschitz
continuous if and only if every line that intersects the graph of $f$ in at least two
distinct
points has slope less than or equal to $K$.  See \figureref{fig:lipschitz}.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{lipschitzfig.eepic}
\caption{The slope of a secant line.
A function is Lipschitz if $\abs{\text{slope}} =
\abs{\frac{f(x)-f(y)}{x-y}} \leq K$ for all $x$ and $y$.\label{fig:lipschitz}}
%\end{center}
\end{myfigureht}

\begin{example}
The functions $\sin(x)$ and $\cos(x)$ are Lipschitz continuous.
We have seen (\exampleref{sincos:example}) the following two inequalities.
\begin{equation*}
\abs{\sin(x)-\sin(y)} 
\leq \abs{x-y}
\qquad \text{and} \qquad
\abs{\cos(x)-\cos(y)}
\leq \abs{x-y} .
\end{equation*}

Hence sin and cos are Lipschitz continuous with $K=1$.
\end{example}

\begin{example}
The function $f \colon [1,\infty) \to \R$ defined by $f(x) := \sqrt{x}$
is Lipschitz continuous. Proof:
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} = 
\abs{\frac{x-y}{\sqrt{x}+\sqrt{y}}}
=
\frac{\abs{x-y}}{\sqrt{x}+\sqrt{y}} .
\end{equation*}
As $x \geq 1$ and $y \geq 1$, we see that $\frac{1}{\sqrt{x}+\sqrt{y}}
\leq \frac{1}{2}$.  Therefore
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} = 
\abs{\frac{x-y}{\sqrt{x}+\sqrt{y}}}
\leq
\frac{1}{2}
\abs{x-y}.
\end{equation*}

On the other hand $f \colon [0,\infty) \to \R$ defined by
$f(x) := \sqrt{x}$ is not Lipschitz continuous.  Let us see why:
Suppose we have
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} 
\leq
K \abs{x-y} ,
\end{equation*}
for some $K$.  Let $y=0$ to obtain
$\sqrt{x} \leq K x$.   If $K > 0$, then for $x > 0$ we then get
$\nicefrac{1}{K} \leq \sqrt{x}$.  This cannot possibly be true for all
$x > 0$.  Thus no such $K > 0$ exists and $f$ is not
Lipschitz continuous.

The last example is a function that is uniformly
continuous but not Lipschitz continuous.  To see that $\sqrt{x}$
is
uniformly continuous on $[0,\infty)$ note that it is uniformly continuous on
$[0,1]$ by \thmref{unifcont:thm}.  It is also Lipschitz (and
therefore uniformly continuous) on $[1,\infty)$.  It is not hard (exercise)
to show that this means that $\sqrt{x}$ is uniformly continuous on
$[0,\infty)$.
\end{example}

\subsection{Exercises}

\begin{exercise}
Let $f \colon S \to \R$ be uniformly continuous.  Let $A \subset S$.
Then the restriction $f|_A$ is uniformly continuous.
\end{exercise}

\begin{exercise}
Let $f \colon (a,b) \to \R$ be a uniformly continuous function.
Finish the proof of \propref{context:prop} by showing that
the limit
$\lim\limits_{x \to b} f(x)$
exists.
\end{exercise}

\begin{exercise}
Show that $f \colon (c,\infty) \to \R$ for some $c > 0$
and defined by $f(x) := \nicefrac{1}{x}$ is Lipschitz continuous.
\end{exercise}

\begin{exercise}
Show that $f \colon (0,\infty) \to \R$
defined by $f(x) := \nicefrac{1}{x}$ is not Lipschitz continuous.
\end{exercise}

\begin{exercise}
Let $A, B$ be intervals.
Let $f \colon A \to \R$ and $g \colon B \to \R$ be uniformly continuous
functions such that $f(x) = g(x)$ for $x \in A \cap B$.  Define
the function $h \colon A \cup B \to \R$ by $h(x) := f(x)$ if
$x \in A$ and $h(x) := g(x)$ if $x \in B \setminus A$.
a) Prove that if $A \cap B \not= \emptyset$, then $h$ is uniformly continuous.
b) Find an example where $A \cap B = \emptyset$ and $h$ is not even
continuous.
\end{exercise}

\begin{exercise}[Challenging]
Let $f \colon \R \to \R$ be a polynomial of degree 
$d \geq 2$.  Show that $f$ is not Lipschitz
continuous.
\end{exercise}

\begin{exercise}
Let $f \colon (0,1) \to \R$ be a bounded continuous function.  Show that
the function
$g(x) := x(1-x)f(x)$ is uniformly continuous.
\end{exercise}

\begin{exercise}
Show that $f \colon (0,\infty) \to \R$ defined by $f(x) := \sin
(\nicefrac{1}{x})$ is not uniformly continuous.
\end{exercise}

\begin{exercise}[Challenging]
Let $f \colon \Q \to \R$ be a uniformly continuous function.  Show that
there exists a uniformly continuous function $\widetilde{f} \colon \R \to \R$
such that $f(x) = \widetilde{f}(x)$ for all $x \in \Q$.
\end{exercise}

\begin{exercise}
a) Find a continuous $f \colon (0,1) \to \R$ and a sequence $\{ x_n \}$ in
$(0,1)$ that is Cauchy, but such that $\{ f(x_n) \}$ is not Cauchy.
b) Prove that if $f \colon \R \to \R$ is continuous, and $\{ x_n \}$ is
Cauchy, then $\{ f(x_n) \}$ is Cauchy.
\end{exercise}

\begin{exercise}
a) If $f \colon S \to \R$ and $g \colon S \to \R$ are uniformly continuous,
then show that $h \colon S \to \R$ given by $h(x) := f(x) + g(x)$
is uniformly continuous.\\
b) If $f \colon S \to \R$ is uniformly continuous and $a \in \R$,
then show that $h \colon S \to \R$ given by $h(x) := a f(x)$
is uniformly continuous.
\end{exercise}

\begin{exercise}
a) If $f \colon S \to \R$ and $g \colon S \to \R$ are Lipschitz,
then show that $h \colon S \to \R$ given by $h(x) := f(x) + g(x)$
is Lipschitz.\\
b) If $f \colon S \to \R$ is Lipschitz and $a \in \R$,
then show that $h \colon S \to \R$ given by $h(x) := a f(x)$
is Lipschitz.
\end{exercise}

\begin{exercise}
a) If $f \colon [0,1] \to \R$ is given by $f(x) := x^m$ for an integer
$m \geq 0$,
show $f$ is Lipschitz and find the best (the smallest) Lipschitz constant
$K$ (depending on $m$ of course).
Hint: $(x-y)(x^{m-1} + x^{m-2}y + x^{m-3}y^2 + \cdots + x y^{m-2} + y^{m-1}) = x^m - y^m$.
\\
b) Using the previous exercise, show that if $f \colon [0,1] \to \R$
is a polynomial, that is, $f(x) := a_m x^m + a_{m-1} x^{m-1} + \cdots + a_0$,
then $f$ is Lipschitz.
\end{exercise}

\begin{exercise}
Suppose for $f \colon [0,1] \to \R$ we have $\abs{f(x)-f(y)} \leq K
\abs{x-y}$ for all $x,y$ in $[0,1]$,
and $f(0) = f(1) = 0$.
Prove that $\abs{f(x)} \leq \nicefrac{K}{2}$ for all $x \in [0,1]$.  Further show by example that
$\nicefrac{K}{2}$ is the best possible, that is, there exists such a continuous function
for which $\abs{f(x)} = \nicefrac{K}{2}$ for some $x \in [0,1]$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuous and periodic with period
$P > 0$.  That is, $f(x+P) = f(x)$ for all $x \in \R$.  Show that $f$
is uniformly continuous.
\end{exercise}

\begin{exercise}
Suppose $f \colon S \to \R$ and $g \colon [0,\infty) \to [0,\infty)$
are functions, $g$ is continuous at $0$, $g(0) = 0$, and
whenever $x$ and $y$ are in $S$ we have $\abs{f(x)-f(y)} \leq g(\abs{x-y})$.
Prove that $f$ is uniformly continuous.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is a function such that for every $c \in
[a,b]$ there is a $K_c > 0$ and an $\epsilon_c > 0$ for which
$\abs{f(x)-f(y)} \leq K_c \abs{x-y}$ for all $x$ and $y$ in
$(c-\epsilon,c+\epsilon) \cap [a,b]$.  In other words, $f$ is ``locally Lipschitz.''
\\
a)~Prove that there exists a single $K > 0$ such that
$\abs{f(x)-f(y)} \leq K \abs{x-y}$ for all $x,y$ in $[a,b]$.
\\
b)~Find a counterexample to the above if the interval is open, that is,
find an $f \colon (a,b) \to \R$ that is locally Lipschitz, but not
Lipschitz.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Limits at infinity}
\label{sec:limitatinf}

\sectionnotes{less than 1 lecture (optional, can safely be omitted unless
\sectionref{sec:monotonefunc} or
\sectionref{sec:impropriemann} is also covered)}

\subsection{Limits at infinity}

As for sequences, a continuous variable can also approach infinity.  Let
us make this notion precise.

\begin{defn}
We say $\infty$ is a cluster point of $S \subset \R$, if for every
$M \in \R$, there exists an $x \in S$ such that $x \geq M$.  Similarly
$- \infty$ is a cluster point of $S \subset \R$, if for every
$M \in \R$, there exists an $x \in S$ such that $x \leq M$.

\index{limit of a function at infinity}%
Let $f \colon S \to \R$ be a function, where 
$\infty$ is a cluster point of $S$.
If there exists an $L \in \R$
such that for every $\epsilon > 0$, there is an $M \in \R$ such that
\begin{equation*}
\abs{f(x) - L} < \epsilon 
\end{equation*}
whenever $x \geq M$, then we say $f(x)$ \emph{\myindex{converges}} to $L$
as $x$ goes to $\infty$.  We call $L$ the \emph{\myindex{limit}} and write
\begin{equation*}
\lim_{x \to \infty} f(x) := L .
\end{equation*}
Alternatively we write $f(x) \to L$ as $x \to \infty$.

Similarly, if $-\infty$ is a cluster point of $S$
and
there exists an $L \in \R$
such that for every $\epsilon > 0$, there is an $M \in \R$ such that
\begin{equation*}
\abs{f(x) - L} < \epsilon 
\end{equation*}
whenever $x \leq M$, then we say $f(x)$ \emph{converges} to $L$
as $x$ goes to $-\infty$.  We call $L$ the \emph{limit} and write
\begin{equation*}
\lim_{x \to -\infty} f(x) := L .
\end{equation*}
Alternatively we write $f(x) \to L$ as $x \to -\infty$.
\end{defn}

We cheated a little bit again and said \emph{the} limit.
We leave it as an exercise for the reader to prove the following proposition.

\begin{prop} \label{liminfty:unique}
The limit at $\infty$ or $-\infty$ as defined above is unique if it exists.
\end{prop}

\begin{example}
Let $f(x) := \frac{1}{\abs{x}+1}$.  Then
\begin{equation*}
\lim_{x\to \infty} f(x) = 0 \qquad \text{and} \qquad
\lim_{x\to -\infty} f(x) = 0 .
\end{equation*}

Proof:
Let $\epsilon > 0$ be given.  Find $M > 0$ large enough
so that $\frac{1}{M+1} < \epsilon$.  If
$x \geq M$, then $\frac{1}{x+1} \leq \frac{1}{M+1} < \epsilon$.
Since $\frac{1}{\abs{x}+1} > 0$ for all $x$ the first limit is proved.
The proof for $-\infty$ is left to the reader.
\end{example}

\begin{example}
Let $f(x) := \sin(\pi x)$.  Then $\lim_{x\to\infty} f(x)$ does not exist.
To prove this fact note that if $x = 2n+\nicefrac{1}{2}$ for some $n \in \N$ then $f(x)=1$,
while if $x = 2n+\nicefrac{3}{2}$ then $f(x)=-1$, so they cannot both be
within a small $\epsilon$ of a single real number.

We must be careful not to confuse continuous limits with limits of sequences.
%For $f(x) = \sin(\pi x)$
We could say
\begin{equation*}
\lim_{n \to \infty} \sin(\pi n) = 0, \qquad \text{but} \qquad
\lim_{x \to \infty} \sin(\pi x) ~ \text{does not exist}.
\end{equation*}
Of course the notation is ambiguous: are we thinking of the
sequence $\{ \sin (\pi n) \}_{n=1}^\infty$ or the function $\sin(\pi x)$
of a real variable.  We are simply using the convention
that $n \in \N$, while $x \in \R$.  When the notation is not clear,
it is good to explicitly mention where the variable lives, or what kind
of limit are you using.  If there is possibility of confusion, one can
write, for example,
\begin{equation*}
\lim_{\substack{n \to \infty\\n \in \N}} \sin(\pi n) .
\end{equation*}
\end{example}

There is a connection of continuous limits to limits of sequences, but we must take all
sequences going to infinity, just as before in \lemmaref{seqflimit:lemma}.

\begin{lemma} \label{seqflimitinf:lemma}
Suppose $f \colon S \to \R$ is a function, $\infty$ is a cluster
point of $S \subset \R$, and $L \in \R$.  Then
\begin{equation*}
\lim_{x\to\infty} f(x) = L
% \qquad \text{if and only if} \qquad
\end{equation*}
if and only if
\begin{equation*}
\lim_{n\to\infty} f(x_n) = L% ~~\text{for all sequences $\{ x_n \}$ such that $\lim\, x_n = \infty$} .
\end{equation*}
for all sequences $\{ x_n \}$ such that $\lim\limits_{n\to\infty} x_n = \infty$.
\end{lemma}

The lemma holds for the limit as $x \to -\infty$.
Its proof is almost identical and
is left as an exercise.

\begin{proof}
First suppose $f(x) \to L$ as $x \to \infty$.
Given an $\epsilon > 0$, there exists an $M$ such that for all $x \geq M$
we have $\abs{f(x)-L} < \epsilon$.
Let $\{ x_n \}$
be a sequence in $S$ such that $\lim \, x_n = \infty$.  Then there exists an
$N$ such that for all $n \geq N$ we have $x_n \geq M$.  And thus
$\abs{f(x_n)-L} < \epsilon$.

We prove the converse by contrapositive.  Suppose $f(x)$ does
not go to $L$ as $x \to \infty$.
This means that there exists an $\epsilon > 0$,
such that for every $M \in \N$, there exists an $x \in S$, $x \geq M$, let
us call it $x_M$, such that $\abs{f(x_M)-L} \geq \epsilon$.
Consider the sequence $\{ x_n \}$.  Clearly 
$\{ f(x_n) \}$ does not converge to $L$.  It remains to note
that $\lim\, x_n = \infty$, because $x_n \geq n$ for all $n$.
\end{proof}

Using the lemma, we again translate results about sequential
limits into results about continuous limits as $x$ goes to infinity.  That
is, we have almost immediate analogues of the corollaries
in \sectionref{subseq:sequentiallimits}.  We simply allow 
the cluster point $c$ to be either $\infty$ or $-\infty$, in addition
to a real number.  We leave it to
the student to verify these statements.

\subsection{Infinite limit}

Just as for sequences, it is often convenient to distinguish certain
divergent sequences, and talk about limits being infinite
almost as if the limits existed.

\begin{defn}
\index{infinite limit of a function}%
Let $f \colon S \to \R$ be a function and suppose 
$S$ has $\infty$ as a cluster point.
We say $f(x)$
\emph{\myindex{diverges to infinity}} 
as $x$ goes to $\infty$,
if for every $N \in \R$
there exists an $M \in \R$ such that
\begin{equation*}
f(x) > N
\end{equation*}
whenever $x \in S$ and $x \geq M$.
We write
\begin{equation*}
\lim_{x \to \infty} f(x) := \infty ,
\end{equation*}
or we say that $f(x) \to \infty$ as $x \to \infty$.
\end{defn}

A similar definition can be made for limits as $x \to -\infty$
or as $x \to c$ for a finite $c$.  Also similar definitions can be
made for limits being $-\infty$.  Stating these definitions is left
as an exercise.
Note that
sometimes \emph{\myindex{converges to infinity}} is used.
We can again use sequential limits, and an analogue of 
\lemmaref{seqflimit:lemma} is left as an exercise.

\begin{example}
Let us show that $\lim_{x \to \infty} \frac{1+x^2}{1+x} = \infty$.

Proof: For $x \geq 1$ we have
\begin{equation*}
\frac{1+x^2}{1+x} \geq 
\frac{x^2}{x+x}  = 
\frac{x}{2} .
\end{equation*}
Given $N \in \R$, take $M = \max \{ 2N+1 , 1 \}$.
If $x \geq M$, then $x \geq 1$ and $\nicefrac{x}{2} > N$.
So
\begin{equation*}
\frac{1+x^2}{1+x} \geq 
\frac{x}{2} > N .
\end{equation*}
\end{example}

\subsection{Compositions}

Finally, just as for limits at finite numbers we can compose functions
easily.

\begin{prop} \label{prop:inflimcompositions}
Suppose $f \colon A \to B$, $g \colon B \to \R$, $A, B \subset \R$, 
$a \in \R \cup \{ -\infty, \infty\}$ is a cluster point of $A$,
and $b \in \R \cup \{ -\infty, \infty\}$ is a cluster point of $B$.
Suppose 
\begin{equation*}
\lim_{x \to a} f(x) = b\qquad \text{and} \qquad \lim_{y \to b} g(y) = c
\end{equation*}
for some $c \in \R \cup \{ -\infty, \infty \}$.
If $b \in B$, then suppose $g(b) = c$.
Then
\begin{equation*}
\lim_{x \to a} g\bigl(f(x)\bigr) = c .
\end{equation*}
\end{prop}

The proof is straightforward, and left as an exercise.  We already
know the proposition when $a, b, c \in \R$, see Exercises
\ref{exercise:contlimitcomposition} and
\ref{exercise:contlimitbadcomposition}.  Again the requirement that $g$ is
continuous at $b$, if $b \in B$, is necessary.

\begin{example}
Let $h(x) := e^{-x^2+x}$.  Then
\begin{equation*}
\lim_{x\to \infty} h(x) = 0 .
\end{equation*}

Proof:
The claim follows once we know
\begin{equation*}
\lim_{x\to \infty} -x^2+x = -\infty
\end{equation*}
and
\begin{equation*}
\lim_{y\to -\infty} e^y = 0 ,
\end{equation*}
which is usually proved when the exponential function is defined.
\end{example}

\subsection{Exercises}

\begin{exercise}
Prove \propref{liminfty:unique}.
\end{exercise}

\begin{exercise}
Let $f \colon [1,\infty) \to \R$ be a function.  Define
$g \colon (0,1] \to \R$ via $g(x) := f(\nicefrac{1}{x})$.
Using the definitions of limits directly,
show that $\lim_{x\to 0^+} g(x)$
exists if and only if $\lim_{x\to \infty} f(x)$ exists, in which
case they are equal.
\end{exercise}

\begin{exercise}
Prove \propref{prop:inflimcompositions}.
\end{exercise}

\begin{exercise}
Let us justify terminology.
Let $f \colon \R \to \R$ be a function such that
$\lim_{x \to \infty} f(x) = \infty$ (diverges to infinity).
Show that $f(x)$ diverges (i.e.\ does not converge) as $x \to \infty$.
\end{exercise}

\begin{exercise}
Come up with the definitions for limits of $f(x)$ going to $-\infty$ as $x \to
\infty$, $x \to -\infty$, and as $x \to c$ for a finite $c \in \R$.
Then state the definitions for limits of $f(x)$ going to $\infty$ 
as $x \to -\infty$, and as $x \to c$ for a finite $c \in \R$.
\end{exercise}

\begin{exercise}
Suppose $P(x) := x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0$ is a \emph{\myindex{monic polynomial}}
of degree $n \geq 1$ (monic means that the coefficient of $x^n$ is 1). a)
Show that if $n$ is even then $\lim_{x\to\infty} P(x) = 
\lim_{x\to-\infty} P(x) = \infty$.  b)
Show that if $n$ is odd then
$\lim_{x\to\infty} P(x) = \infty$ and
$\lim_{x\to-\infty} P(x) = -\infty$ (see previous exercise).
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence.  Consider $S := \N \subset \R$, and
$f \colon S \to \R$ defined by $f(n) := x_n$.  Show that
the two notions of limit,
\begin{equation*}
\lim_{n\to\infty} x_n \qquad \text{and} \qquad
\lim_{x\to\infty} f(x) 
\end{equation*}
are equivalent.  That is, show that if one exists so does
the other one, and in this case they are equal.
\end{exercise}

\begin{exercise}
Extend \lemmaref{seqflimitinf:lemma} as follows.
Suppose $S \subset \R$ has a cluster point $c \in \R$, $c = \infty$,
or $c = -\infty$.  Let $f \colon S \to \R$ be a function and let
$L = \infty$ or $L = -\infty$.  Show that
\begin{equation*}
\lim_{x\to c} f(x) = L \qquad \text{if and only if} \qquad
\lim_{n\to\infty} f(x_n) = L ~~\text{for all sequences $\{ x_n \}$ such that $\lim\, x_n =
c$} .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a 2-periodic function, that is $f(x +2) =
f(x)$ for all $x$.  Define $g \colon \R \to \R$ by 
\begin{equation*}
g(x) := f\left(\frac{\sqrt{x^2+1}-1}{x}\right)
\end{equation*}
a)~Find the function $\varphi \colon (-1,1) \to \R$ such that
$g\bigl(\varphi(t)\bigr) = f(t)$, that is $\varphi^{-1}(x) = 
\frac{\sqrt{x^2+1}-1}{x}$.
\\
b)~Show that $f$ is continuous if and only if $g$ is continuous and
\begin{equation*}
\lim_{x \to \infty} g(x) = 
\lim_{x \to -\infty} g(x) = 
f(1) = f(-1) .
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Monotone functions and continuity}
\label{sec:monotonefunc}

\sectionnotes{1 lecture (optional, can safely be omitted unless
\sectionref{sec:ift} is also covered, requires \sectionref{sec:limitatinf})}

\begin{defn}
Let $S \subset \R$.
We say $f \colon S \to \R$ is \emph{\myindex{increasing}}
(resp.\  \emph{\myindex{strictly increasing}}) if $x,y \in S$ with
$x < y$ implies $f(x) \leq f(y)$ (resp.\ $f(x) < f(y)$).
We define
\emph{\myindex{decreasing}} and
\emph{\myindex{strictly decreasing}} in the same way by switching the
inequalities for $f$.

If a function is either increasing or decreasing we say it is
\emph{monotone}\index{monotone function}.  If it is
strictly increasing or strictly decreasing we say it is
\emph{strictly monotone}\index{strictly monotone function}.
\end{defn}

Sometimes \emph{\myindex{nondecreasing}}
(resp.\ \emph{\myindex{nonincreasing}}) is used
for increasing (resp.\ decreasing) function to emphasize it is not
strictly increasing (resp.\ strictly decreasing).

If $f$ is increasing, then $-f$ is decreasing and vice-versa.  Therefore,
many results about monotone functions can just be proved for say increasing
functions, and the results will follow easily for decreasing functions.

\subsection{Continuity of monotone functions}

It is easy to compute one-sided limits for monotone functions.

\begin{prop} \label{prop:monotlimits}
Let $S \subset \R$, $c \in \R$,
$f \colon S \to \R$ be increasing,
and
$g \colon S \to \R$ be decreasing.
If $c$ is a cluster point of $S \cap (-\infty,c)$, then
\begin{equation*}
\lim_{x \to c^-} f(x) = \sup \{ f(x) : x < c, x \in S \}
\qquad \text{and} \qquad
\lim_{x \to c^-} g(x) = \inf \{ g(x) : x < c, x \in S \} .
\end{equation*}
If $c$ is a cluster point of $S \cap (c,\infty)$, then
\begin{equation*}
\lim_{x \to c^+} f(x) = \inf \{ f(x) : x > c, x \in S \}
\qquad \text{and} \qquad
\lim_{x \to c^+} g(x) = \sup \{ g(x) : x > c, x \in S \} .
\end{equation*}
If $\infty$ is a cluster point of $S$, then
\begin{equation*}
\lim_{x \to \infty} f(x) = \sup \{ f(x) : x \in S \}
\qquad \text{and} \qquad
\lim_{x \to \infty} g(x) = \inf \{ g(x) : x \in S \} .
\end{equation*}
If $-\infty$ is a cluster point of $S$, then
\begin{equation*}
\lim_{x \to -\infty} f(x) = \inf \{ f(x) : x \in S \}
\qquad \text{and} \qquad
\lim_{x \to -\infty} g(x) = \sup \{ g(x) : x \in S \} .
\end{equation*}
\end{prop}

In particular all the one-sided limits exist whenever they make
sense.  For monotone functions therefore, when we say
the left hand limit $x \to c^-$
exists, we mean that $c$ is a cluster point of $S \cap (-\infty,c)$,
and same for the right hand limit.

\begin{proof}
Let us assume $f$ is increasing, and we will show the first
equality.  The rest of the proof is very similar and is left as an
exercise.

Let $a := \sup \{ f(x) : x < c, x \in S \}$.  If $a = \infty$,
then given an $M \in \R$, there exists an $x_M \in S$, $x_M < c$, such that $f(x_M) > M$. 
As $f$ is increasing, $f(x) \geq f(x_M) >  M$ for all $x \in S$ with $x > x_M$.  If
we take $\delta := c-x_M > 0$, then we obtain the definition of the limit going to
infinity.

Next suppose $a < \infty$.
Let $\epsilon > 0$ be given.  Because $a$ is the supremum and
$S \cap (-\infty,c)$ is nonempty, $a \in \R$ and
there exists an
$x_\epsilon \in S$,
$x_\epsilon < c$,
such that $f(x_\epsilon) > a-\epsilon$.  As $f$ is increasing,
if $x \in S$ and $x_\epsilon < x < c$, we have
$a-\epsilon < f(x_\epsilon) \leq f(x) \leq a$.  Let
$\delta := c-x_\epsilon$.  Then for $x \in S \cap (-\infty,c)$
with $\abs{x-c} < \delta$,
we have $\abs{f(x)-a} < \epsilon$.
\end{proof}

Suppose $f \colon S \to \R$, $c \in S$, and
that both one-sided limits exist.
Since $f(x) \leq f(c) \leq f(y)$
whenever $x < c < y$, taking the limits we obtain
\begin{equation*}
\lim_{x \to c^-} f(x) \leq f(c) \leq \lim_{x \to c^+} f(x) .
\end{equation*}
Then $f$ is continuous at $c$ if and only if both limits are equal
to each other (and hence equal to $f(c)$).  See also
\propref{prop:onesidedlimits}.
See \figureref{fig:figinccont} to get an idea of a what a discontinuity
looks like.

%The proposition has an easy corollary, the proof of which we
%leave to the reader.
%
%\begin{cor} \label{cor:contofmonotone}
%Let $S \subset \R$ and $f \colon S \to \R$ be increasing.
%If $c \in S$ is a cluster point of both 
%$S \cap (-\infty,c)$ and
%$S \cap (c,\infty)$, then
%\begin{equation*}
%\lim_{x \to c^-} f(x) \leq \lim_{x \to c^+} f(x) .
%\end{equation*}
%The function $f$ is continuous at $c$ if and only if equality is achieved above.
%
%If $c \in S$ is a cluster point of
%$S \cap (-\infty,c)$ but not of $S \cap (c,\infty)$ then
%\begin{equation*}
%\lim_{x \to c^-} f(x) \leq f(c) .
%\end{equation*}
%
%If $c \in S$ is a cluster point of
%$S \cap (c,\infty)$ but not of $S \cap (-\infty,c)$ then
%\begin{equation*}
%f(c) \leq \lim_{x \to c^+} f(x) .
%\end{equation*}
%
%The function $f$ is continuous at $c$ if and only if equality is achieved in
%the inequalities above or if $c$ is not a cluster point of $S$.
%
%If $f$ is decreasing instead, the same result holds with
%the inequality reversed.
%\end{cor}

\begin{cor} \label{cor:continterval}
If $I \subset \R$ is an interval and $f \colon I \to \R$ is 
monotone and not constant, then $f(I)$ is an interval if and only if $f$
is continuous.
\end{cor}

Assuming $f$ is not constant is to avoid the technicality
that $f(I)$ is a single point in that case; $f(I)$ is a single
point if and only if $f$ is constant.  A constant function is 
continuous.

\begin{proof}
Without loss of generality suppose $f$ is increasing.

First suppose $f$ is continuous.  Take two points
$f(x_1) < f(x_2)$ in $f(I)$.
As $f$ is increasing then $x_1 < x_2$.  By the
\hyperref[IVT:thm]{intermediate value theorem},
given any $y$ with $f(x_1) < y < f(x_2)$, we find
a $c \in (x_1,x_2) \subset I$ such that $f(c) = y$, so $y \in f(I)$. 
Hence, $f(I)$ is an interval.

%See also \corref{cor:imageofinterval}.

Let us prove the reverse direction by contrapositive.
Suppose $f$ is not continuous at $c \in I$,
and that $c$ is not an endpoint of $I$.
Let
\begin{equation*}
a := \lim_{x \to c^-} f(x) = \sup \{ f(x) : x \in I, x < c \} ,
\qquad
b := \lim_{x \to c^+} f(x) = \inf \{ f(x) : x \in I, x > c \} .
\end{equation*}
As $c$ is a discontinuity, $a < b$.
If $x < c$, then $f(x) \leq a$, and
if $x > c$, then $f(x) \geq b$.  Therefore
no point
in $(a,b) \setminus \{ f(c) \}$ is in $f(I)$.
However there exists $x_1 \in S$, $x_1 < c$, so
$f(x_1) \leq a$, and there exists $x_2 \in S$, $x_2 > c$,
so $f(x_2) \geq b$.  Both $f(x_1)$ and $f(x_2)$ are in $f(I)$,
but there are points in between them that are not in $f(I)$.
So $f(I)$ is not an interval.  See \figureref{fig:figinccont}.

When $c \in I$ is an endpoint, the proof is similar and is left as an exercise.
\end{proof}

\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figinccont.pdf_t}
\caption{Increasing function $f \colon I \to \R$ discontinuity at
$c$.\label{fig:figinccont}}
%\end{center}
\end{myfigureht}

A striking property of monotone functions is that they cannot have
too many discontinuities.

\begin{cor} \label{cor:monotcountcont}
Let $I \subset \R$ be an interval and
$f \colon I \to \R$ be monotone.  Then $f$ has at most
countably many discontinuities.
\end{cor}

\begin{proof}
Let $E \subset I$ be the set of all discontinuities
that are not endpoints of $I$.  As there are
only two endpoints, it is enough to show that $E$ is countable.
Without loss of generality, suppose $f$ is increasing.
We will define an injection $h \colon E \to \Q$.
For each $c \in E$
the one-sided limits of $f$ both exist as $c$ is not an endpoint.
Let
\begin{equation*}
a := \lim_{x \to c^-} f(x) = \sup \{ f(x) : x \in I, x < c \} ,
\qquad
b := \lim_{x \to c^+} f(x) = \inf \{ f(x) : x \in I, x > c \} .
\end{equation*}
As $c$ is a discontinuity, we have $a < b$.  
There exists a rational number $q \in (a,b)$, so let $h(c) := q$.
If $d \in E$ is another discontinuity, then if $d > c$, then there
exist an $x \in I$ with $c < x < d$, and so $\lim_{x \to d^-} f(x) \geq b$.
Hence the rational number we choose for $h(d)$ is different from $q$,
since $q=h(c) < b$ and $h(d) > b$.
Similarly if $d < c$.  So after making such a choice for
every $c \in E$, we have a 
one-to-one (injective) function into $\Q$.  Therefore, $E$ is countable.
\end{proof}

\begin{example} \label{example:countdiscont}
By $\lfloor x \rfloor$ denote the largest integer less than or equal to $x$.
Define $f \colon [0,1] \to \R$ by
\begin{equation*}
f(x) :=
x +
\sum_{n=0}^{\lfloor 1/(1-x) \rfloor}
2^{-n} ,
\end{equation*}
for $x < 1$ and $f(1) = 3$.
It is left as an exercise to show that $f$ is strictly increasing, bounded, and
has a discontinuity at all points $1-\nicefrac{1}{k}$ for $k \in \N$.  In particular,
there are countably many discontinuities, but the function is bounded and
defined on a closed bounded interval.  See \figureref{fig:countdiscont}.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{increasing-discont-fig.eepic}
\caption{Increasing function with countably many
discontinuities.\label{fig:countdiscont}}
%\end{center}
\end{myfigureht}

Similarly one can find an example of a function discontinuous on a dense set
such as the rational numbers.  See the exercises.
\end{example}


\subsection{Continuity of inverse functions}


A strictly monotone function $f$ is one-to-one (injective).  To see this
notice that if $x \not= y$ then we can assume $x < y$.  Then either $f(x) <
f(y)$ if $f$ is strictly increasing or $f(x) > f(y)$ if $f$ is strictly
decreasing, so $f(x) \not= f(y)$.
Hence, it
must have an inverse $f^{-1}$ defined on its range.

\begin{prop} \label{prop:invcont}
If $I \subset \R$ is an interval and $f \colon I \to \R$ is strictly
monotone.  Then the inverse $f^{-1} \colon f(I) \to I$ is continuous.
\end{prop}

\begin{proof}
Let us suppose $f$ is strictly increasing.  The proof is almost
identical for a strictly decreasing function.
Since $f$ is strictly increasing, so is $f^{-1}$.  That is, if $f(x) <
f(y)$, then we must have $x < y$ and therefore
$f^{-1}\bigl(f(x)\bigr) < f^{-1}\bigl(f(y)\bigr)$.

Take $c \in f(I)$.
If $c$ is not a cluster point of $f(I)$, then $f^{-1}$ is continuous at $c$
automatically.  So let $c$ be a cluster point of $f(I)$.
Suppose both of the following one-sided limits exist:
\begin{align*}
x_0 & := \lim_{y \to c^-} f^{-1}(y) =
\sup \{ f^{-1}(y) : y < c, y \in f(I) \}
=
\sup \{ x \in I : f(x) < c \} , \\
x_1 & := \lim_{y \to c^+} f^{-1}(y) =
\inf \{ f^{-1}(y) : y > c, y \in f(I) \}
=
\inf \{ x \in I : f(x) > c \} .
\end{align*}
We have $x_0 \leq x_1$ as $f^{-1}$ is increasing.
For all $x > x_0$ with $x \in I$, we have $f(x) \geq c$.  As $f$ is strictly increasing,
we must have $f(x) > c$ for all $x > x_0$, $x \in I$.  Therefore,
\begin{equation*}
\{ x \in I : x > x_0 \} \subset \{ x \in I : f(x) > c \}.
\end{equation*}
The infimum of the left hand set is $x_0$, and the infimum of the right hand
set is $x_1$, so we obtain $x_0 \geq x_1$.
So $x_1 = x_0$, and $f^{-1}$ is continuous at $c$.

If one of the one-sided limits does not exist the argument is similar
and is left as an exercise.
\end{proof}

\begin{example}
The proposition does not require $f$ itself to be continuous.  For example, let
$f \colon \R \to \R$
\begin{equation*}
f(x) :=
\begin{cases}
x & \text{if $x < 0$}, \\
x+1 & \text{if $x \geq 0$}. \\
\end{cases}
\end{equation*}
The function $f$ is not continuous at $0$.
The image of $I = \R$ is the set 
$(-\infty,0)\cup [1,\infty)$, not an interval.
Then $f^{-1} \colon (-\infty,0)\cup [1,\infty)
\to \R$ can be written as
\begin{equation*}
f^{-1}(x) =
\begin{cases}
x & \text{if $x < 0$}, \\
x-1 & \text{if $x \geq 1$}. 
\end{cases}
\end{equation*}
It is not difficult to see that $f^{-1}$ is a continuous function.
\end{example}

Notice what happens with the proposition if $f(I)$ is an interval.  In that case we could simply
apply \corref{cor:continterval} to both $f$ and $f^{-1}$.  That is, if
$f \colon I \to J$ is an onto strictly monotone function and $I$ and $J$ are intervals,
then both $f$ and $f^{-1}$ are continuous.  Furthermore $f(I)$ is an
interval precisely when $f$ is continuous.

%\begin{cor} \label{cor:continterval}
%If $I \subset \R$ is an interval and $f \colon I \to \R$ is 
%strictly monotone, then $f(I)$ is an interval if and only if $f$
%is continuous.
%\end{cor}
%
%\begin{proof}
%If $f$ is continuous then $f(I)$ is an interval is a consequence of
%\hyperref[IVT:thm]{intermediate value theorem}.  See also
%\exerciseref{exercise:imageofinterval}.
%
%For the reverse direction, suppose that $f(I)$ is an interval.  Let
%$f^{-1} \colon f(I) \to I$ be
%its inverse.
%The function $f$ is the inverse of $f^{-1}$, which is itself strictly
%monotone, and so $f$ is continuous by \propref{prop:invcont}.  
%\end{proof}

\subsection{Exercises}

\begin{exercise}
Suppose $f \colon [0,1] \to \R$ is monotone.  Prove $f$ is bounded.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:monotlimits}.
Hint: You can halve your work by noticing that if $g$ is decreasing
then $-g$ is increasing.
\end{exercise}

\begin{exercise}
Finish the proof of \corref{cor:continterval}.
\end{exercise}

\begin{exercise}
Prove the claims in \exampleref{example:countdiscont}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:invcont}.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$, and $f \colon S \to \R$ is an increasing
function.
a) If $c$ is a cluster point
of $S \cap (c,\infty)$ show that 
$\lim\limits_{x\to c^+} f(x) < \infty$.
b) If $c$ is a cluster point of $S \cap (-\infty,c)$
and $\lim\limits_{x\to c^-} f(x) = \infty$, prove that 
$S \subset (-\infty,c)$.
\end{exercise}

\begin{exercise}
Suppose $I \subset \R$ is an interval and $f \colon I \to \R$ is a function
such that for each $c \in I$, there exist $a, b \in \R$ with
$a > 0$ such that $f(x) \geq a x + b$ for all $x \in I$
and $f(c) = a c + b$.  Show that $f$ is strictly increasing.
\end{exercise}

\begin{exercise}
Suppose $f \colon I \to J$ is a continuous, bijective (one-to-one and onto)
function for two intervals $I$ and $J$.  Show that $f$ is strictly monotone.
\end{exercise}

\begin{exercise}
Consider a monotone function $f \colon I \to \R$ on an interval $I$.  Prove that there exists
a function $g \colon I \to \R$ such that
$\lim\limits_{x \to c^-} g(x) = g(c)$ for all $c \in I$, except the
smaller (left) endpoint of $I$, and such that
$g(x) = f(x)$ for all but countably many $x$.
\end{exercise}

\begin{exercise}
a) Let $S \subset \R$ be any subset.  If $f \colon S \to \R$ is increasing,
then show that there exists an increasing $F \colon \R \to \R$
such that $f(x) = F(x)$ for all $x \in S$.
b) Find an example of a strictly increasing $f \colon S \to \R$ such that
an increasing $F$ as above is never strictly increasing.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:increasingfuncdiscatQ}
Find an example of an increasing function $f \colon [0,1] \to \R$
that has a discontinuity at each rational number.  Then show that the image
$f([0,1])$ contains no interval.  Hint: Enumerate
the rational numbers and define
the function with a series.
\end{exercise}

\begin{exercise}
Suppose $I$ is an interval and $f \colon I \to \R$ is monotone.
Show that $\R \setminus f(I)$ is a countable union of disjoint intervals.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to (0,1)$ is increasing.  Show that for any
$\epsilon > 0$, there exists
a strictly increasing $g \colon [0,1] \to (0,1)$ such that
$g(0) = f(0)$, $f(x) \leq g(x)$ for all $x$, and $g(1)-f(1) < \epsilon$.
\end{exercise}

\begin{exercise}
Prove that the Dirichlet function $f \colon [0,1] \to\R$ defined by $f(x) :=
1$ if $x$ is rational and $f(x) := 0$ otherwise cannot be written as a
difference of two increasing functions.  That is, there do not exist
increasing $g$ and $h$ such that, $f(x) = g(x) - h(x)$.
\end{exercise}

\begin{exercise}
Suppose $f \colon (a,b) \to (c,d)$ is a strictly increasing
onto function.  Prove that there exists a $g \colon (a,b) \to (c,d)$,
which is also strictly increasing and onto, and $g(x) < f(x)$ for all $x \in
(a,b)$.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The Derivative} \label{der:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The derivative}
\label{sec:der}

\sectionnotes{1 lecture}

The idea of a derivative is the following.
Let us suppose a graph of a function looks locally like a straight line.
We can then talk about the slope of this line.  The slope tells us 
the rate at which 
the value of the function changing at the particular point.
Of course, we are leaving out any function that has corners or
discontinuities.  Let us be precise.

\subsection{Definition and basic properties}

\begin{defn}
Let $I$ be an interval, let
$f \colon I \to \R$ be a function, and let $c \in I$.  If 
the limit
\begin{equation*}
L := \lim_{x \to c} \frac{f(x)-f(c)}{x-c} 
\end{equation*}
exists, then we say $f$ is \emph{\myindex{differentiable}} at
$c$, that $L$ is the \emph{\myindex{derivative}} of $f$ at $c$,
and write $f'(c) := L$.

\medskip

If $f$ is differentiable at all $c \in I$, then we simply say that
$f$ is \emph{differentiable}, and then we obtain a function
$f' \colon I \to \R$.

\medskip

The expression $\frac{f(x)-f(c)}{x-c}$ is called the
\emph{\myindex{difference quotient}}.
\end{defn}

The graphical interpretation of the derivative is  depicted in
\figureref{derivfig}.  The left-hand plot gives the line through
$\bigl(c,f(c)\bigr)$
and $\bigl(x,f(x)\bigr)$ with slope
$\frac{f(x)-f(c)}{x-c}$, that is,
the so-called \emph{\myindex{secant line}}.  When we take the limit as $x$ goes to $c$,
we get the right-hand plot, where we see
that the derivative of the function
at the point $c$ is the slope of the line tangent to the graph of $f$
at the point $\bigl(c,f(c)\bigr)$.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{derivdfig.eepic}
\qquad
\subimport*{figures/}{derivfig.eepic}
\caption{Graphical interpretation of the derivative.\label{derivfig}}
%\end{center}
\end{myfigureht}

We allow $I$ to be a closed interval and we allow
$c$ to be an endpoint of $I$.  Some calculus books do not allow $c$ to be an
endpoint of an interval, but all the theory still works by allowing it, and
it makes our work easier.

\begin{example}
Let $f(x) := x^2$ defined on the whole real line.  We find that if
$x \not=c$,
\begin{equation*}
\frac{x^2-c^2}{x-c} =
\frac{(x+c)(x-c)}{x-c} =
(x+c) .
\end{equation*}
Therefore,
\begin{equation*}
f'(c) = 
\lim_{x\to c} \frac{x^2-c^2}{x-c} =
\lim_{x\to c} (x+c) = 2c.
\end{equation*}
\end{example}

\begin{example}
The function $f(x) := \sqrt{x}$ is differentiable for $x > 0$.  Fix $c > 0$,
and take $x \not= c$, $x > 0$,
\begin{equation*}
\frac{\sqrt{x}-\sqrt{c}}{x-c}
=
\frac{\sqrt{x}-\sqrt{c}}{(\sqrt{x}-\sqrt{c})(\sqrt{x}+\sqrt{c})}
=
\frac{1}{\sqrt{x}+\sqrt{c}} .
\end{equation*}
Therefore,
\begin{equation*}
f'(c) =
\lim_{x\to c}
\frac{\sqrt{x}-\sqrt{c}}{x-c}
=
\lim_{x\to c}
\frac{1}{\sqrt{x}+\sqrt{c}}
=
\frac{1}{2\sqrt{c}} .
\end{equation*}
\end{example}

\begin{example}
The function $f(x) := \abs{x}$ is not differentiable
at the origin.  When $x > 0$, then
\begin{equation*}
\frac{\abs{x}-\abs{0}}{x-0} =
\frac{x-0}{x-0} = 1 ,
\end{equation*}
and when $x < 0$ we have
\begin{equation*}
\frac{\abs{x}-\abs{0}}{x-0} =
\frac{-x-0}{x-0} = -1 .
\end{equation*}
\end{example}

A famous example of Weierstrass shows that there exists a continuous
function that is not differentiable at \emph{any} point.  The construction
of this function is beyond the scope of this book.  On the other hand,
a differentiable function
is always continuous.

\begin{prop}
Let $f \colon I \to \R$ be differentiable at $c \in I$,
then it is continuous at $c$.
\end{prop}

\begin{proof}
We know the limits
\begin{equation*}
\lim_{x\to c}\frac{f(x)-f(c)}{x-c} = f'(c)
\qquad
\text{and}
\qquad
\lim_{x\to c}(x-c) = 0
\end{equation*}
exist.  Furthermore,
\begin{equation*}
f(x)-f(c) = 
\left( \frac{f(x)-f(c)}{x-c} \right) (x-c) .
\end{equation*}
Therefore the limit of $f(x)-f(c)$ exists and
\begin{equation*}
\lim_{x\to c} \bigl( f(x)-f(c) \bigr) =
\left(\lim_{x\to c} \frac{f(x)-f(c)}{x-c} \right)
\left(\lim_{x\to c} (x-c) \right) =
f'(c) \cdot 0  = 0.
\end{equation*}
Hence $\lim\limits_{x\to c} f(x) = f(c)$, and $f$ is continuous at $c$.
\end{proof}

An important property of the derivative is linearity.  The
derivative is the approximation of a function by a straight line.
The slope of a line through two points changes linearly when the
$y$-coordinates are changed linearly.  By taking the limit,
it makes sense that the derivative is linear.

\begin{prop}
\index{linearity of the derivative}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be differentiable at $c \in I$,
and let $\alpha \in \R$.
\begin{enumerate}[(i)]
\item
Define $h \colon I \to \R$ by $h(x) := \alpha f(x)$.  Then
$h$ is differentiable at $c$ and
$h'(c) = \alpha f'(c)$.
\item
Define $h \colon I \to \R$ by $h(x) :=  f(x) + g(x)$.  Then
$h$ is differentiable at $c$ and
$h'(c) =  f'(c) + g'(c)$.
\end{enumerate}
\end{prop}

\begin{proof}
First, let $h(x) := \alpha f(x)$.
For $x \in I$, $x \not= c$ we have
\begin{equation*}
\frac{h(x)-h(c)}{x-c} =
\frac{\alpha f(x) - \alpha f(c)}{x-c}
=
\alpha \frac{f(x) - f(c)}{x-c} .
\end{equation*}
The limit as $x$ goes to $c$ exists on the right
by \corref{falg:cor}.  We get
\begin{equation*}
\lim_{x\to c}\frac{h(x)-h(c)}{x-c} =
\alpha \lim_{x\to c} \frac{f(x) - f(c)}{x-c} .
\end{equation*}
Therefore $h$ is differentiable at $c$,
and the derivative is computed as given.

Next, define $h(x) := f(x)+g(x)$.
For $x \in I$, $x \not= c$ we have
\begin{equation*}
\frac{h(x)-h(c)}{x-c} =
\frac{\bigl(f(x) + g(x)\bigr) - \bigl(f(c) + g(c)\bigr)}{x-c}
=
\frac{f(x) - f(c)}{x-c}
+
\frac{g(x) - g(c)}{x-c} .
\end{equation*}
The limit as $x$ goes to $c$ exists on the right
by \corref{falg:cor}.  We get
\begin{equation*}
\lim_{x\to c}\frac{h(x)-h(c)}{x-c} =
\lim_{x\to c} \frac{f(x) - f(c)}{x-c}
+
\lim_{x\to c}\frac{g(x) - g(c)}{x-c} .
\end{equation*}
Therefore $h$ is differentiable at $c$
and the derivative is computed as given.
\end{proof}

It is not true that the derivative of a multiple of two functions is
the multiple of the derivatives.  Instead we get the so-called \emph{product
rule} or the \emph{\myindex{Leibniz rule}}%
\footnote{Named for the German mathematician
\href{http://en.wikipedia.org/wiki/Leibniz}{Gottfried Wilhelm Leibniz}
(1646--1716).}.

\begin{prop}[Product rule]\index{product rule}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be 
functions differentiable at $c$.  If $h \colon I \to \R$
is defined by
\begin{equation*}
h(x) := f(x) g(x) ,
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = f(c) g'(c) + f'(c) g(c) .
\end{equation*}
\end{prop}

The proof of the product rule is left as an exercise.  The key to the proof is 
the identity
$f(x) g(x) - f(c) g(c) =
f(x)\bigl( g(x) - g(c) \bigr)
+ \bigl( f(x) - f(c) \bigr) g(c)$,
which is illustrated in \figureref{figprodrule}.
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figprodrule.pdf_t}
\caption{The idea of product rule.  The area of the entire rectangle
$f(x)g(x)$ differs from the area of the white rectangle $f(c)g(c)$
by the area of the lightly shaded rectangle
$f(x)\bigl( g(x) - g(c) \bigr)$ plus the darker shaded rectangle
$\bigl( f(x) - f(c) \bigr) g(c)$.
In other words $\Delta (f \cdot g)
= f \cdot \Delta g + \Delta f \cdot g$.\label{figprodrule}}
%\end{center}
\end{myfigureht}



\begin{prop}[Quotient Rule]\index{quotient rule}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be differentiable at $c$
and $g(x) \not= 0$ for all $x \in I$.
If $h \colon I \to \R$
is defined by
\begin{equation*}
h(x) := \frac{f(x)}{g(x)},
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = \frac{f'(c) g(c) - f(c) g'(c)}{{\bigl(g(c)\bigr)}^2} .
\end{equation*}
\end{prop}

Again the proof is left as an exercise.

\subsection{Chain rule}

A useful rule for computing derivatives 
is the chain rule.

\begin{prop}[Chain Rule]
\index{chain rule}
Let $I_1, I_2$ be intervals, let
$g \colon I_1 \to I_2$ be differentiable at $c \in I_1$,
and
$f \colon I_2 \to \R$ be differentiable at $g(c)$.
If $h \colon I_1 \to \R$
is defined by
\begin{equation*}
h(x) := (f \circ g) (x) = f\bigl(g(x)\bigr) ,
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = f'\bigl(g(c)\bigr)g'(c) .
\end{equation*}
\end{prop}

\begin{proof}
Let $d := g(c)$.  Define
$u \colon I_2 \to \R$ and $v \colon I_1 \to \R$ by
\begin{align*}
& u(y) :=
\begin{cases}
 \frac{f(y) - f(d)}{y-d}  & \text{ if $y \not=d$,} \\
f'(d) & \text{ if $y = d$,}
\end{cases}
\\
& v(x) :=
\begin{cases}
\frac{g(x) - g(c)}{x-c} & \text{ if $x \not=c$,} \\
g'(c) & \text{ if $x = c$.}
\end{cases}
\end{align*}
We note that
\begin{equation*}
f(y)-f(d) = u(y) (y-d)
\qquad \text{and} \qquad
g(x)-g(c) = v(x) (x-c) .
\end{equation*}
We plug in to obtain
\begin{equation*}
h(x)-h(c)
=
f\bigl(g(x)\bigr)-f\bigl(g(c)\bigr)
=
u\bigl( g(x) \bigr) \bigl(g(x)-g(c)\bigr)
=
u\bigl( g(x) \bigr) \bigl(v(x) (x-c)\bigr) .
\end{equation*}
Therefore,
\begin{equation} \label{eq:chainruleeq}
\frac{h(x)-h(c)}{x-c}
=
u\bigl( g(x) \bigr) v(x) .
\end{equation}
We compute the limits $\lim_{y \to d} u(y)
= f'(d) = f'\bigl(g(c)\bigr)$ and
$\lim_{x \to c} v(x) = g'(c)$.
That is, the functions $u$ and $v$
are continuous at $d = g(c)$ and $c$ respectively.
Furthermore the function $g$ is continuous at $c$.
%We note that $\displaystyle \lim_{x\to c} v(x) = g'(c)$,
%$g$ is continuous at $c$, that is
%$\displaystyle \lim_{x\to c} g(x) = g(c)$,
%and
%finally that
%$\displaystyle \lim_{y\to g(c)} u(y) = f'\bigl(g(c)\bigr)$.
Hence the limit of
the right-hand side of \eqref{eq:chainruleeq}
as $x$ goes to $c$
exists and is equal to $f'\bigl(g(c)\bigr) g'(c)$.  Thus $h$
is differentiable at $c$ and the limit is $f'\bigl(g(c)\bigr)g'(c)$.
\end{proof}

\subsection{Exercises}

\begin{exercise}
Prove the product rule.
Hint: Use
$f(x) g(x) - f(c) g(c) = f(x)\bigl( g(x) - g(c) \bigr) + \bigl( f(x) -
f(c) \bigr) g(c)$.
\end{exercise}

\begin{exercise}
Prove the quotient rule.  Hint: You can do this directly, but it may be
easier to find the derivative of $\nicefrac{1}{x}$ and then use
the chain rule and the product rule.
\end{exercise}

\begin{exercise} \label{exercise:diffofxn}
For $n \in \Z$,
prove that $x^n$ is differentiable and find the derivative,
unless, of course, $n < 0$ and $x=0$.
Hint: Use the product rule.
\end{exercise}

\begin{exercise}
Prove that a polynomial is differentiable and find the derivative.
Hint: Use the previous exercise.
\end{exercise}

\begin{exercise}
Define $f \colon \R \to \R$ by
\begin{equation*}
f(x) :=
\begin{cases}
x^2 & \text{ if $x \in \Q$,}\\
0 & \text{ otherwise.}
\end{cases}
\end{equation*}
Prove that $f$ is differentiable at $0$, but discontinuous at all points
except $0$.
\end{exercise}

\begin{exercise}
Assume the inequality $\abs{x-\sin(x)} \leq x^2$.  Prove that sin is
differentiable at $0$, and find the derivative at $0$.
\end{exercise}

\begin{exercise}
Using the previous exercise, prove that sin is differentiable at all $x$
and that the derivative is $\cos(x)$.  Hint: Use the sum-to-product
trigonometric identity as we did before.
\end{exercise}

\begin{exercise}
Let $f \colon I \to \R$ be differentiable.  Given $n \in \Z$, define $f^n$
be the function defined by $f^n(x) := {\bigl( f(x) \bigr)}^n$.  If
$n < 0$ assume $f(x) \not= 0$.  Prove that
$(f^n)'(x) = n {\bigl(f(x) \bigr)}^{n-1} f'(x)$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a differentiable
Lipschitz continuous function.
Prove that $f'$ is a bounded function.
\end{exercise}

\begin{exercise}
Let $I_1, I_2$ be intervals.
Let $f \colon I_1 \to I_2$ be a bijective function and $g \colon I_2 \to I_1$
be the inverse.  Suppose that both $f$ is differentiable at $c \in I_1$ and
$f'(c) \not=0$ and $g$ is differentiable at $f(c)$.  Use the chain rule
to find a formula for $g'\bigl(f(c)\bigr)$ (in terms of $f'(c)$).
\end{exercise}

\begin{exercise} \label{exercise:bndmuldiff}
Suppose $f \colon I \to \R$ is bounded and $g \colon I \to
\R$ is differentiable at $c \in I$ and $g(c) = g'(c) = 0$.  Show
that $h(x) := f(x) g(x)$ is differentiable at $c$.  Hint: You
cannot apply the product rule.
\end{exercise}

\begin{exercise} \label{exercise:diffsqueeze}
Suppose $f \colon I \to \R$, 
$g \colon I \to \R$, and
$h \colon I \to \R$, are functions.  Suppose $c \in I$ is such that
$f(c) = g(c) = h(c)$, $g$ and $h$ are differentiable at $c$,
and $g'(c) = h'(c)$.  Furthermore suppose $h(x) \leq f(x) \leq g(x)$ for
all $x \in I$.  Prove $f$ is differentiable at $c$ and $f'(c) = g'(c) =
h'(c)$.
\end{exercise}

\begin{exercise}
Suppose $f \colon (-1,1) \to \R$ is a function such that $f(x) = x h(x)$ for a bounded
function $h$.  a)~Show that $g(x) := {\bigl( f(x) \bigr)}^2$ is
differentiable at the origin and $g'(0) = 0$.  b)~Find an example of a
continuous function $f \colon (-1,1) \to \R$ with $f(0) = 0$, but such
that $g(x) := {\bigl( f(x) \bigr)}^2$ is not differentiable at the origin.
\end{exercise}

\begin{exercise}
Suppose $f \colon I \to \R$ is differentiable at $c \in I$.
Prove there exist numbers $a$ and $b$ with the property that
for every $\epsilon > 0$, there is a $\delta > 0$, such that
$\abs{a+b(x-c) - f(x)} \leq \epsilon \abs{x-c}$, whenever $x \in I$ and
$\abs{x-c} < \delta$.
In other words, show that
there exists a function $g \colon I \to \R$
such that $\lim_{x\to c} g(x) = 0$ and
$\abs{a+b(x-c) - f(x)} \leq \abs{x-c} g(x)$.
\end{exercise}

\begin{exercise} \label{exercise:simpleLHopital}
Prove the following simple version of \myindex{L'Hopital's rule}.  Suppose 
$f \colon (a,b) \to \R$ and $g \colon (a,b) \to \R$ are differentiable
functions
whose derivatives $f'$ and $g'$ are continuous functions.
Suppose that at $c \in (a,b)$, $f(c) = 0$, $g(c)=0$,
and
$g'(x) \not= 0$ for all $x \in (a,b)$, and suppose
that the limit of $\nicefrac{f'(x)}{g'(x)}$ as $x$ goes to $c$ exists.  Show that
\begin{equation*}
\lim_{x \to c} \frac{f(x)}{g(x)} = 
\lim_{x \to c} \frac{f'(x)}{g'(x)} .
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Mean value theorem}
\label{sec:mvt}

\sectionnotes{2 lectures (some applications may be skipped)}

\subsection{Relative minima and maxima}

We talked about absolute maxima and minima.  These are the tallest peaks and
lowest valleys in the whole mountain range.  We also want to talk
about peaks of individual mountains and bottoms of individual valleys.

\begin{defn}
Let $S \subset \R$ be a set and
let $f \colon S \to \R$ be a function.  The function $f$ is said to have
a \emph{\myindex{relative maximum}} at $c \in S$ if there exists a $\delta>0$
such that for all $x \in S$ where $\abs{x-c} < \delta$
we have $f(x) \leq f(c)$.  The definition of \emph{\myindex{relative
minimum}} is analogous.
\end{defn}

\begin{lemma}\label{relminmax:lemma}
Let $f \colon [a,b] \to \R$ be a function differentiable at $c \in (a,b)$,
and $c$
is a relative minimum or a relative maximum of $f$.  Then
$f'(c) = 0$.
\end{lemma}

\begin{proof}
We prove the statement for a maximum.  For a minimum the statement
follows by considering the function $-f$.

Let $c$ be a relative maximum of $f$.  In particular as long
as $\abs{x-c} < \delta$ we have $f(x)-f(c) \leq 0$.
Then we look at the difference
quotient.  If $x > c$ we note that
\begin{equation*}
\frac{f(x)-f(c)}{x-c} \leq 0 ,
\end{equation*}
and if $y < c$ we have
\begin{equation*}
\frac{f(y)-f(c)}{y-c} \geq 0 .
\end{equation*}
See \figureref{fig:critpt} for an illustration.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{critpt.eepic}
\caption{Slopes of secants at a relative maximum.\label{fig:critpt}}
%\end{center}
\end{myfigureht}

As $a < c < b$, there exist
sequences $\{ x_n\}$ and $\{ y_n \}$, such
that $x_n > c$, and
$y_n < c$ for all $n \in \N$, and such that
 $\lim\, x_n = \lim\, y_n = c$.
Since $f$
is differentiable at $c$ we know 
\begin{equation*}
0 \geq \lim_{n\to\infty} \frac{f(x_n)-f(c)}{x_n-c} 
=
f'(c)
=
\lim_{n\to\infty} \frac{f(y_n)-f(c)}{y_n-c} \geq 0.  \qedhere
\end{equation*}
\end{proof}

For a differentiable function, a point where 
$f'(c) = 0$ is called a \emph{\myindex{critical point}}.  When $f$ is not
differentiable at some points,
it is common to also say $c$ is a critical point
if $f'(c)$ does not exist.
The theorem says that a relative minimum or maximum at an interior point
of an interval must be a critical point.
As you remember from calculus, finding minima and maxima of a function can
be done by finding all the critical points together with the endpoints of
the interval and simply checking at which of these points
is the function biggest or smallest.

\subsection{Rolle's theorem}

Suppose a function has the same value at both endpoints of an interval.
Intuitively it ought to attain a minimum or a maximum in the interior of the
interval,
then at such a minimum or a maximum, the derivative should be zero.
See \figureref{rollefig} for the geometric idea.  This is the content of the
so-called Rolle's theorem%
\footnote{Named after the French mathematician
\href{https://en.wikipedia.org/wiki/Michel_Rolle}{Michel Rolle}
(1652--1719).}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{rollefig.eepic}
\caption{Point where tangent line is horizontal, that is $f'(c) =
0$.\label{rollefig}}
%\end{center}
\end{myfigureht}

\begin{thm}[Rolle] \label{thm:rolle}
\index{Rolle's theorem}
Let $f \colon [a,b] \to \R$ be continuous function
differentiable on $(a,b)$ such that $f(a) = f(b)$.
Then there exists a $c \in (a,b)$ such that $f'(c) = 0$.
\end{thm}

\begin{proof}
As $f$ is continuous on $[a,b]$ it attains an absolute minimum and an
absolute 
maximum in $[a,b]$.  We wish to apply \lemmaref{relminmax:lemma} and
so we need a minimum or maximum at some $c \in (a,b)$.
Write $K := f(a) = f(b)$.
If there exists an $x$ such that $f(x) > K$, then the absolute
maximum is bigger than $K$ and hence occurs at $c \in (a,b)$, and
therefore we get $f'(c) = 0$.  On the other hand if there exists an $x$
such that $f(x) < K$, then the absolute minimum occurs at some
$c \in (a,b)$ and we have that $f'(c) = 0$.  If there is no $x$ such that
$f(x) > K$ or
$f(x) < K$, then we have that $f(x) = K$ for all $x$ and then
$f'(x) = 0$ for all $x \in [a,b]$, so any $c \in (a,b)$ works.
%If it attains an absolute maximum at $c \in (a,b)$, then $c$
%is also a relative maximum and we apply 
%\lemmaref{relminmax:lemma} to find that $f'(c) = 0$.
%If the absolute maximum is at $a$ or at $b$,
%then we look for the absolute minimum.
%If the absolute minimum is at $c \in (a,b)$,
%then again we find that $f'(c) = 0$.  So suppose that the absolute
%minimum is also at $a$ or $b$.  Write $k := f(a) = f(b)$.
%Hence the absolute minimum is $k$ and
%the absolute maximum is $k$, and the function is
%identically $k$.  Thus $f'(x) = 0$ for all $x \in [a,b]$, so pick
%an arbitrary $c \in (a,b)$.
\end{proof}

It is absolutely necessary for the derivative to exist for all $x
\in (a,b)$.  For example take the function $f(x) = \abs{x}$ on $[-1,1]$.
Clearly $f(-1) = f(1)$, but there is no point where $f'(c) = 0$.

\subsection{Mean value theorem}

We extend \hyperref[thm:rolle]{Rolle's theorem}
to functions that attain different
values at the endpoints.

\begin{thm}[Mean value theorem] \label{thm:mvt}
\index{mean value theorem}
Let $f \colon [a,b] \to \R$ be a continuous function
differentiable on $(a,b)$.  Then there exists a point $c \in (a,b)$
such that
\begin{equation*}
f(b)-f(a) = f'(c)(b-a) .
\end{equation*}
\end{thm}

For a geometric interpretation of the mean value theorem, see
\figureref{mvtfig}.  The idea is that the value $\frac{f(b)-f(a)}{b-a}$
is the slope of the line between the points $\bigl(a,f(a)\bigr)$
and $\bigl(b,f(b)\bigr)$.
Then $c$ is the point such that $f'(c) = \frac{f(b)-f(a)}{b-a}$, that 
is, the tangent line at the point $\bigl(c,f(c)\bigr)$ has the same slope as the
line between $\bigl(a,f(a)\bigr)$ and $\bigl(b,f(b)\bigr)$.
The theorem follows from \hyperref[thm:rolle]{Rolle's theorem},
by subtracting from $f$ the affine linear function with the derivative
$\frac{f(b)-f(a)}{b-a}$ with the same values at $a$ and $b$ as $f$.
That is, we subtract the function whose graph is the straight line
$\bigl(a,f(a)\bigr)$ and $\bigl(b,f(b)\bigr)$.
Then we are looking for a point where this new
function has derivative zero.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{mvtfig.eepic}
\caption{Graphical interpretation of the mean value theorem.\label{mvtfig}}
%\end{center}
\end{myfigureht}


\begin{proof}
Define the
function $g \colon [a,b] \to \R$ by
\begin{equation*}
g(x) := f(x)-f(b)-\frac{f(b)-f(a)}{b-a}(x-b) .
\end{equation*}
The function $g$ is differentiable on $(a,b)$,
continuous on $[a,b]$, such that $g(a) = 0$ and $g(b) = 0$.  Thus there exists
a
$c \in (a,b)$ such that $g'(c) = 0$.
\begin{equation*}
0 = g'(c) = f'(c)-\frac{f(b)-f(a)}{b-a}
\end{equation*}
Or in other words
$f'(c)(b-a) = f(b)-f(a)$.
\end{proof}

The proof generalizes.  By considering
$g(x) :=
f(x)-f(b)-\frac{f(b)-f(a)}{\varphi(b)-\varphi(a)}\bigl(\varphi(x)-\varphi(b)\bigr)$
one can prove the following version.  We leave the proof as an exercise.

\begin{thm}[Cauchy's mean value theorem] \label{thm:cauchymvt}
\index{Cauchy's mean value theorem}
Let $f \colon [a,b] \to \R$ and $\varphi \colon [a,b] \to \R$ be continuous
functions
differentiable on $(a,b)$.  Then there exists a point $c \in (a,b)$
such that
\begin{equation*}
\bigl(f(b)-f(a)\bigr)\varphi'(c) = f'(c)\bigl(\varphi(b)-\varphi(a)\bigr) .
\end{equation*}
\end{thm}

The mean value theorem has the distinction of being one of the few theorems
commonly cited
in court.  That is, when police measure the speed of cars by aircraft, or
via cameras reading license plates, they 
measure the time the car takes to go between two points.
The mean value theorem then
says that the car must have somewhere attained the speed you get by dividing the
difference in distance by the difference in time.



\subsection{Applications}

We now solve our very first differential equation.

\begin{prop} \label{prop:derzeroconst}
Let $I$ be an interval and
let $f \colon I \to \R$ be a differentiable function such that $f'(x) = 0$
for all $x \in I$.
Then $f$ is constant.
\end{prop}

\begin{proof}
Take arbitrary $x,y \in I$ with $x < y$.
Then $f$ restricted to $[x,y]$ satisfies the hypotheses
of the \hyperref[thm:mvt]{mean value theorem}.
Therefore there is a $c \in (x,y)$ such that
\begin{equation*}
f(y)-f(x) = f'(c)(y-x).
\end{equation*}
as $f'(c) = 0$, we have $f(y) = f(x)$.  Therefore,
the function is constant.
\end{proof}

Now that we know what it means for the function to stay constant, let us look
at increasing and decreasing functions.
We say $f \colon I \to \R$ is \emph{\myindex{increasing}}
(resp.\  \emph{\myindex{strictly increasing}}) if
$x < y$ implies $f(x) \leq f(y)$ (resp.\ $f(x) < f(y)$).
We define
\emph{\myindex{decreasing}} and
\emph{\myindex{strictly decreasing}} in the same way by switching the
inequalities for $f$.

\begin{prop} \label{incdecdiffprop}
Let $I$ be an interval and
let $f \colon I \to \R$ be a differentiable function.
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
\begin{enumerate}[(i)]
\item $f$ is increasing if and only if $f'(x) \geq 0$ for all $x \in I$.
\item $f$ is decreasing if and only if $f'(x) \leq 0$ for all $x \in I$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us prove the first item.  Suppose $f$ is increasing, then
for all $x$ and $c$ in $I$ we have
\begin{equation*}
\frac{f(x)-f(c)}{x-c} \geq 0 .
\end{equation*}
Taking a limit as $x$ goes to $c$ we see that $f'(c) \geq 0$.

For the other direction, suppose $f'(x) \geq 0$ for all $x \in I$.
Take any $x, y \in I$ where $x < y$.  By the \hyperref[thm:mvt]{mean value theorem}
there is some $c \in
(x,y)$ such that
\begin{equation*}
f(y)-f(x) = f'(c)(y-x) .
\end{equation*}
As $f'(c) \geq 0$, and $y-x > 0$, then $f(y) - f(x) \geq 0$ or $f(x) \leq
f(y)$ and so
$f$ is increasing.

We leave the decreasing part to the reader as exercise.
\end{proof}

\begin{example}
A similar but weaker statement is true for strictly increasing and
decreasing functions.  If $f'(x) > 0$ for all $x \in I$, then
$f$ is strictly increasing.  The proof is left as an exercise.
The converse is not true.  For example,
$f(x) := x^3$ is a strictly increasing function, but $f'(0) = 0$.
\end{example}

Another application of the \hyperref[thm:mvt]{mean value theorem} is the following result about
location of extrema.  The theorem is stated for an absolute minimum and
maximum, but the way it is applied to find relative minima
and maxima is to restrict $f$ to an interval $(c-\delta,c+\delta)$.

\begin{prop} \label{firstderminmaxtest}
Let $f \colon (a,b) \to \R$ be continuous.  Let $c \in (a,b)$
and suppose
$f$ is differentiable on $(a,c)$ and $(c,b)$.
\begin{enumerate}[(i)]
\item If $f'(x) \leq 0$ for $x \in (a,c)$ and
 $f'(x) \geq 0$ for $x \in (c,b)$, then $f$ has an absolute minimum 
at $c$.
\item If $f'(x) \geq 0$ for $x \in (a,c)$ and
 $f'(x) \leq 0$ for $x \in (c,b)$, then $f$ has an absolute maximum
at $c$.
\end{enumerate}
\end{prop}

\begin{proof}
We prove the first item leaving the second to the reader.
Take $x \in (a,c)$
and $\{ y_n\}$ a sequence such that $x < y_n < c$ and $\lim\, y_n = c$.
By the preceding proposition,
$f$ is decreasing on $(a,c)$ so $f(x) \geq f(y_n)$.
As $f$ is
continuous at $c$, we take the limit to get
$f(x) \geq f(c)$ for all $x \in (a,c)$.

Similarly take $x \in (c,b)$
and $\{ y_n\}$ a sequence such that $c < y_n < x$ and $\lim\, y_n = c$.
The function is increasing on $(c,b)$ so $f(x) \geq f(y_n)$.
By continuity of $f$ we get
$f(x) \geq f(c)$ for all $x \in (c,b)$.  Thus $f(x) \geq f(c)$ for all
$x \in (a,b)$.
\end{proof}

The converse of the proposition does not hold.  See
\exampleref{baddifffunc:example} below.

\subsection{Continuity of derivatives and the intermediate value theorem}

Derivatives of functions satisfy an
intermediate value property.
%The result is usually
%called \myindex{Darboux's theorem}.

\begin{thm}[Darboux] \label{thm:darboux} \index{Darboux's theorem}
Let $f \colon [a,b] \to \R$ be differentiable.  Suppose $y \in \R$ is such
that $f'(a) < y < f'(b)$ or
$f'(a) > y > f'(b)$.  Then there exists a $c \in (a,b)$ such that $f'(c) =
y$.
\end{thm}

The proof follows by subtracting $f$ and a linear function with derivative
$y$.  The new function $g$ reduces the problem
to the case $y=0$, where $g'(a) > 0 > g'(b)$.  That is, $g$ is increasing at $a$ and
decreasing at $b$, so it must attain a maximum inside $(a,b)$,
where the derivative is zero.  See \figureref{darbouxthmfig}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{darbouxthmfig.eepic}
\caption{Idea of the proof of Darboux theorem.\label{darbouxthmfig}}
%\end{center}
\end{myfigureht}

\begin{proof}
Suppose 
$f'(a) < y < f'(b)$.
Define
\begin{equation*}
g(x) := yx - f(x) .
\end{equation*}
The function $g$ is continuous on $[a,b]$, and so $g$ attains a maximum at some $c \in
[a,b]$.

The function $g$ is also differentiable on $[a,b]$.
Compute $g'(x) = y-f'(x)$.  Thus $g'(a) > 0$.  As the derivative is
the limit of difference quotients and is positive, there must be some
difference quotient that is positive.  That is, there must exist
an $x > a$ such that
\begin{equation*}
\frac{g(x)-g(a)}{x-a} > 0 ,
\end{equation*}
or $g(x) > g(a)$.  Thus $a$
cannot possibly be a maximum of $g$.  Similarly as $g'(b) < 0$,
we find an $x < b$ (a different $x$) such that
$\frac{g(x)-g(b)}{x-b} < 0$ or that $g(x) > g(b)$, thus
$b$ cannot possibly be a maximum.
Therefore $c \in (a,b)$
and \hyperref[thm:rolle]{Rolle's theorem} applies: As $g$ attains a maximum
at $c$ we find $g'(c) = 0$
and $f'(c) = y$.

Similarly if $f'(a) > y > f'(b)$, consider $g(x) := f(x)- yx$.
\end{proof}

We have seen already that
there exist discontinuous functions that have the
intermediate value property.  While it is hard to imagine at first, there
also
exist functions that are differentiable everywhere and the derivative is not
continuous.

\begin{example} \label{baddifffunc:example}
Let $f \colon \R \to \R$ be the function defined by
\begin{equation*}
f(x) :=
\begin{cases}
{\bigl( x \sin(\nicefrac{1}{x}) \bigr)}^2 & \text{ if $x \not= 0$,} \\
0 & \text{ if $x = 0$.}
\end{cases}
\end{equation*}
We claim that $f$ is differentiable everywhere, but
$f' \colon \R \to \R$ is not continuous at
the origin.  Furthermore, $f$ has a minimum at 0, but the derivative
changes sign infinitely often near the origin.
See \figureref{fig:nonc1diff}.
\begin{myfigureht}
%\begin{center}
\includegraphics{figures/nonc1difffig}
\qquad
\includegraphics{figures/nonc1diffderfig}
\caption{A function with a discontinuous derivative. The function $f$ is on the left
and $f'$ is on the right.  Notice that $f(x) \leq x^2$ on the left graph.\label{fig:nonc1diff}}
%\end{center}
\end{myfigureht}

Proof: It is immediate from the definition that $f$ has an absolute
minimum at 0: we know $f(x) \geq 0$ for all $x$ and $f(0) = 0$.

The function $f$ is differentiable for $x\not=0$,
and
the derivative 
is $2 \sin (\nicefrac{1}{x}) \bigl( x \sin (\nicefrac{1}{x}) -
\cos(\nicefrac{1}{x}) \bigr)$.
As an exercise show that for $x_n = \frac{4}{(8n+1)\pi}$
we have
$\lim\, f'(x_n) = -1$, and for
$y_n = \frac{4}{(8n+3)\pi}$  we have
$\lim\, f'(y_n) = 1$.  Hence if $f'$ exists at $0$,
then it cannot be continuous.

Let us show that $f'$ exists at 0.  We claim that the derivative is zero.
In other words $\abs{\frac{f(x)-f(0)}{x-0} - 0}$ goes to zero
as $x$ goes to zero.  For $x \not= 0$ we have
\begin{equation*}
\abs{\frac{f(x)-f(0)}{x-0} - 0}
=
\abs{\frac{x^2 \sin^2(\nicefrac{1}{x})}{x}}
=
\abs{x \sin^2(\nicefrac{1}{x})}
\leq
\abs{x} .
\end{equation*}
And, of course, as $x$ tends to zero, then $\abs{x}$ tends to zero and hence
$\abs{\frac{f(x)-f(0)}{x-0} - 0}$ goes to zero.  Therefore, $f$
is differentiable at 0 and the derivative at 0 is 0.
A key point in the above calculation is that 
is that $\abs{f(x)} \leq x^2$,
see also Exercises \ref{exercise:bndmuldiff} and
\ref{exercise:diffsqueeze}.
\end{example}

It is sometimes useful to assume the derivative of a differentiable
function is continuous.  If $f \colon I \to \R$ is differentiable and
the derivative $f'$ is continuous on $I$, then we say $f$ is
\emph{\myindex{continuously differentiable}}.  It is common to
write $C^1(I)$ for the set of continuously differentiable functions on $I$.

\subsection{Exercises}

\begin{exercise}
Finish the proof of \propref{incdecdiffprop}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{firstderminmaxtest}.
\end{exercise}

\begin{exercise} \label{exercise:boundeddermeanslip}
Suppose $f \colon \R \to \R$ is a differentiable
function such that $f'$ is a bounded function.  Prove
$f$ is a Lipschitz continuous function.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is differentiable and $c \in [a,b]$.
Then show there exists a sequence $\{ x_n \}$ converging to $c$, $x_n
\not= c$ for all $n$, such that
\begin{equation*}
f'(c) = \lim_{n\to \infty} f'(x_n).
\end{equation*}
Do note this does \emph{not} imply that $f'$ is continuous (why?).
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a function such that
$\abs{f(x)-f(y)} \leq \abs{x-y}^2$ for all $x$ and $y$.  Show that
$f(x) = C$ for some constant $C$.  Hint: Show that $f$ is differentiable
at all points and compute the derivative.
\end{exercise}

\begin{exercise} \label{exercise:posderincr}
Suppose $I$ is an interval and
$f \colon I \to \R$ is a differentiable function.
If $f'(x) > 0$ for all $x \in I$, show that $f$ is strictly increasing.
\end{exercise}

\begin{exercise}
Suppose $f \colon (a,b) \to \R$ is a differentiable function
such that
$f'(x) \not= 0$ for all $x \in (a,b)$.  Suppose there
exists
a point $c \in (a,b)$ such that $f'(c) > 0$.
Prove $f'(x) > 0$ for all $x \in (a,b)$.
\end{exercise}

\begin{exercise} \label{exercise:samediffconst}
Suppose $f \colon (a,b) \to \R$ and $g \colon (a,b) \to \R$ are
differentiable functions such that $f'(x) = g'(x)$ for all $x \in (a,b)$,
then show that there exists a constant $C$ such that $f(x) = g(x) + C$.
\end{exercise}

\begin{exercise}
Prove the following version of \myindex{L'Hopital's rule}.  Suppose 
$f \colon (a,b) \to \R$ and $g \colon (a,b) \to \R$ are differentiable
functions.  Suppose that at $c \in (a,b)$, $f(c) = 0$, $g(c)=0$,
$g'(x) \not= 0$ when $x \not= c$, and
that the limit of $\nicefrac{f'(x)}{g'(x)}$ as $x$ goes to $c$ exists.  Show that
\begin{equation*}
\lim_{x \to c} \frac{f(x)}{g(x)} = 
\lim_{x \to c} \frac{f'(x)}{g'(x)} .
\end{equation*}
Compare to \exerciseref{exercise:simpleLHopital}.
\end{exercise}

\begin{exercise}
Let $f \colon (a,b) \to \R$ be an unbounded differentiable function.  Show
$f' \colon (a,b) \to \R$ is unbounded.
\end{exercise}

\begin{exercise}
Prove the theorem Rolle actually proved in 1691:
\emph{If $f$ is a polynomial,
$f'(a) = f'(b) = 0$ for some $a < b$,
and there is no $c \in (a,b)$ such that $f'(c) = 0$,
then there is at most one root of $f$ in $(a,b)$,
that is at most one $x \in (a,b)$ such that $f(x) = 0$.}
In other words, between any two consecutive roots of $f'$ is at most one
root of $f$.
Hint: Suppose there are two roots and see what happens.
\end{exercise}

\begin{exercise}
Suppose $a,b \in \R$ and $f \colon \R \to \R$ is differentiable,
$f'(x) = a$ for all $x$, and $f(0) = b$.  Find $f$ and prove that 
it is the unique differentiable function with this property.
\end{exercise}

\begin{exercise} \label{exercise:extendboundedder}
Suppose $f \colon (0,1) \to \R$ is differentiable and $f'$
is bounded.\\
a) Show that there exists a continuous function $g \colon [0,1) \to \R$
such that $f(x) = g(x)$ for all $x \not= 0$.  Hint: \propref{context:prop} and
\exerciseref{exercise:boundeddermeanslip}.
\\
b) Find an example where the $g$ is not differentiable at $x=0$.
Hint: Consider something based on $\sin(\ln x)$,
and assume you know basic properties of
$\sin$ and $\ln$ from calculus.
\end{exercise}

\begin{exercise}
Suppose $f \colon (a,b) \to \R$ is differentiable everywhere but at $c \in
(a,b)$ and $\lim_{x \to c} f'(x) = L$.  Show that $f$ is differentiable at
$c$ and $f'(c) = L$.
\end{exercise}

\begin{exercise}
Prove \thmref{thm:cauchymvt}.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Taylor's theorem}
\label{sec:taylor}

\sectionnotes{half a lecture (optional section)}

\subsection{Derivatives of higher orders}

When $f \colon I \to \R$ is differentiable, we obtain a function
$f' \colon I \to \R$.  The function
$f'$ is called the \emph{\myindex{first derivative}} of $f$.
If $f'$ is differentiable, we denote by
$f'' \colon I \to \R$ the derivative of $f'$.  The function $f''$
is called the \emph{\myindex{second derivative}} of $f$.
We similarly obtain
$f'''$, $f''''$, and so on.
With a larger number of derivatives
the notation would get out of hand; we denote
by $f^{(n)}$ the \emph{$n$th derivative}\index{nth derivative} of $f$.

When $f$ possesses $n$ derivatives, we say $f$ is
\emph{$n$ times differentiable}\index{n times differentiable}.

\subsection{Taylor's theorem}

Taylor's theorem%
\footnote{Named for the English mathematician
\href{http://en.wikipedia.org/wiki/Brook_Taylor}{Brook Taylor}
(1685--1731).
It was first found by
the Scottish mathematician
\href{http://en.wikipedia.org/wiki/James_Gregory_(mathematician)}{James Gregory}
(1638 -- 1675).  The statement we give
was proved by
\href{http://en.wikipedia.org/wiki/Lagrange}{Joseph-Louis Lagrange}
(1736 -- 1813)}
is a generalization of the \hyperref[thm:mvt]{mean value theorem}.
Mean value theorem says that up to a small error $f(x)$ for $x$ near $x_0$ can be
approximated by $f(x_0)$, that is
\begin{equation*}
f(x) = f(x_0) + f'(c)(x-x_0),
\end{equation*}
where the ``error'' is measured in terms of the first derivative
at some point $c$ between $x$ and $x_0$.
Taylor's theorem generalizes this result to higher derivatives.
It tells us that up to a small error, any $n$
times differentiable function can be approximated at a point $x_0$
by a polynomial.  The
error of this approximation behaves like ${(x-x_0)}^{n}$ near the point $x_0$.
To see why this is a good approximation notice that for a big $n$, 
${(x-x_0)}^n$ is very small in a small interval around $x_0$.

\begin{defn}
For an $n$ times differentiable function $f$ defined near a point $x_0 \in \R$, define the
$n$th \emph{\myindex{Taylor polynomial}}%
\index{nth Taylor polynomial for f}
for $f$ at $x_0$ as
\begin{equation*}
\begin{split}
P_n^{x_0}(x)
& :=
\sum_{k=0}^n
\frac{f^{(k)}(x_0)}{k!}{(x-x_0)}^k
\\
& =
f(x_0)
+ f'(x_0)(x-x_0)
+ \frac{f''(x_0)}{2}{(x-x_0)}^2
+ \frac{f^{(3)}(x_0)}{6}{(x-x_0)}^3
+ \cdots
+ \frac{f^{(n)}(x_0)}{n!}{(x-x_0)}^n .
\end{split}
\end{equation*}
\end{defn}

See \figureref{fig:taylorsin} for
the odd degree Taylor polynomials for the sin function at $x_0=0$.
The even degree terms are all zero, as even derivatives 
of sine are again a sine, which are zero at the origin.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{taylorsin.eepic}
\caption{The odd degree Taylor polynomials for the sine
function.\label{fig:taylorsin}}
%\end{center}
\end{myfigureht}

Taylor's theorem says a function behaves like its $n$th
Taylor polynomial.  The 
\hyperref[thm:mvt]{mean value theorem} is really Taylor's theorem
for the first derivative.

\begin{thm}[Taylor]\index{Taylor's theorem} \label{thm:taylor}
Suppose $f \colon [a,b] \to \R$ is a function with $n$ continuous
derivatives on $[a,b]$ and such that $f^{(n+1)}$ exists on $(a,b)$.
Given distinct points $x_0$ and $x$ in $[a,b]$,
we can find a point $c$ between $x_0$
and $x$ such that
\begin{equation*}
f(x)=P_{n}^{x_0}(x)+\frac{f^{(n+1)}(c)}{(n+1)!}{(x-x_0)}^{n+1} .
\end{equation*}
\end{thm}

The term $R_n^{x_0}(x):=\frac{f^{(n+1)}(c)}{(n+1)!}{(x-x_0)}^{n+1}$ is called the
\emph{remainder term}\index{remainder term in Taylor's formula}.  This
form 
of the remainder term is called the
\emph{\myindex{Lagrange form}} of the remainder.  There are other ways
to write the remainder term, but we skip those.  Note that $c$ depends on
both $x$ and $x_0$.

\begin{proof}
Find a number $M_{x,x_0}$ (depending on $x$ and $x_0$) solving the equation
\begin{equation*}
f(x)=P_{n}^{x_0}(x)+M_{x,x_0}{(x-x_0)}^{n+1} .
\end{equation*}
Define a function $g(s)$ by
\begin{equation*}
g(s) := f(s)-P_n^{x_0}(s)-M_{x,x_0}{(s-x_0)}^{n+1} .
\end{equation*}
We compute
%A simple computation shows that 
the $k$th derivative at $x_0$ of the Taylor polynomial
${(P_n^{x_0})}^{(k)}(x_0) = f^{(k)}(x_0)$ for
$k=0,1,2,\ldots,n$ (the zeroth derivative of a function is the function
itself).  Therefore,
\begin{equation*}
g(x_0) = g'(x_0) = g''(x_0) = \cdots = g^{(n)}(x_0) = 0 .
\end{equation*}
In particular $g(x_0) = 0$.
On the other hand $g(x) = 0$.  By the
\hyperref[thm:mvt]{mean value theorem}
there exists an $x_1$ between $x_0$ and $x$ such that $g'(x_1) = 0$.
Applying the \hyperref[thm:mvt]{mean value theorem}
to $g'$ we obtain that there exists
$x_2$ between $x_0$ and $x_1$ (and therefore between $x_0$ and $x$)
such that $g''(x_2) = 0$.  We repeat the
argument $n+1$ times to obtain a number $x_{n+1}$ between $x_0$ and $x_n$
(and therefore between $x_0$ and $x$) such that $g^{(n+1)}(x_{n+1}) = 0$.

Let $c:=x_{n+1}$.
We compute the $(n+1)$th derivative of $g$ to find
\begin{equation*}
g^{(n+1)}(s) = f^{(n+1)}(s)-(n+1)!\,M_{x,x_0} .
\end{equation*}
Plugging in $c$ for $s$ we obtain $M_{x,x_0} = \frac{f^{(n+1)}(c)}{(n+1)!}$, and
we are done.
\end{proof}

In the proof we have computed 
${(P_n^{x_0})}^{(k)}(x_0) = f^{(k)}(x_0)$ for $k=0,1,2,\ldots,n$.
Therefore the Taylor polynomial has the same derivatives as $f$ at $x_0$
up to the $n$th derivative.  That is why the Taylor polynomial is
a good approximation to $f$.
Notice that in \figureref{fig:taylorsin} the Taylor polynomials are
reasonably good approximations to the sine near $x=0$.

One does not necessarily get good Taylor polynomial approximations
everywhere.  For example, if we start expanding the function $f(x) =
\frac{x}{1-x}$ around 0, we get the graphs in
\figureref{fig:taylorgeom}.  The dashed line is
the 20th degree polynomial, and yet the approximation only seems to get
better with the degree for $x > -1$, and for smaller $x$, it in fact gets worse.
The polynomials
are the partial sums of the geometric series $\sum_{n=1}^\infty x^n$,
and the series only converges on $(-1,1)$.
See the discussion of power series
\sectionref{sec:moreonseries}.

\begin{myfigureht}
%\begin{center}
% No fonts there so don't do the eepic, but do the pdf
\includegraphics{figures/taylorgeom}
\caption{The function $\frac{x}{1-x}$, and the Taylor polynomials
$P_1^0$, $P_2^0$, $P_3^0$ (all dotted), and the polynomial $P_{20}^0$
(dashed).\label{fig:taylorgeom}}
%\end{center}
\end{myfigureht}

If $f$ is infinitely differentiable, that is, if $f$ can be
differentiated any number of times, then 
we define the \emph{\myindex{Taylor series}}:
\begin{equation*}
T^{x_0}(x)
:=
\sum_{k=0}^\infty
\frac{f^{(k)}(x_0)}{k!}{(x-x_0)}^k .
\end{equation*}
However, there is no guarantee that this series converges for any
$x \not= x_0$.  And even where it does converge, there is no guarantee
that it converges to the function $f$.  Functions for which
the Taylor series converges to the function at least in an interval
around every point are called
\emph{\myindex{analytic functions}}\index{analytic function}.
Most functions one tends to see in practice are in fact analytic.
See \exerciseref{exercise:nonanalytic}, for an example of a non-analytic
function.

The definition of derivative says that
a function is
differentiable if it
is locally approximated by a line.
%, that is the definition of the derivative.
Similarly we mention in passing that there exists a converse to Taylor's
theorem,
which we will neither state nor prove,
saying that if a function is
locally approximated in a certain way by a polynomial of degree $d$, then it
has $d$ derivatives.

\subsection{Exercises}

\begin{exercise}
Compute the $n$th Taylor Polynomial at $0$ for the exponential function.
\end{exercise}

\begin{exercise}
Suppose $p$ is a polynomial of degree $d$.  Given any $x_0 \in \R$,
show that
the $(d+1)$th Taylor polynomial for $p$ at $x_0$ is equal to $p$.
\end{exercise}

\begin{exercise}
Let $f(x) := \abs{x}^3$.  Compute $f'(x)$ and $f''(x)$ for all $x$,
but show that $f^{(3)}(0)$ does not exist.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ has $n$ continuous derivatives.  Show
that for any $x_0 \in \R$,
there exist polynomials $P$ and $Q$ of degree $n$ and 
an $\epsilon > 0$ such that $P(x) \leq f(x) \leq Q(x)$ for all $x \in
[x_0-\epsilon,x_0+\epsilon]$  and
$Q(x)-P(x) = \lambda {(x-x_0)}^n$ for some $\lambda \geq 0$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ has $n+1$ continuous derivatives
and $x_0 \in [a,b]$,
prove
$\lim\limits_{x\to x_0} \frac{R_n^{x_0}(x)}{{(x-x_0)}^n} = 0$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ has $n+1$ continuous derivatives
and $x_0 \in (a,b)$.
Show that $f^{(k)}(x_0) = 0$ for all $k = 0, 1, 2, \ldots, n$
if and only if $g(x) := \frac{f(x)}{{(x-x_0)}^{n+1}}$ is continuous at $x_0$.
\end{exercise}

\begin{exercise}
Suppose $a,b,c \in \R$ and $f \colon \R \to \R$ is differentiable,
$f''(x) = a$ for all $x$, $f'(0) = b$, and $f(0) = c$.  Find $f$ and prove that 
it is the unique differentiable function with this property.
\end{exercise}

\begin{exercise}[Challenging]
Show that a simple converse to Taylor's theorem does not hold.
Find a function $f \colon \R \to \R$ with no second derivative at $x=0$ such that
$\abs{f(x)} \leq \abs{x^3}$, that is, $f$ goes to zero at 0 faster than $x^3$, and
while $f'(0)$ exists, $f''(0)$ does not.
\end{exercise}

\begin{exercise} \label{exercise:extendboundedder2}
Suppose $f \colon (0,1) \to \R$ is differentiable and $f''$
is bounded.\\
a) Show that there exists a once differentiable function $g \colon [0,1) \to \R$
such that $f(x) = g(x)$ for all $x \not= 0$.  Hint: 
See
\exerciseref{exercise:extendboundedder}.
\\
b) Find an example where the $g$ is not twice differentiable at $x=0$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Inverse function theorem}
\label{sec:ift}

\sectionnotes{less than 1 lecture (optional section, needed for
\sectionref{sec:logandexp}, requires 
\sectionref{sec:monotonefunc})}

\subsection{Inverse function theorem}

The main idea of differentiating inverse functions is the following lemma.

\begin{lemma} \label{lemma:ift}
Let $I,J \subset \R$ be intervals.
If $f \colon I \to J$ is strictly monotone (hence one-to-one),
onto ($f(I) = J$),
differentiable at $x$, and $f'(x) \not= 0$,
then the inverse 
$f^{-1}$ is differentiable at $y = f(x)$ and
\begin{equation*}
(f^{-1})'(y) = \frac{1}{f'\bigl( f^{-1}(y) \bigr)} = \frac{1}{f'(x)} .
\end{equation*}
If $f$ is continuously differentiable and $f'$ is never zero, then $f^{-1}$
is continuously differentiable.
\end{lemma}

\begin{proof}
By \propref{prop:invcont} $f$ has a continuous inverse.  Let us
call the inverse $g \colon J \to I$ for convenience.
Let $x,y$ be as in the statement, take $t \in I$ to be arbitrary
and let $s := f(t)$.
Then
\begin{equation*}
\frac{g(s)-g(y)}{s-y} =
\frac{g\bigl(f(t)\bigr)-g\bigl(f(x)\bigr)}{f(t)-f(x)} =
\frac{t-x}{f(t)-f(x)} .
\end{equation*}
As $f$ is differentiable at $x$ and $f'(x) \not= 0$, then 
$\frac{t-x}{f(t)-f(x)} \to \nicefrac{1}{f'(x)}$ as $t \to x$.
Because $g(s) \to g(y)$ as $s \to y$, we 
can plug in $g(s)$ for $t$, and $g(y)$ for $x$ and take the limit
as $s$ goes to $y$, that is, the limit exists.  In other words,
\begin{equation*}
\lim_{s \to y}
\frac{g(s)-g(y)}{s-y} 
=
\lim_{t \to x} \frac{t-x}{f(t)-f(x)} 
=
\frac{1}{f'(x)} 
=
\frac{1}{f'\bigl(g(y)\bigr)}
\end{equation*}
See \figureref{inversefig} for the geometric idea.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{inversefigA.eepic}
\quad
\subimport*{figures/}{inversefigB.eepic}
\caption{Interpretation of the derivative of the inverse
function.\label{inversefig}}
%\end{center}
\end{myfigureht}

If both $f'$ and $g$ are continuous, $f'$ is nonzero at all $x$,
then the lemma applies at all points $x \in I$ and the resulting function $g'(y) =
\frac{1}{f'\bigl(g(t)\bigr)}$ must be continuous.
\end{proof}

What is usually called the inverse function theorem is the following result.

\begin{thm}[Inverse function theorem]\index{inverse function theorem}
Let $f \colon (a,b) \to \R$ be a continuously differentiable function,
$x_0 \in (a,b)$ a point where $f'(x_0) \not= 0$.  Then there exists
an interval $I \subset (a,b)$ with $x_0 \in I$, the
restriction $f|_{I}$ is injective with an inverse
$g \colon J \to I$ defined on $J := f(I)$,
which is continuously differentiable and
\begin{equation*}
g'(y) = \frac{1}{f'\bigl( g(y) \bigr)} , \qquad \text{for all $y \in J$}.
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality, suppose $f'(x_0) > 0$.  As $f'$ is
continuous, there must exist an interval $I$ with $x_0 \in I$
such that $f'(x) > 0$ for all $x_0 \in I$.

By \exerciseref{exercise:posderincr} $f$ is strictly increasing
on $I$, and hence the restriction $f|_{I}$ bijective onto $J: = f(I)$.
As $f$ is continuous, then by the
\hyperref[IVT:thm]{intermediate value theorem}
(see also \corref{cor:continterval}), $f(I)$ is in interval.
Now apply \lemmaref{lemma:ift}.
\end{proof}

If you tried to prove the existence of roots directly as in
\exampleref{example:sqrt2} you may have seen
how difficult that endeavor is.  However, with the machinery we have built
for inverse functions it becomes
an almost trivial exercise, and with and the inverse function theorem
we prove far more than mere existence.

\begin{cor}
Given any $n \in \N$ and any $x \geq 0$ there exists a unique 
number $y \geq 0$ (denoted $x^{1/n} := y$), such that $y^n = x$.  Furthermore,
the function $g \colon (0,\infty) \to (0,\infty)$ defined by
$g(x) := x^{1/n}$ is continuously differentiable and
\begin{equation*}
g'(x) = \frac{1}{nx^{(n-1)/n}} = \frac{1}{n} \, x^{(1-n)/n} ,
\end{equation*}
using the convention $x^{n/m} := {(x^{1/m})}^{n}$.
\end{cor}

\begin{proof}
For $x=0$ the existence of a unique root is trivial.

Let $f(x) := x^n$.  Using product rule, $f$ is continuously differentiable
and $f'(x) = nx^{n-1}$, see \exerciseref{exercise:diffofxn}.  For $x > 0$ the derivative $f'$ is strictly positive
and so again by \exerciseref{exercise:posderincr}, $f$ is strictly
increasing (this can also be proved directly).  It is also easy to
see that the image of $f$ is the entire interval $(0,\infty)$.  We 
obtain a unique inverse $g$ and so the existence and uniqueness of positive
$n$th roots.  We apply \lemmaref{lemma:ift} to obtain the derivative.
\end{proof}

\begin{example}
The corollary provides a good example of where the inverse function theorem
gives us an interval smaller than $(a,b)$.  Take $f \colon \R \to \R$
defined by $f(x) := x^2$.  Then $f'(x) \not= 0$
as long as $x \not= 0$.  If $x_0 > 0$, we can take $I=(0,\infty)$, but
no larger.
\end{example}

\begin{example}
Another useful example is $f(x) := x^3$.  The function $f \colon \R \to \R$ is
one-to-one and onto, so $f^{-1}(x) = x^{1/3}$ exists on the entire real
line including zero and negative $x$.  The function $f$ has
a continuous derivative, but $f^{-1}$ has no derivative at the origin.  The
point is that $f'(0) = 0$.  See \figureref{cubecuberootfig} for a graph,
notice the vertical tangent on the cube root at the origin.
See also \exerciseref{exercise:oddroot}.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{cubecuberoot.eepic}
\caption{Graphs of $x^3$ and $x^{1/3}$.\label{cubecuberootfig}}
%\end{center}
\end{myfigureht}
\end{example}


\subsection{Exercises}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuously differentiable such that
$f'(x) > 0$ for all $x$.  Show that $f$ is invertible on the interval $J =
f(\R)$, the inverse is continuously differentiable, and ${(f^{-1})}'(y) >
0$ for all $y \in f(\R)$.
\end{exercise}

%FIXME: we changed the lemma to be exactly this
%\begin{exercise}
%Prove the following version of the inverse function theorem:
%Let $I,J \subset \R$ be intervals.
%Let $f \colon I \to J$ be strictly monotone (hence one-to-one)
%and onto.  Suppose $f$ is differentiable at $x_0$
%and $f'(x_0) \not= 0$.  Then prove that the inverse
%$f^{-1}$ is differentiable at $y_0 = f'(x_0)$ and
%$(f^{-1})'(y_0) = \frac{1}{f'(x_0)}$.
%\end{exercise}

\begin{exercise}
Suppose $I,J$ are intervals and a monotone onto $f \colon I \to J$ has an inverse $g \colon J \to I$.
Suppose you already know that both $f$ and $g$ are differentiable
everywhere and $f'$ is never zero.  Using chain rule but not \lemmaref{lemma:ift} prove the
formula $g'(y) = \frac{1}{f'(g(y))}$. % \bigl( \bigr) are overly big here!
\end{exercise}

\begin{exercise}
Let $n\in \N$ be even.
Prove that every $x > 0$ has a unique negative $n$th root.
That is, there exists a negative number $y$ such that $y^n = x$.
Compute the derivative
of the function $g(x) := y$.
\end{exercise}

\begin{exercise} \label{exercise:oddroot}
Let $n \in \N$ be odd and $n \geq 3$.
Prove that every $x$ has a unique $n$th root.
That is, there exists a number $y$ such that $y^n = x$.  Prove that
the function defined by $g(x) := y$ is differentiable except at $x=0$
and compute the derivative.  Prove that $g$ is not differentiable at $x=0$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:taylor}]
Show that if in the inverse function theorem $f$ has $k$ continuous
derivatives, then the inverse function $g$ also has $k$ continuous
derivatives.
\end{exercise}

\begin{exercise}
Let $f(x) := x + 2 x^2 \sin(\nicefrac{1}{x})$ for $x \not= 0$ and
$f(0) = 0$.  Show that $f$ is differentiable at all $x$, that $f'(0) > 0$,
but that $f$ is not invertible
on any interval containing the origin.
\end{exercise}

\begin{exercise}
a) Let $f \colon \R \to \R$ be a continuously differentiable function
and $k > 0$ be a number such that $f'(x) \geq k$ for all $x \in \R$.
Show $f$ is one-to-one and onto, and has a continuously differentiable
inverse $f^{-1} \colon \R \to \R$. b) Find an example $f \colon \R \to \R$
where $f'(x) > 0$
for all $x$, but $f$ is not onto.
\end{exercise}

\begin{exercise}
Suppose $I,J$ are intervals and a monotone onto $f \colon I \to J$ has an inverse $g \colon J \to I$.
Suppose $x \in I$ and $y := f(x) \in J$, and that $g$ is differentiable at
$y$.  Prove:
\\
a) If $g'(y) \not= 0$, then $f$ is differentiable at $x$.
\\
b) If $g'(y) = 0$, then $f$ is not differentiable at $x$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The Riemann Integral} \label{int:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Riemann integral}
\label{sec:rint}

\sectionnotes{1.5 lectures}

We now get to the fundamental concept of integration.  There is
often confusion among students of
calculus between \emph{integral} and \emph{antiderivative}.
The integral is (informally) the area under the curve, nothing else.
That we can compute an antiderivative using the integral is a nontrivial
result we have to prove.  
In this chapter we define the \emph{Riemann integral}%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Riemann}{Georg Friedrich Bernhard Riemann}
(1826--1866).}
using the Darboux integral%
\footnote{Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Darboux}{Jean-Gaston Darboux} (1842--1917).},
which is technically simpler than (but equivalent to) the traditional
definition as done by Riemann.

\subsection{Partitions and lower and upper integrals}

We want to integrate a bounded function defined on an interval $[a,b]$.
We first define two auxiliary integrals that can be defined for all
bounded functions.  Only then can we talk about the Riemann integral and
the Riemann integrable functions.

\begin{defn}
A \emph{\myindex{partition}} $P$ of the interval $[a,b]$ is
a finite set of numbers $\{ x_0,x_1,x_2,\ldots,x_n \}$ such that
\begin{equation*}
a = x_0 < x_1 < x_2 < \cdots < x_{n-1} < x_n = b .
\end{equation*}
We write
\begin{equation*}
\Delta x_i := x_i - x_{i-1} .
\end{equation*}
%"size" was never used
%We say $P$ is of size $n$.

\medskip

Let $f \colon [a,b] \to \R$ be a bounded function.  Let $P$ be a partition of
$[a,b]$.
Define
\begin{align*}
& m_i := \inf \{ f(x) : x_{i-1} \leq x \leq x_i \} , \\
& M_i := \sup \{ f(x) : x_{i-1} \leq x \leq x_i \} , \\
& L(P,f) :=
\sum_{i=1}^n m_i \Delta x_i , \\
& U(P,f) :=
\sum_{i=1}^n M_i \Delta x_i .
\end{align*}
We call $L(P,f)$ the \emph{\myindex{lower Darboux sum}} and
$U(P,f)$ the \emph{\myindex{upper Darboux sum}}\index{Darboux sum}.
\end{defn}

The geometric idea of Darboux sums is indicated in
\figureref{darbouxfig}.  The lower sum is the area of the shaded
rectangles, and the upper sum is the area of the entire
rectangles, shaded plus unshaded parts.  The width of the $i$th rectangle is $\Delta x_i$,
the height of the shaded rectangle is $m_i$ and the height
of the entire rectangle is $M_i$.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{darbouxfig.eepic}
\caption{Sample Darboux sums.\label{darbouxfig}}
%\end{center}
\end{myfigureht}

\begin{prop} \label{sumulbound:prop}
Let $f \colon [a,b] \to \R$ be a bounded function.  Let $m, M \in \R$ be 
such that for all $x$ we have $m \leq f(x) \leq M$.  For any partition $P$ of $[a,b]$
we have
\begin{equation}
\label{sumulbound:eq}
m(b-a) \leq
L(P,f) \leq U(P,f)
\leq M(b-a) .
\end{equation}
\end{prop}

\begin{proof}
Let $P$ be a partition.  Then note that $m \leq m_i$ for all $i$ and
$M_i \leq M$ for all $i$.  Also $m_i \leq M_i$ for all $i$.  Finally
$\sum_{i=1}^n \Delta x_i = (b-a)$.  Therefore,
\begin{multline*}
m(b-a) =
m \left( \sum_{i=1}^n \Delta x_i \right)
=
\sum_{i=1}^n m \Delta x_i
\leq
\sum_{i=1}^n m_i \Delta x_i 
\leq
\\
\leq
\sum_{i=1}^n M_i \Delta x_i
\leq
\sum_{i=1}^n M \Delta x_i 
=
M \left( \sum_{i=1}^n \Delta x_i \right)
=
M(b-a) .
\end{multline*}
Hence we get \eqref{sumulbound:eq}.  In particular, the set of lower and
upper sums are bounded sets.
\end{proof}

%FIXME: use the following snippet idea for upper and lower integrals
%& \displaystyle{\int_{\mspace{-12mu} \underline{\mspace{10mu}} % \medspace
%a}^{b} f(x) \, dx} \\

\begin{defn}
As the sets of lower and upper Darboux sums are bounded, we define
\begin{align*}
& \underline{\int_a^b} f(x)~dx := \sup \{ L(P,f) : P \text{ a
partition of $[a,b]$} \} , \\
& \overline{\int_a^b} f(x)~dx := \inf \{ U(P,f) : P \text{ a
partition of $[a,b]$} \} .
\end{align*}
We call $\underline{\int}$ the \emph{\myindex{lower Darboux integral}} and
$\overline{\int}$ the \emph{\myindex{upper Darboux integral}}.
To avoid worrying about the variable of integration, 
we often simply write
\begin{equation*}
\underline{\int_a^b} f :=
\underline{\int_a^b} f(x)~dx 
\qquad \text{and} \qquad
\overline{\int_a^b} f :=
\overline{\int_a^b} f(x)~dx  .
\end{equation*}
\end{defn}

If integration is to make sense, then the lower and upper Darboux
integrals should be the same number, as we want a single number to call
\emph{the integral}.  However, these two integrals may in fact differ for
some functions.

\begin{example}
Take the Dirichlet function $f \colon [0,1] \to \R$, where $f(x) := 1$ if
$x \in \Q$ and $f(x) := 0$ if $x \notin \Q$.  Then
\begin{equation*}
\underline{\int_0^1} f = 0 \qquad \text{and} \qquad
\overline{\int_0^1} f = 1 .
\end{equation*}
The reason is that for every $i$ we have 
$m_i = \inf \{ f(x) : x \in [x_{i-1},x_i] \} = 0$  and
$M_i = \sup \{ f(x) : x \in [x_{i-1},x_i] \} = 1$.  Thus
\begin{align*}
& L(P,f) = \sum_{i=1}^n 0 \cdot \Delta x_i = 0 , \\
& U(P,f) = \sum_{i=1}^n 1 \cdot \Delta x_i = \sum_{i=1}^n \Delta x_i = 1  .
\end{align*}
\end{example}

\begin{remark}
The same definition of $\underline{\int_a^b} f$ and
$\overline{\int_a^b} f$
is used when $f$ is defined on a larger set $S$ such that
$[a,b] \subset S$.  In that case, we use the restriction of $f$ to $[a,b]$
and we must ensure that the restriction is bounded on $[a,b]$.
\end{remark}

To compute the integral we often take a partition $P$ and make it finer.
That is, we cut intervals in the partition into yet smaller pieces.

\begin{defn}
Let $P := \{ x_0, x_1, \ldots, x_n \}$ and
$\widetilde{P} := \{ \widetilde{x}_0, \widetilde{x}_1, \ldots, \widetilde{x}_m \}$ be
partitions of $[a,b]$.  We say $\widetilde{P}$ is a
\emph{refinement}\index{refinement of a partition} of $P$
if as sets $P \subset \widetilde{P}$.
\end{defn}

That is, $\widetilde{P}$ is a refinement of a partition if it contains all the
numbers in $P$ and perhaps some other numbers in between.  For example,
$\{ 0, 0.5, 1, 2 \}$ is a partition of $[0,2]$ and
$\{ 0, 0.2, 0.5, 1, 1.5, 1.75, 2 \}$ is a refinement.
The main reason for introducing refinements is the following proposition.

\begin{prop} \label{prop:refinement}
Let $f \colon [a,b] \to \R$ be a bounded function, and let $P$
be a partition of $[a,b]$.  Let $\widetilde{P}$ be a refinement of $P$.
Then
\begin{equation*}
L(P,f) \leq L(\widetilde{P},f) 
\qquad \text{and} \qquad
U(\widetilde{P},f) \leq U(P,f) .
\end{equation*}
\end{prop}

\begin{proof}
The tricky part of this proof is to get the notation correct.
Let $\widetilde{P} := \{ \widetilde{x}_0, \widetilde{x}_1, \ldots,
\widetilde{x}_m \}$ be
a refinement of 
$P := \{ x_0, x_1, \ldots, x_n \}$.  Then
$x_0 = \widetilde{x}_0$ and 
$x_n = \widetilde{x}_m$.  In fact, we can find integers
$k_0 < k_1 < \cdots < k_n$ such that $x_j = \widetilde{x}_{k_j}$ for
$j=0,1,2,\ldots,n$.

Let $\Delta \widetilde{x}_p = \widetilde{x}_{p-1} - \widetilde{x}_p$.
See \figureref{fig:refinement}.
We get 
\begin{equation*}
\Delta x_j
=
x_j - x_{j-1} =
\widetilde{x}_{k_j} - \widetilde{x}_{k_{j-1}} =
\sum_{p=k_{j-1}+1}^{k_j} 
\widetilde{x}_{p} - \widetilde{x}_{p-1}
=
\sum_{p=k_{j-1}+1}^{k_j} \Delta \widetilde{x}_p .
\end{equation*}
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{figrefinement.pdf_t}
\caption{Refinement of a subinterval.  Notice $\Delta x_j =
\Delta \widetilde{x}_{p-2} +
\Delta \widetilde{x}_{p-1} +
\Delta \widetilde{x}_{p}$,
and also
$k_{j-1}+1 = p-2$ and
$k_{j} = p$.\label{fig:refinement}}
%\end{center}
\end{myfigureht}

Let $m_j$ be as before and correspond to the partition $P$.
Let $\widetilde{m}_j := \inf \{ f(x) : \widetilde{x}_{j-1} \leq x \leq
\widetilde{x}_j \}$.
Now, $m_j \leq \widetilde{m}_p$ for $k_{j-1} < p \leq k_j$.  Therefore,
\begin{equation*}
m_j \Delta x_j
=
m_j \sum_{p=k_{j-1}+1}^{k_j} \Delta \widetilde{x}_p
=
\sum_{p=k_{j-1}+1}^{k_j} m_j \Delta \widetilde{x}_p
\leq
\sum_{p=k_{j-1}+1}^{k_j} \widetilde{m}_p \Delta \widetilde{x}_p .
\end{equation*}
So
\begin{equation*}
L(P,f) =
\sum_{j=1}^n m_j \Delta x_j
\leq
\sum_{j=1}^n \,
\sum_{p=k_{j-1}+1}^{k_j} \widetilde{m}_p \Delta \widetilde{x}_p
=
\sum_{j=1}^m
\widetilde{m}_j \Delta \widetilde{x}_j = L(\widetilde{P},f).
\end{equation*}

The proof of $U(\widetilde{P},f) \leq U(P,f)$ is left as an exercise.
\end{proof}

Armed with refinements we prove the following.
The key point of this next proposition is that
the lower Darboux integral is less than or equal to the upper Darboux
integral.

\begin{prop} \label{intulbound:prop}
Let $f \colon [a,b] \to \R$ be a bounded function.  Let $m, M \in \R$ be 
such that for all $x \in [a,b]$ we have $m \leq f(x) \leq M$.  Then
\begin{equation}
\label{intulbound:eq}
m(b-a) \leq
\underline{\int_a^b} f \leq \overline{\int_a^b} f
\leq M(b-a) .
\end{equation}
\end{prop}

\begin{proof}
By \propref{sumulbound:prop} we have for any partition $P$
\begin{equation*}
m(b-a) \leq L(P,f) \leq U(P,f) \leq M(b-a).
\end{equation*}
The inequality
$m(b-a) \leq L(P,f)$ implies $m(b-a) \leq \underline{\int_a^b} f$.
Also
$U(P,f) \leq M(b-a)$ implies $\overline{\int_a^b} f \leq M(b-a)$.

The middle inequality in
\eqref{intulbound:eq} is the main point of this proposition.
Let $P_1, P_2$ be partitions of $[a,b]$.  Define 
$\widetilde{P} := P_1 \cup P_2$.
The set $\widetilde{P}$ is a partition of $[a,b]$, which
is a refinement of $P_1$ and a refinement of $P_2$.
By \propref{prop:refinement},
$L(P_1,f) \leq L(\widetilde{P},f)$ and
$U(\widetilde{P},f) \leq U(P_2,f)$.  So
\begin{equation*}
L(P_1,f) \leq L(\widetilde{P},f) \leq U(\widetilde{P},f) \leq U(P_2,f) .
\end{equation*}
In other words, for two arbitrary partitions $P_1$ and $P_2$, we have
$L(P_1,f) \leq U(P_2,f)$.  
Recall \propref{infsupineq:prop}, and take the supremum and
infimum over all partitions:
\begin{equation*}
\underline{\int_a^b} f = 
\sup \{ L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\leq
\inf \{ U(P,f) : \text{$P$ a partition of $[a,b]$} \}
=
\overline{\int_a^b} f . \qedhere
\end{equation*}
\end{proof}

\subsection{Riemann integral}

We can finally define the Riemann integral.  However, the Riemann
integral is only defined on a certain class of functions, called the
Riemann integrable functions.

\begin{defn}
Let $f \colon [a,b] \to \R$ be a bounded function such that
\begin{equation*}
\underline{\int_a^b} f(x)~dx = \overline{\int_a^b} f(x)~dx .
\end{equation*}
Then $f$ is said to be \emph{\myindex{Riemann integrable}}.
The set of Riemann integrable functions on $[a,b]$ is denoted
by $\sR[a,b]$.  When $f \in \sR[a,b]$ we define
\begin{equation*}
\int_a^b f(x)~dx := 
\underline{\int_a^b} f(x)~dx = \overline{\int_a^b} f(x)~dx .
\end{equation*}
As before, we often simply write
\begin{equation*}
\int_a^b f := \int_a^b f(x)~dx.
\end{equation*}
The number $\int_a^b f$ is called the \emph{\myindex{Riemann integral}}
of $f$, or sometimes simply the \emph{integral} of $f$.
\end{defn}

By definition, any Riemann integrable function is bounded.
By appealing to \propref{intulbound:prop} we immediately obtain
the following proposition.  See also \figureref{fig:integralminmax}.

\begin{prop} \label{intbound:prop}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.
Let $m, M \in \R$ be 
such that $m \leq f(x) \leq M$ for all $x \in [a,b]$.  Then
\begin{equation*}
m(b-a) \leq
\int_a^b f
\leq M(b-a) .
\end{equation*}
\end{prop}
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{integralminmax.eepic}
\caption{The area under the curve is bounded from above by
the area of the entire rectangle, $M(b-a)$, and from below by
the area of the shaded part, $m(b-a)$.\label{fig:integralminmax}}
%\end{center}
\end{myfigureht}

Often we use a weaker form of this proposition.  That is, if
$\abs{f(x)} \leq M$ for all $x \in [a,b]$, then
\begin{equation*}
\abs{\int_a^b f} \leq M(b-a) .
\end{equation*}

\begin{example}
We integrate constant functions using
\propref{intulbound:prop}.
If $f(x) := c$ for some constant $c$, then we take $m = M = c$.
In inequality \eqref{intulbound:eq}
all the inequalities must be equalities.
Thus $f$ is integrable on $[a,b]$ and $\int_a^b f = c(b-a)$.
\end{example}

\begin{example}
Let $f \colon [0,2] \to \R$ be defined by
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x < 1$,}\\
\nicefrac{1}{2} & \text{ if $x = 1$,}\\
0 & \text{ if $x > 1$.}
\end{cases}
\end{equation*}
We claim $f$ is Riemann integrable and $\int_0^2 f = 1$.

Proof: Let $0 < \epsilon < 1$ be arbitrary.
Let $P := \{0, 1-\epsilon, 1+\epsilon, 2\}$ be a partition.  We use the notation from
the definition of the Darboux sums.  Then
\begin{align*}
m_1 &= \inf \{ f(x) : x \in [0,1-\epsilon] \} = 1 , & 
M_1 &= \sup \{ f(x) : x \in [0,1-\epsilon] \} = 1 , \\
m_2 &= \inf \{ f(x) : x \in [1-\epsilon,1+\epsilon] \} = 0 , & 
M_2 &= \sup \{ f(x) : x \in [1-\epsilon,1+\epsilon] \} = 1 , \\
m_3 &= \inf \{ f(x) : x \in [1+\epsilon,2] \} = 0 , & 
M_3 &= \sup \{ f(x) : x \in [1+\epsilon,2] \} = 0 .
\end{align*}
Furthermore, $\Delta x_1 = 1-\epsilon$, $\Delta x_2 = 2\epsilon$ and
$\Delta x_3 = 1-\epsilon$.
We compute
\begin{align*}
& L(P,f) = \sum_{i=1}^3 m_i \Delta x_i =
1 \cdot (1-\epsilon) + 0 \cdot 2\epsilon + 0 \cdot (1-\epsilon)
= 1-\epsilon , \\
& U(P,f) = \sum_{i=1}^3 M_i \Delta x_i =
1 \cdot (1-\epsilon) + 1 \cdot 2\epsilon + 0 \cdot (1-\epsilon)
= 1+\epsilon .
\end{align*}
Thus,
\begin{equation*}
\overline{\int_0^2} f - 
\underline{\int_0^2} f
\leq
U(P,f) - L(P,f)
=
(1+\epsilon)
- (1-\epsilon) = 2 \epsilon .
\end{equation*}
See \figureref{darbouxfigstep}.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{darbouxfigstep.eepic}
\caption{Darboux sums for the step function.  $L(P,f)$ is the area of the
shaded rectangle, $U(P,f)$ is the area of both rectangles, and
$U(P,f)-L(P,f)$ is the area of the unshaded rectangle.\label{darbouxfigstep}}
%\end{center}
\end{myfigureht}
By \propref{intulbound:prop} we have $\underline{\int_0^2} f \leq \overline{\int_0^2} f$.
As $\epsilon$ was arbitrary we see 
$\overline{\int_0^2} f = \underline{\int_0^2} f$.  So $f$ is Riemann
integrable.  Finally,
\begin{equation*}
1-\epsilon = L(P,f) \leq \int_0^2 f \leq U(P,f) =
1+\epsilon.
\end{equation*}
Hence, $\bigl\lvert \int_0^2 f - 1 \bigr\rvert \leq \epsilon$.  As $\epsilon$ was arbitrary,
we have $\int_0^2 f = 1$.
\end{example}

It may be worthwhile to extract part of the technique of the example into a
proposition.

\begin{prop}
Let $f \colon [a,b] \to \R$ be a bounded function.  Then $f$ is Riemann
integrable if for every $\epsilon > 0$, there exists a partition $P$ such that
\begin{equation*}
U(P,f) - L(P,f) < \epsilon .
\end{equation*}
\end{prop}

\begin{proof}
If for every $\epsilon > 0$, a $P$ exists we have:
\begin{equation*}
0 \leq
\overline{\int_a^b} f - 
\underline{\int_a^b} f
\leq
U(P,f) - L(P,f) < \epsilon .
\end{equation*}
Therefore, 
$\overline{\int_a^b} f = \underline{\int_a^b} f$, and $f$ is integrable.
\end{proof}

\begin{example}
Let us show $\frac{1}{1+x}$ is integrable on $[0,b]$ for any $b > 0$.
We will see later that all continuous functions are integrable, but let us
demonstrate how we do it directly.

Let $\epsilon > 0$ be given.  Take $n \in \N$ and
pick $x_j := \nicefrac{jb}{n}$, to form the 
partition $P := \{ x_0,x_1,\ldots,x_n \}$ of $[0,b]$.
We have $\Delta x_j = \nicefrac{b}{n}$ for all $j$.  
As $f$ is decreasing, for any subinterval $[x_{j-1},x_j]$ we obtain
\begin{equation*}
m_j = \inf \left\{ \frac{1}{1+x} : x \in [x_{j-1},x_j] \right\} = \frac{1}{1+x_j} ,
\qquad
M_j = \sup \left\{ \frac{1}{1+x} : x \in [x_{j-1},x_j] \right\} =
\frac{1}{1+x_{j-1}} .
\end{equation*}
Then we have
\begin{multline*}
U(P,f)-L(P,f)
=
\sum_{j=1}^n
\Delta x_j
(M_j-m_j)
=
\frac{b}{n}
\sum_{j=1}^n 
\left( \frac{1}{1+\nicefrac{(j-1)b}{n}} - \frac{1}{1+\nicefrac{jb}{n}} \right) 
=
\\
=
\frac{b}{n}
\left( \frac{1}{1+\nicefrac{0b}{n}} - \frac{1}{1+\nicefrac{nb}{n}} \right) 
=
\frac{b^2}{n(b+1)} .
\end{multline*}
The sum telescopes, the terms successively cancel each other, something
we have seen before.
Picking $n$ to be such that
$\frac{b^2}{n(b+1)} < \epsilon$ the proposition is satisfied, and the
function is integrable.
\end{example}

\subsection{More notation}

When $f \colon S \to \R$ is defined on a larger set $S$ and
$[a,b] \subset S$,
we say $f$ is Riemann integrable on $[a,b]$ if the restriction of $f$
to $[a,b]$ is Riemann integrable. 
In this case,
we say $f \in \sR[a,b]$,
and
we write $\int_a^b f$ to mean the Riemann integral
of the restriction of $f$ to $[a,b]$.

It is useful to define the integral $\int_a^b f$ even if
$a \not< b$.  Suppose $b < a$ and $f \in \sR[b,a]$,
then define
\begin{equation*}
\int_a^b f := - \int_b^a f .
\end{equation*}
For any function $f$ we define
\begin{equation*}
\int_a^a f := 0 .
\end{equation*}

At times, the variable $x$ may already have some other meaning.  When
we need to write down the variable of integration, we may simply
use a different letter.  For example,
\begin{equation*}
\int_a^b f(s)~ds := \int_a^b f(x)~dx .
\end{equation*}

\subsection{Exercises}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be defined by $f(x) := x^3$
and let $P := \{ 0, 0.1, 0.4, 1 \}$.  Compute $L(P,f)$ and $U(P,f)$.
\end{exercise}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be defined by $f(x) := x$.
Show that $f \in \sR[0,1]$ and
compute $\int_0^1 f$ using the definition of the integral
(but
feel free to use the propositions of this section).%\propref{intulbound:prop}).
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be a bounded function.
Suppose there exists a sequence of partitions $\{ P_k \}$ of $[a,b]$
such that
\begin{equation*}
\lim_{k \to \infty} \bigl( U(P_k,f) - L(P_k,f) \bigr) = 0 .
\end{equation*}
Show that $f$ is Riemann integrable and that
\begin{equation*}
\int_a^b f = 
\lim_{k \to \infty} U(P_k,f)
=
\lim_{k \to \infty} L(P_k,f) .
\end{equation*}
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:refinement}.
\end{exercise}

\begin{exercise}
Suppose $f \colon [-1,1] \to \R$ is defined as
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x > 0$,} \\
0 & \text{ if $x \leq 0$.}
\end{cases}
\end{equation*}
Prove that $f \in \sR[-1,1]$ and
compute $\int_{-1}^1 f$ using the definition of the integral
(but
feel free to use the propositions of this section).
%(feel free to use \propref{intulbound:prop}).
\end{exercise}

\begin{exercise}
Let $c \in (a,b)$ and let $d \in \R$.
Define $f \colon [a,b] \to \R$ as
\begin{equation*}
f(x) :=
\begin{cases}
d & \text{ if $x = c$,} \\
0 & \text{ if $x \not= c$.}
\end{cases}
\end{equation*}
Prove that $f \in \sR[a,b]$ and
compute
$\int_a^b f$ using the definition of the integral
%(feel free to use \propref{intulbound:prop}).
(but
feel free to use the propositions of this section).
\end{exercise}

\begin{exercise} \label{exercise:taggedpartition}
Suppose $f \colon [a,b] \to \R$ is Riemann integrable.  Let $\epsilon
> 0$ be given.  Then show that there exists a partition $P = \{ x_0, x_1,
\ldots, x_n \}$
such that if we
pick any set of numbers $\{ c_1, c_2, \ldots, c_n \}$ with
$c_k \in [x_{k-1},x_k]$ for all $k$, then
\begin{equation*}
\abs{\int_a^b f - \sum_{k=1}^n f(c_k) \Delta x_k} < \epsilon .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.
Let $\alpha > 0$ and $\beta \in \R$.
Then define $g(x) := f(\alpha x + \beta)$ on the interval
$I = [\frac{a-\beta}{\alpha}, \frac{b-\beta}{\alpha}]$.  Show
that $g$ is Riemann integrable on $I$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to \R$ and $g \colon [0,1] \to \R$
are such that for all $x \in (0,1]$
we have $f(x) = g(x)$.  Suppose $f$ is Riemann integrable. 
Prove $g$ is Riemann integrable and $\int_{0}^1 f = \int_{0}^1 g$.
\end{exercise}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be a bounded function.
Let $P_n = \{ x_0,x_1,\ldots,x_n \}$ be a uniform partition of $[0,1]$,
that is, $x_j := \nicefrac{j}{n}$.  Is $\{ L(P_n,f) \}_{n=1}^\infty$
always monotone?  Yes/No: Prove or find a counterexample.
\end{exercise}

\begin{exercise}[Challenging]
For a bounded function $f \colon [0,1] \to \R$ let
$R_n := (\nicefrac{1}{n})\sum_{j=1}^n f(\nicefrac{j}{n})$ (the
uniform right hand rule).
a) If $f$ is Riemann integrable show $\int_0^1 f = \lim \, R_n$.
b) Find an $f$ that is not Riemann integrable, but $\lim \, R_n$ exists.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:riemannintdarboux}
Generalize the previous exercise.
Show that $f \in \sR[a,b]$ if and only if there exists an $I \in \R$,
such that for every $\epsilon > 0$ there exists
a $\delta > 0$ such that if $P$ is a partition with $\Delta x_i < \delta$
for all $i$, then
$\abs{L(P,f) - I} < \epsilon$ and
$\abs{U(P,f) - I} < \epsilon$.  If $f \in \sR[a,b]$, then $I = \int_a^b f$.
\end{exercise}

\begin{exercise}
Using \exerciseref{exercise:riemannintdarboux} and the idea of
the proof in \exerciseref{exercise:taggedpartition}, show that 
Darboux integral is the same as the standard definition
of Riemann integral, which you have most likely seen in calculus.  That is,
show that
$f \in \sR[a,b]$ if and only if there exists an $I \in \R$,
such that for every $\epsilon > 0$ there exists
a $\delta > 0$ such that if $P = \{ x_0,x_1,\ldots,x_n \}$
is a partition with $\Delta x_i < \delta$
for all $i$, then
$\abs{\sum_{i=1}^n f(c_i) \Delta x_i - I} < \epsilon$ for any set
$\{ c_1,c_2,\ldots,c_n \}$ with $c_i \in [x_{i-1},x_i]$.
If $f \in \sR[a,b]$, then $I = \int_a^b f$.
\end{exercise}


\begin{exercise}[Challenging]
Construct functions $f$ and $g$, 
where
$f \colon [0,1] \to \R$ is Riemann integrable,
$g \colon [0,1] \to [0,1]$ is one-to-one and onto,
and such that the composition $f \circ g$ is not Riemann integrable.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Properties of the integral}
\label{sec:rintprop}

\sectionnotes{2 lectures, integrability of functions with 
discontinuities can safely be skipped}

\subsection{Additivity}

The next result we prove is usually referred to as the
\myindex{additive property of the integral}.  First we prove the additivity
property for the lower and upper Darboux integrals.

\begin{lemma} \label{lemma:darbouxadd}
Suppose $a < b < c$ and $f \colon [a,c] \to \R$ is a bounded function.
Then
\begin{equation*}
\underline{\int_a^c} f
=
\underline{\int_a^b} f
+
\underline{\int_b^c} f
\end{equation*}
and
\begin{equation*}
\overline{\int_a^c} f
=
\overline{\int_a^b} f
+
\overline{\int_b^c} f .
\end{equation*}
\end{lemma}

\begin{proof}
If we have partitions $P_1 = \{ x_0,x_1,\ldots,x_k \}$
of $[a,b]$ and $P_2 = \{ x_k, x_{k+1}, \ldots, x_n \}$ of $[b,c]$,
then the set $P := P_1 \cup P_2 = \{ x_0, x_1, \ldots, x_n \}$ is
a partition of $[a,c]$.  Then
\begin{equation*}
L(P,f) =
\sum_{j=1}^n m_j \Delta x_j
=
\sum_{j=1}^k m_j \Delta x_j
+
\sum_{j=k+1}^n m_j \Delta x_j
=
L(P_1,f) + L(P_2,f) .
\end{equation*}
When we take the supremum of the right hand side over all $P_1$ and $P_2$,
we are taking a supremum of the left hand side
over all partitions $P$ of $[a,c]$ that contain $b$.  If $Q$ is any partition
of $[a,c]$ and $P = Q \cup \{ b \}$, then $P$ is a refinement of $Q$
and so $L(Q,f) \leq L(P,f)$.  Therefore, taking a supremum only over the $P$
that contain $b$ is sufficient to find the supremum of $L(P,f)$
over all partitions $P$, see \exerciseref{exercise:dominatingb}.
Finally recall \exerciseref{exercise:supofsum}
to compute
\begin{equation*}
\begin{split}
\underline{\int_a^c} f
& =
\sup \{ L(P,f) : \text{$P$ a partition of $[a,c]$} \}
\\
& =
\sup \{ L(P,f) : \text{$P$ a partition of $[a,c]$, $b \in P$} \}
\\
& =
\sup \{ L(P_1,f) + L(P_2,f) :
\text{$P_1$ a partition of $[a,b]$, $P_2$ a partition of $[b,c]$} \}
\\
& =
\sup \{ L(P_1,f) : \text{$P_1$ a partition of $[a,b]$} \}
+
\sup \{ L(P_2,f) : \text{$P_2$ a partition of $[b,c]$} \}
\\
&=
\underline{\int_a^b} f + \underline{\int_b^c} f .
\end{split}
\end{equation*}

Similarly, for $P$, $P_1$, and $P_2$ as above we obtain
\begin{equation*}
U(P,f) =
\sum_{j=1}^n M_j \Delta x_j
=
\sum_{j=1}^k M_j \Delta x_j
+
\sum_{j=k+1}^n M_j \Delta x_j
=
U(P_1,f) + U(P_2,f) .
\end{equation*}
We wish to take the infimum on the right
over all $P_1$ and $P_2$, and so we are taking the infimum
over all partitions $P$ of $[a,c]$ that contain $b$.  If $Q$ is any partition
of $[a,c]$ and $P = Q \cup \{ b \}$, then $P$ is a refinement of $Q$
and so $U(Q,f) \geq U(P,f)$.  Therefore, taking an infimum only over the $P$
that contain $b$ is sufficient to find the infimum of $U(P,f)$ for
all $P$.
We obtain
\begin{equation*}
\overline{\int_a^c} f
=
\overline{\int_a^b} f + \overline{\int_b^c} f .  \qedhere
\end{equation*}
\end{proof}

\begin{prop}
Let $a < b < c$.  A function $f \colon [a,c] \to \R$ is Riemann integrable
if and only if $f$ is Riemann integrable on $[a,b]$ and $[b,c]$.  If
$f$ is Riemann integrable, then
\begin{equation*}
\int_a^c f
=
\int_a^b f
+
\int_b^c f .
\end{equation*}
\end{prop}

\begin{proof}
Suppose $f \in \sR[a,c]$, then 
$\overline{\int_a^c} f = 
\underline{\int_a^c} f = 
\int_a^c f$.  We apply the lemma to get
\begin{equation*}
\int_a^c f
=
\underline{\int_a^c} f
 =
\underline{\int_a^b} f + \underline{\int_b^c} f
 \leq
\overline{\int_a^b} f + \overline{\int_b^c} f
 =
\overline{\int_a^c} f
 =
\int_a^c f .
\end{equation*}
Thus the inequality is an equality and
\begin{equation*}
\underline{\int_a^b} f + \underline{\int_b^c} f
=
\overline{\int_a^b} f + \overline{\int_b^c} f .
\end{equation*}
As we also know 
$\underline{\int_a^b} f \leq \overline{\int_a^b} f$
and
$\underline{\int_b^c} f \leq \overline{\int_b^c} f$, we 
conclude 
\begin{equation*}
\underline{\int_a^b} f
=
\overline{\int_a^b} f
\qquad \text{and} \qquad
\underline{\int_b^c} f
=
\overline{\int_b^c} f .
\end{equation*}
Thus $f$ is Riemann integrable on $[a,b]$ and $[b,c]$ and the desired formula
holds.

Now assume the restrictions of $f$ to $[a,b]$ and to $[b,c]$
are Riemann integrable.  We again apply the lemma to get
\begin{equation*}
\underline{\int_a^c} f
=
\underline{\int_a^b} f + \underline{\int_b^c} f
=
\int_a^b f + \int_b^c f
=
\overline{\int_a^b} f + \overline{\int_b^c} f
=
\overline{\int_a^c} f .
\end{equation*}
Therefore $f$ is Riemann integrable on $[a,c]$, and the integral is computed
as indicated.
\end{proof}

An easy consequence of the additivity is the following corollary.  We
leave the details to the reader as an exercise.

\begin{cor} \label{intsubcor}
If $f \in \sR[a,b]$ and
$[c,d] \subset [a,b]$, then
the restriction $f|_{[c,d]}$ is in $\sR[c,d]$.
\end{cor}

\subsection{Linearity and monotonicity}

\begin{prop}[Linearity]
\index{linearity of the integral}
Let $f$ and $g$ be in $\sR[a,b]$ and $\alpha \in \R$.
\begin{enumerate}[(i)]
\item $\alpha f$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \alpha f(x) ~dx = \alpha \int_a^b f(x) ~dx .
\end{equation*}
\item $f+g$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \bigl( f(x)+g(x) \bigr) ~dx = 
\int_a^b f(x) ~dx 
+
\int_a^b g(x) ~dx .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us prove the first item for $\alpha \geq 0$. 
Let $P$ be a partition of $[a,b]$.
Let $m_i := \inf \{ f(x) : x \in [x_{i-1},x_i] \}$ as usual.
Since $\alpha$ is nonnegative, we can move the multiplication by $\alpha$
past the infimum,
\begin{equation*}
\inf \{ \alpha f(x) : x \in [x_{i-1},x_i] \}
=
\alpha \inf \{ f(x) : x \in [x_{i-1},x_i] \} = \alpha m_i .
\end{equation*}
Therefore
\begin{equation*}
L(P,\alpha f) =
\sum_{i=1}^n \alpha m_i \Delta_i = \alpha \sum_{i=1}^n m_i \Delta_i = \alpha
L(P,f).
\end{equation*}
Similarly 
\begin{equation*}
U(P,\alpha f) = \alpha U(P,f) .
\end{equation*}
Again, as $\alpha \geq 0$ we
may move multiplication by $\alpha$ past the supremum.  Hence,
\begin{equation*}
\begin{split}
\underline{\int_a^b} \alpha f(x)~dx & =
\sup \{ L(P,\alpha f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\sup \{ \alpha L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\alpha
\sup \{ L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\alpha
\underline{\int_a^b} f(x)~dx .
\end{split}
\end{equation*}
Similarly we show 
\begin{equation*}
\overline{\int_a^b} \alpha f(x)~dx
=
\alpha
\overline{\int_a^b} f(x)~dx .
\end{equation*}
The conclusion now follows for $\alpha \geq 0$.

To finish the proof of the first item, we need to show 
that $-f$ is Riemann integrable and
$\int_a^b - f(x)~dx =
-
\int_a^b f(x)~dx$.  The proof of this fact is left as an exercise.

The proof of the second item in the proposition is also left as an exercise.
It is not as
trivial as it may appear at first glance.
\end{proof}

The second item in the proposition does not hold with
equality for the Darboux integrals, but we do obtain inequalities.

\begin{prop} \label{prop:upperlowerlinineq}
Let $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$ be bounded
functions.  Then
\begin{equation*}
%\overline{\int_a^b} \bigl(f(x)+g(x)\bigr)~dx \leq
%\overline{\int_a^b}f(x)~dx+\overline{\int_a^b}g(x)~dx
\overline{\int_a^b} (f+g) \leq \overline{\int_a^b}f+\overline{\int_a^b}g
,
\qquad
\text{and}
\qquad
\underline{\int_a^b} (f+g) \geq \underline{\int_a^b}f+\underline{\int_a^b}g
%\underline{\int_a^b} \bigl(f(x)+g(x)\bigr)~dx \geq
%\underline{\int_a^b}f(x)~dx+\underline{\int_a^b}g(x)~dx
.
\end{equation*}
\end{prop}

The proof of the above proposition is \exerciseref{exercise:upperlowerlinineq}.
It follows as supremum of a sum is less than or equal to the sum of
suprema and similarly for infima, see \exerciseref{exercise:sumofsup}.

\begin{prop}[Monotonicity]
\index{monotonicity of the integral}
Let $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$ be
bounded, and $f(x) \leq g(x)$
for all $x \in [a,b]$.  Then
\begin{equation*}
\underline{\int_a^b} f 
\leq
\underline{\int_a^b} g 
\qquad \text{and} \qquad
\overline{\int_a^b} f 
\leq
\overline{\int_a^b} g .
\end{equation*}
Moreover, if $f$ and $g$ are in $\sR[a,b]$, then
\begin{equation*}
\int_a^b f 
\leq
\int_a^b g .
\end{equation*}
\end{prop}

\begin{proof}
Let $P = \{ x_0, x_1, \ldots, x_n \}$ be a partition of $[a,b]$.  Then
let
\begin{equation*}
m_i := \inf \{ f(x) : x \in [x_{i-1},x_i] \}
\qquad \text{and} \qquad
\widetilde{m}_i := \inf \{ g(x) : x \in [x_{i-1},x_i] \} .
\end{equation*}
As $f(x) \leq g(x)$, then $m_i \leq \widetilde{m}_i$.
Therefore,
\begin{equation*}
L(P,f)
=
\sum_{i=1}^n m_i \Delta x_i
\leq
\sum_{i=1}^n \widetilde{m}_i \Delta x_i
=
L(P,g) .
\end{equation*}
We take the supremum over all $P$ (see \propref{prop:funcsupinf}) to obtain 
\begin{equation*}
\underline{\int_a^b} f 
\leq
\underline{\int_a^b} g .
\end{equation*}
Similarly we obtain the same conclusion for the upper integrals.
Finally,
if $f$ and $g$ are Riemann integrable all the integrals are equal,
and the conclusion follows.
\end{proof}

\subsection{Continuous functions}

Let us show that continuous functions are Riemann integrable.  In fact we
will show we can even allow some discontinuities.  We start with a
function continuous on the whole closed interval $[a,b]$.

\begin{lemma} \label{lemma:contint}
If $f \colon [a,b] \to \R$ is a continuous function,
then $f \in \sR[a,b]$.
\end{lemma}

\begin{proof}
As $f$ is continuous on a closed bounded interval, it is
uniformly continuous.
Let $\epsilon > 0$ be given.  Find a $\delta > 0$ such that
$\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} < \frac{\epsilon}{b-a}$.

Let $P = \{ x_0, x_1, \ldots, x_n \}$
be a partition of $[a,b]$ such that $\Delta x_i < \delta$ for all $i = 1,2,
\ldots, n$.  For example,
take $n$ such that $\frac{b-a}{n} < \delta$ and
let $x_i := \frac{i}{n}(b-a) + a$.
Then for all $x, y \in [x_{i-1},x_i]$ we have 
$\abs{x-y} \leq \Delta x_i < \delta$ and so
\begin{equation*}
f(x)-f(y) \leq \abs{f(x)-f(y)} < \frac{\epsilon}{b-a} .
\end{equation*}
As $f$ is continuous on $[x_{i-1},x_i]$, it attains a maximum and a minimum
on this interval.
Let $x$ be a point where $f$ attains the maximum and $y$ be a point
where $f$ attains the minimum.  Then $f(x) = M_i$
and $f(y) = m_i$ in the notation from the definition of the integral.
Therefore,
\begin{equation*}
M_i-m_i = f(x)-f(y) < 
\frac{\epsilon}{b-a} .
\end{equation*}
And so
\begin{equation*}
\begin{split}
\overline{\int_a^b} f - 
\underline{\int_a^b} f 
& \leq
U(P,f) - L(P,f)
\\
& =
\left(
\sum_{i=1}^n
M_i \Delta x_i
\right)
-
\left(
\sum_{i=1}^n
m_i \Delta x_i
\right)
\\
& =
\sum_{i=1}^n
(M_i-m_i) \Delta x_i
\\
& <
\frac{\epsilon}{b-a}
\sum_{i=1}^n
\Delta x_i
\\
& =
\frac{\epsilon}{b-a} (b-a)
= \epsilon .
\end{split}
\end{equation*}
As $\epsilon > 0$ was arbitrary,
\begin{equation*}
\overline{\int_a^b} f = \underline{\int_a^b} f ,
\end{equation*}
and $f$ is Riemann integrable on $[a,b]$.
\end{proof}

The second lemma says that we need the function to only be ``Riemann integrable
inside the interval,'' as long as it is bounded.  It also tells us how to
compute the integral.

\begin{lemma} \label{lemma:boundedimpriemann}
Let $f \colon [a,b] \to \R$ be a bounded function,
$\{ a_n \}$ and $\{b_n \}$ be sequences such that
$a < a_n < b_n < b$ for all $n$, with
$\lim \, a_n = a$ and $\lim \, b_n = b$.
Suppose $f \in \sR[a_n,b_n]$ for all $n$.
Then $f \in \sR[a,b]$ and
\begin{equation*}
\int_a^b f = 
\lim_{n \to \infty} \int_{a_n}^{b_n} f .
\end{equation*}
\end{lemma}

\begin{proof}
Let $M > 0$ be a real number such that $\abs{f(x)} \leq M$.
%Pick two
%sequences of numbers $a < a_n < b_n < b$ such that $\lim\, a_n = a$
%and $\lim\, b_n = b$.
As $(b-a) \geq (b_n-a_n)$,
\begin{equation*}
-M(b-a) \leq
-M(b_n-a_n) \leq
\int_{a_n}^{b_n} f
\leq
M(b_n-a_n) \leq
M(b-a) .
\end{equation*}
Therefore the sequence of numbers
$\{ \int_{a_n}^{b_n} f \}_{n=1}^\infty$ is bounded and by
\hyperref[thm:bwseq]{Bolzano--Weierstrass}
has a convergent subsequence indexed by $n_k$.  Let us call
$L$ the limit of the subsequence
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}_{k=1}^\infty$.

\lemmaref{lemma:darbouxadd} says that
the lower and upper integral are additive
and the hypothesis says that
$f$ is integrable on $[a_{n_k},b_{n_k}]$.
Therefore
\begin{equation*}
\underline{\int_a^b} f
=
\underline{\int_a^{a_{n_k}}} f
+
\int_{a_{n_k}}^{b_{n_k}} f
+
\underline{\int_{b_{n_k}}^b} f
\geq
-M(a_{n_k}-a)
+
\int_{a_{n_k}}^{b_{n_k}} f
-
M(b-b_{n_k}) .
\end{equation*}
We take the limit as $k$ goes to $\infty$ on the right-hand side,
\begin{equation*}
\underline{\int_a^b} f
\geq
-M\cdot 0
+
L
-
M\cdot 0
= L .
\end{equation*}

Next we use additivity of the upper integral,
\begin{equation*}
\overline{\int_a^b} f
=
\overline{\int_a^{a_{n_k}}} f
+
\int_{a_{n_k}}^{b_{n_k}} f
+
\overline{\int_{b_{n_k}}^b} f
\leq
M(a_{n_k}-a)
+
\int_{a_{n_k}}^{b_{n_k}} f
+
M(b-b_{n_k}) .
\end{equation*}
We take the same subsequence 
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}_{k=1}^\infty$ and take the limit 
to obtain
\begin{equation*}
\overline{\int_a^b} f
\leq
M\cdot 0
+
L
+
M\cdot 0
= L .
\end{equation*}
Thus $\overline{\int_a^b} f = \underline{\int_a^b} f = L$
and hence $f$ is Riemann integrable and $\int_a^b f = L$.
In particular, no matter what
%sequences $\{ a_n \}$ and $\{b_n\}$ we started with and what
subsequence we chose,
the $L$ is the same number.

To prove the final statement of the lemma we use 
\propref{seqconvsubseqconv:prop}.  We have shown that every convergent
subsequence
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}$ converges to $L = \int_a^b f$.
Therefore, the sequence
$\{ \int_{a_n}^{b_n} f \}$ is convergent and converges to $\int_a^b f$.
\end{proof}

We say a function $f \colon [a,b] \to \R$ has \emph{\myindex{finitely many
discontinuities}} if there exists a finite set $S := \{ x_1, x_2, \ldots, x_n \}
\subset [a,b]$, and $f$ is continuous
at all points of $[a,b] \setminus S$.

\begin{thm}
Let $f \colon [a,b] \to \R$ be a bounded function with finitely
many discontinuities.  Then $f \in \sR[a,b]$.
\end{thm}

\begin{proof}
We divide the interval into finitely many intervals $[a_i,b_i]$
so that $f$ is continuous
on the interior $(a_i,b_i)$.  If $f$ is continuous on $(a_i,b_i)$,
then it is continuous and hence integrable on $[c_i,d_i]$ whenever $a_i < c_i < d_i < b_i$.  By
\lemmaref{lemma:boundedimpriemann}
the restriction
of $f$ to $[a_i,b_i]$ is integrable.  By additivity of the integral (and
\hyperref[induction:thm]{induction}) $f$ is integrable on the union of the intervals.
\end{proof}

\subsection{More on integrable functions}

Sometimes it is convenient (or necessary)
to change certain values of a function and
then integrate.  The next result says
that if we change the values only at finitely
many points, the integral does not change.

\begin{prop}
Let $f \colon [a,b] \to \R$ be Riemann integrable.  Let $g \colon [a,b] \to
\R$ be a function such that $f(x) = g(x)$ for all $x \in [a,b] \setminus S$,
where $S$ is a finite set.  Then $g$ is a Riemann integrable function
and
\begin{equation*}
\int_a^b g = \int_a^b f.
\end{equation*}
\end{prop}

\begin{proof}[Sketch of proof]
Using additivity of the integral, we split up the interval $[a,b]$ into
smaller intervals such that $f(x) = g(x)$ holds for all $x$ except at the
endpoints (details are left to the reader).

Therefore, without loss of generality suppose $f(x) = g(x)$ for
all $x \in (a,b)$.  The proof follows by \lemmaref{lemma:boundedimpriemann},
and is left as an exercise.
\end{proof}

Finally, monotone (increasing or decreasing) functions are always
Riemann integrable.  The proof is left to the reader as
\exerciseref{exercise:boundedvariationintegrable}.

\begin{prop} \label{prop:monotoneintegrable}
Let $f \colon [a,b] \to \R$ be a monotone function.  Then $f \in \sR[a,b]$.
\end{prop}

\subsection{Exercises}

\begin{exercise}
Let $f$ be in $\sR[a,b]$.  Prove that
$-f$ is in $\sR[a,b]$ and 
\begin{equation*}
\int_a^b - f(x) ~dx = - \int_a^b f(x) ~dx .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f$ and $g$ be in $\sR[a,b]$.
Prove, without using \propref{prop:upperlowerlinineq}, that $f+g$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \bigl( f(x)+g(x) \bigr) ~dx = 
\int_a^b f(x) ~dx 
+
\int_a^b g(x) ~dx .
\end{equation*}
Hint: One way to do it is to use \propref{prop:refinement} to find a single partition $P$
such that $U(P,f)-L(P,f) < \nicefrac{\epsilon}{2}$ and
$U(P,g)-L(P,g) < \nicefrac{\epsilon}{2}$.
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be Riemann integrable.  Let $g \colon [a,b] \to
\R$ be a function such that $f(x) = g(x)$ for all $x \in (a,b)$.
Prove that $g$ is Riemann integrable and that
\begin{equation*}
\int_a^b g = \int_a^b f.
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{mean value theorem for integrals}}.  That is,
prove that if $f \colon [a,b] \to \R$ is continuous, then there exists
a $c \in [a,b]$ such that $\int_a^b f = f(c)(b-a)$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ is a continuous function such that $f(x) \geq 0$
for all $x \in [a,b]$ and $\int_a^b f = 0$.  Prove that $f(x) = 0$
for all $x$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ is a continuous function
for all $x \in [a,b]$ and $\int_a^b f = 0$.  Prove that
there exists a $c \in [a,b]$ such that $f(c) = 0$ (Compare with the
previous exercise).
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$
are continuous functions such that $\int_a^b f = \int_a^b g$.
Then show that there exists a $c \in [a,b]$ such that $f(c) = g(c)$.
\end{exercise}

\begin{exercise}
Let $f \in \sR[a,b]$.  Let $\alpha, \beta, \gamma$ be arbitrary numbers in
$[a,b]$ (not necessarily ordered in any way).  Prove 
\begin{equation*}
\int_\alpha^\gamma f =
\int_\alpha^\beta f +
\int_\beta^\gamma f .
\end{equation*}
Recall what $\int_a^b f$ means if $b \leq a$.
\end{exercise}

\begin{exercise}
Prove \corref{intsubcor}.
\end{exercise}

\begin{exercise} \label{exercise:easyabsint}
Suppose $f \colon [a,b] \to \R$ is bounded and
has finitely many discontinuities.
Show that as a function of $x$ the expression $\abs{f(x)}$ is bounded with finitely many
discontinuities and is thus Riemann integrable.  Then show 
\begin{equation*}
\abs{\int_a^b f(x)~dx} \leq \int_a^b \abs{f(x)}~dx .
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Show that the
Thomae\index{Thomae function} or \myindex{popcorn function}
(see \exampleref{popcornfunction:example})
is Riemann integrable.  Therefore, there exists a
function discontinuous at all rational numbers (a dense set)
that is Riemann integrable.

In particular,
define $f \colon [0,1] \to \R$ by
\begin{equation*}
f(x) := 
\begin{cases}
\nicefrac{1}{k} & \text{ if $x=\nicefrac{m}{k}$ where $m,k \in \N$
and $m$ and $k$ have no common divisors,} \\
0 & \text{ if $x$ is irrational}.
\end{cases}
\end{equation*}
Show $\int_0^1 f = 0$.
\end{exercise}


\begin{exnote}
If $I \subset \R$ is a bounded interval, then
the function
\begin{equation*}
\varphi_I(x) :=
\begin{cases}
1 & \text{if $x \in I$,} \\
0 & \text{otherwise,}
\end{cases}
\end{equation*}
is called an \emph{\myindex{elementary step function}}.
\end{exnote}

\begin{exercise}
Let $I$ be an arbitrary bounded interval (you should consider all types
of intervals: closed, open, half-open) and $a < b$, then
using only the definition of the integral
show that
the elementary step function $\varphi_I$ is integrable
on $[a,b]$, and find the integral in terms of $a$, $b$, and the
endpoints of $I$.
\end{exercise}

\begin{exnote}
When a function $f$ can be written as
\begin{equation*}
f(x) = \sum_{k=1}^n \alpha_k \varphi_{I_k} (x)
\end{equation*}
for some real numbers $\alpha_1,\alpha_2, \ldots, \alpha_n$
and some bounded intervals $I_1,I_2,\ldots,I_n$, then 
$f$ is called a \emph{\myindex{step function}}.
\end{exnote}

\begin{exercise}
Using the previous exercise, show that a step function is integrable
on any interval $[a,b]$.  Furthermore, find the integral in terms of
$a$, $b$, the endpoints of $I_k$ and the $\alpha_k$.
\end{exercise}

\begin{exercise}
\label{exercise:boundedvariationintegrable}
Let $f \colon [a,b] \to \R$ be increasing.  a) Show that $f$ is Riemann
integrable.  Hint: Use a uniform partition; each subinterval of same length.
b) Use part a to show that a decreasing function is Riemann
integrable.  c)  Suppose $h = f-g$ where $f$ and $g$ are increasing
functions on $[a,b]$.  Show that $h$ is Riemann integrable%
\footnote{Such an $h$ is said to be of \emph{\myindex{bounded variation}}.}.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:hardabsint}
Suppose $f \in \sR[a,b]$, then the function that takes $x$ to
$\abs{f(x)}$ is also Riemann integrable on $[a,b]$.
Then show the same inequality as \exerciseref{exercise:easyabsint}.
\end{exercise}

\begin{exercise}  \label{exercise:upperlowerlinineq}
Suppose $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$
are bounded. a)~Show
$\overline{\int_a^b} (f+g) \leq \overline{\int_a^b}f+\overline{\int_a^b}g$ and
$\underline{\int_a^b} (f+g) \geq
\underline{\int_a^b}f+\underline{\int_a^b}g$.  b)~Find example $f$ and $g$ where
the inequality is strict.  Hint: $f$ and $g$ should not be Riemann
integrable.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is continuous and $g \colon \R \to \R$ is
Lipschitz continuous.  Define
\begin{equation*}
h(x) := \int_a^b g(t-x) f(t) \, dt .
\end{equation*}
Prove that $h$ is Lipschitz continuous.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Fundamental theorem of calculus}
\label{sec:ftc}

\sectionnotes{1.5 lectures}

In this chapter we discuss and prove the
\emph{\myindex{fundamental theorem of calculus}}.
The entirety of integral calculus is built upon this theorem,
ergo the name.
The theorem relates the seemingly unrelated concepts of integral and
derivative.  It tells us how to compute the antiderivative of a function
using the integral and vice-versa.

\subsection{First form of the theorem}

\begin{thm}
Let $F \colon [a,b] \to \R$ be a continuous function, differentiable
on $(a,b)$.  Let $f \in \sR[a,b]$ be such that $f(x) = F'(x)$ for $x \in
(a,b)$.  Then
\begin{equation*}
\int_a^b f = F(b)-F(a) .
\end{equation*}
\end{thm}

It is not hard to generalize the theorem to allow a finite number of points
in $[a,b]$ where $F$ is not differentiable, as long as it is continuous.
This generalization is left as an exercise.

\begin{proof}
Let $P = \{ x_0, x_1, \ldots, x_n \}$ be a partition of $[a,b]$.
For each interval $[x_{i-1},x_i]$, use the
\hyperref[thm:mvt]{mean value theorem} to find a
$c_i \in (x_{i-1},x_i)$ such that
\begin{equation*}
f(c_i) \Delta x_i = F'(c_i) (x_i - x_{i-1}) = F(x_i) - F(x_{i-1}) .
\end{equation*}
See \figureref{fig:fundthmfig}, and
notice that the area of all
three shaded rectangles is $F(x_{i+1})-F(x_{i-2})$.
The idea is that by picking small enough subintervals
we prove that this area is the integral of $f$.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{fundthmfig.eepic}
\caption{Mean value theorem on subintervals of a partition
approximating area under the curve.\label{fig:fundthmfig}}
%\end{center}
\end{myfigureht}

Using the notation from the definition of the integral, we have
$m_i \leq f(c_i) \leq M_i$ and so
\begin{equation*}
m_i \Delta x_i \leq F(x_i) - F(x_{i-1}) \leq M_i \Delta x_i .
\end{equation*}
We sum over $i = 1,2, \ldots, n$ to get
\begin{equation*}
\sum_{i=1}^n m_i \Delta x_i
\leq \sum_{i=1}^n \bigl(F(x_i) - F(x_{i-1}) \bigr)
\leq \sum_{i=1}^n M_i \Delta x_i .
\end{equation*}
In the middle sum, all the terms except the first and last cancel 
and we end up with $F(x_n)-F(x_0) = F(b)-F(a)$.  The sums on the left
and on the right are the lower and the upper sum respectively.  So
\begin{equation*}
L(P,f) \leq F(b)-F(a) \leq U(P,f) .
\end{equation*}
We take the supremum of $L(P,f)$ over all partitions $P$ and the left inequality
yields 
\begin{equation*}
\underline{\int_a^b} f \leq F(b)-F(a) .
\end{equation*}
Similarly, taking
the infimum of $U(P,f)$ over all partitions $P$ yields
\begin{equation*}
F(b)-F(a) \leq \overline{\int_a^b} f .
\end{equation*}
As $f$ is Riemann integrable, we have
\begin{equation*}
\int_a^b f =
\underline{\int_a^b} f \leq F(b)-F(a) \leq \overline{\int_a^b} f
= \int_a^b f .
\end{equation*}
The inequalities must be equalities and we are done.
\end{proof}

The theorem is used to compute integrals.  Suppose we know that
the function $f(x)$ is a derivative of some other function $F(x)$,
then we can find an explicit expression for $\int_a^b f$. 

\begin{example}
Suppose we are trying to compute
\begin{equation*}
\int_0^1 x^2 ~dx .
\end{equation*}
We notice $x^2$ is the derivative of $\frac{x^3}{3}$.  We
use the fundamental theorem to write 
\begin{equation*}
\int_0^1 x^2 ~dx =
\frac{1^3}{3}
-
\frac{0^3}{3}
= \frac{1}{3}.
\end{equation*}
\end{example}

\subsection{Second form of the theorem}

The second form of the fundamental theorem gives us a way to solve
the differential equation $F'(x) = f(x)$, where $f$ is a known
function and we are trying to find an $F$ that satisfies the equation.

\begin{thm}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.  Define
\begin{equation*}
F(x) := \int_a^x f .
\end{equation*}
First, $F$ is continuous on $[a,b]$.  Second,
if $f$ is continuous at $c \in [a,b]$, then $F$ is differentiable at $c$
and $F'(c) = f(c)$.
\end{thm}

\begin{proof}
As $f$ is bounded, there is an $M > 0$
such that $\abs{f(x)} \leq M$ for all $x \in [a,b]$.  Suppose $x,y \in [a,b]$
with $x > y$.  Then
%Then using
%an exercise from an earlier section we note
\begin{equation*}
\abs{F(x)-F(y)} =
\abs{\int_a^x f - \int_a^y f}
=
\abs{\int_y^x f}
\leq
M\abs{x-y} .
\end{equation*}
By symmetry, the same also holds if $x < y$.
So $F$ is Lipschitz continuous and hence continuous.

Now suppose $f$ is continuous at $c$.
Let $\epsilon > 0$ be given.  Let $\delta > 0$ be such that
for $x \in [a,b]$,
$\abs{x-c} < \delta$ implies $\abs{f(x)-f(c)} < \epsilon$.
In particular,
for such $x$ we have
\begin{equation*}
f(c)-\epsilon \leq f(x) \leq f(c) + \epsilon.
\end{equation*}
Thus if $x > c$, then
\begin{equation*}
\bigl(f(c)-\epsilon\bigr) (x-c) \leq \int_c^x f \leq
\bigl(f(c) + \epsilon\bigr)(x-c).
\end{equation*}
When $c > x$, then the inequalities are reversed.  Therefore,
assuming $c \not= x$ we get
\begin{equation*}
f(c)-\epsilon
\leq
\frac{\int_c^{x} f}{x-c}
\leq
f(c)+\epsilon .
\end{equation*}
As 
\begin{equation*}
\frac{F(x)-F(c)}{x-c}
=
\frac{\int_a^{x} f - \int_a^{c} f}{x-c}
=
\frac{\int_c^{x} f}{x-c} ,
\end{equation*}
we have 
\begin{equation*}
\abs{\frac{F(x)-F(c)}{x-c} - f(c)} \leq \epsilon .
\end{equation*}
The result follows.  It is left to the reader to see why is it OK that we
just have a non-strict inequality.
\end{proof}

Of course, if $f$ is continuous on $[a,b]$, then it is automatically Riemann
integrable, $F$ is differentiable on all of $[a,b]$ and $F'(x) = f(x)$ for
all $x \in [a,b]$.

\begin{remark} \label{remark:fundthmbase}
The second form of the fundamental theorem of calculus still holds if
we let $d \in [a,b]$ and define
\begin{equation*}
F(x) := \int_d^x f .
\end{equation*}
That is, we can use any point of $[a,b]$ as our base point.  The proof is
left as an exercise.
\end{remark}

Let us look at what a simple discontinuity can do.  Take $f(x) := -1$ if $x
< 0$, and $f(x) := 1$ if $x \geq 0$.  Let $F(x) := \int_0^x f$.  It is not
difficult to see that $F(x) = \abs{x}$.  Notice that $f$ is discontinuous at
$0$ and $F$ is not differentiable at $0$.  However, the converse does not
hold.  %Let us do another quick example.
Let $g(x) := 0$ if $x \not= 0$, and $g(0) = 1$.  Letting $G(x) :=
\int_0^x g$, we find that $G(x) = 0$ for all $x$.  So $g$ is discontinuous
at $0$, but $G'(0)$ exists and is equal to 0.

A common misunderstanding of the integral for calculus students is to
think of integrals whose solution cannot be given in closed-form as somehow
deficient.  This is not the case.  Most integrals we write down are not
computable in closed-form.  Even some integrals that we consider
in closed-form are not really such.  For example, how does a computer find
the value of $\ln x$?  One way to do it is to note that
we define the natural log as the antiderivative of $\nicefrac{1}{x}$
such that $\ln 1 = 0$.
Therefore,
\begin{equation*}
\ln x := \int_1^x \nicefrac{1}{s}~ds .
\end{equation*}
Then we can numerically approximate the integral.  Morally,
we did not really ``simplify'' $\int_1^x \nicefrac{1}{s}~ds$ by
writing down $\ln x$.  We simply gave the integral a name.
If we require numerical answers,
it is possible we end up doing
the calculation by approximating an integral anyway.
In the next section, we even define the exponential using
the logarithm, which we define in terms of the integral.

Another common function defined by an integral that cannot
be evaluated symbolically
is the erf function, defined as
\begin{equation*}
\operatorname{erf}(x) := \frac{2}{\sqrt{\pi}} \int_0^x e^{-s^2} ~ds .
\end{equation*}
This function comes up often in applied mathematics.  It is simply 
the antiderivative of $\left(\nicefrac{2}{\sqrt{\pi}}\right) e^{-x^2}$
that is zero at zero.
The second form of the fundamental theorem tells us that we can write the function
as an integral.  If we wish to compute any particular value, we 
numerically approximate the integral.

\subsection{Change of variables}

A theorem often used in calculus to solve integrals is the change of
variables theorem.  Let us prove it now.  Recall 
a function is continuously differentiable if
it is differentiable and the derivative is continuous.

\begin{thm}[Change of variables]
\index{change of variables theorem}
Let $g \colon [a,b] \to \R$ be a continuously differentiable function,
let $f \colon [c,d] \to \R$ be continuous, and suppose
$g([a,b]) \subset [c,d]$.  Then
\begin{equation*}
\int_a^b f\bigl(g(x)\bigr)\, g'(x)~ dx =
\int_{g(a)}^{g(b)} f(s)~ ds .
\end{equation*}
\end{thm}

\begin{proof}
As $g$, $g'$, and $f$ are continuous, we know $f\bigl(g(x)\bigr)\,g'(x)$
is a continuous function on $[a,b]$, therefore it is Riemann integrable.
Similarly $f$ is integrable on any subinterval of $[c,d]$.

Define 
\begin{equation*}
F(y) := \int_{g(a)}^{y} f(s)~ds .
\end{equation*}
By the second form of the fundamental
theorem of calculus (see \remarkref{remark:fundthmbase} and \exerciseref{secondftc:exercise})
$F$ is a differentiable function and $F'(y) = f(y)$.  We apply the chain
rule and write
\begin{equation*}
\bigl( F \circ g \bigr)' (x) =
F'\bigl(g(x)\bigr) g'(x)
=
f\bigl(g(x)\bigr) g'(x) .
\end{equation*}
We note that $F\bigl(g(a)\bigr) = 0$ and we
use the first form of the fundamental theorem
to obtain
\begin{equation*}
\int_{g(a)}^{g(b)} f(s)~ds = F\bigl(g(b)\bigr) = F\bigl(g(b)\bigr)-F\bigl(g(a)\bigr)
=
\int_a^b 
\bigl( F \circ g \bigr)' (x) ~dx
=
\int_a^b 
f\bigl(g(x)\bigr) g'(x)
~dx .  %FIXME \qedhere
\end{equation*}
\end{proof}

The change of variables theorem is often used to solve integrals by changing them
to integrals that we know or that we can solve using the fundamental theorem of
calculus.

\begin{example}
From an exercise, we know that the derivative of $\sin(x)$ is $\cos(x)$.
Therefore we solve
\begin{equation*}
\int_0^{\sqrt{\pi}} x \cos(x^2) ~ dx = \int_0^\pi \frac{\cos(s)}{2} ~ ds
=
\frac{1}{2}
\int_0^\pi \cos(s) ~ ds
=
\frac{
\sin(\pi) - \sin(0)
}{2}
=
0 .
\end{equation*}
\end{example}

However, beware that we must satisfy the hypotheses of the theorem.  The
following example demonstrates why we should not just 
move symbols around mindlessly.
We must be careful that those symbols really make sense.

\begin{example}
Suppose we write down
\begin{equation*}
\int_{-1}^{1} \frac{\ln \abs{x}}{x} ~dx .
\end{equation*}
It may be tempting to take $g(x) := \ln \abs{x}$.  Then take $g'(x) =
\frac{1}{x}$ and try to write
\begin{equation*}
\int_{g(-1)}^{g(1)} s ~ds = 
\int_{0}^{0} s ~ds = 0. 
\end{equation*}
This ``solution'' is incorrect, and it does not say
that we can solve the given integral.  First problem is that
$\frac{\ln \abs{x}}{x}$ is not continuous on $[-1,1]$.
Second, $\frac{\ln \abs{x}}{x}$ is not even Riemann integrable on $[-1,1]$
(it is unbounded).
The integral we wrote down simply does not make sense.
Finally, $g$ is not even continuous 
on $[-1,1]$, let alone continuously differentiable.
\end{example}

\subsection{Exercises}

\begin{exercise}
Compute
$\displaystyle
\frac{d}{dx} \biggl( \int_{-x}^x e^{s^2}~ds \biggr)$.
\end{exercise}

\begin{exercise}
Compute
$\displaystyle
\frac{d}{dx} \biggl( \int_{0}^{x^2} \sin(s^2)~ds \biggr)$.
\end{exercise}

\begin{exercise}
Suppose $F \colon [a,b] \to \R$ is continuous and differentiable
on $[a,b] \setminus S$, where $S$ is a finite set.  Suppose there
exists an $f \in \sR[a,b]$ such that $f(x) = F'(x)$ for $x \in [a,b]
\setminus S$.  Show that
$\int_a^b f = F(b)-F(a)$.
\end{exercise}

\begin{exercise} \label{secondftc:exercise}
Let $f \colon [a,b] \to \R$ be a continuous function.  Let $c \in [a,b]$
be arbitrary.  Define
\begin{equation*}
F(x) := \int_c^x f .
\end{equation*}
Prove that $F$ is differentiable and that $F'(x) = f(x)$ for all $x \in
[a,b]$.
\end{exercise}

\begin{exercise}
Prove \emph{\myindex{integration by parts}}.  That is, suppose $F$ and
$G$ are continuously differentiable functions on $[a,b]$.  Then prove
\begin{equation*}
\int_a^b F(x)G'(x)~dx
=
F(b)G(b)-F(a)G(a)
-
\int_a^b F'(x)G(x)~dx .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $F$ and $G$ are
continuously\footnote{
Compare this hypothesis to \exerciseref{exercise:samediffconst}.}
differentiable
functions defined on $[a,b]$
such that $F'(x) = G'(x)$ for all $x \in [a,b]$.
Using the fundamental theorem of calculus,
show that $F$ and $G$ differ by a constant.  That is, show that
there exists a $C \in \R$ such that
$F(x)-G(x) = C$.
\end{exercise}

\begin{exnote}
The next exercise shows how we can use the integral to ``smooth out'' a
non-differentiable function.
\end{exnote}

\begin{exercise} \label{exercise:smoothingout}
Let $f \colon [a,b] \to \R$ be a continuous function.  Let $\epsilon > 0$
be a constant.  For $x \in [a+\epsilon,b-\epsilon]$, define
\begin{equation*}
g(x) := \frac{1}{2\epsilon} \int_{x-\epsilon}^{x+\epsilon} f .
\end{equation*}
a) Show that $g$ is differentiable and find the derivative.
\\
b) Let $f$ be differentiable and fix $x \in (a,b)$ (let $\epsilon$
be small enough).  What happens to $g'(x)$ as $\epsilon$ gets smaller?
\\
c) Find $g$ for $f(x) := \abs{x}$, $\epsilon = 1$ (you can assume 
$[a,b]$ is large enough).
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is continuous and
$\int_a^x f = \int_x^b f$ for all $x \in [a,b]$.  Show that $f(x) = 0$
for all $x \in [a,b]$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is continuous and
$\int_a^x f = 0$ for all rational $x$ in $[a,b]$.  Show that $f(x) = 0$
for all $x \in [a,b]$.
\end{exercise}

\begin{exercise}
A function $f$ is an \emph{\myindex{odd function}} if $f(x) = -f(-x)$,
and $f$ is an \emph{\myindex{even function}} if $f(x) = f(-x)$.  Let $a >
0$.  Assume $f$ is continuous.  Prove: a)~If $f$ is odd, then $\int_{-a}^a f
= 0$.  b)~If $f$ is even, then $\int_{-a}^a f = 2 \int_0^a f$.
\end{exercise}

\begin{exercise}
a) Show that $f(x) := \sin(\nicefrac{1}{x})$
is integrable on any interval (you can define $f(0)$ to be anything).
b)~Compute $\int_{-1}^1 \sin(\nicefrac{1}{x})\,dx$.  (Mind the
discontinuity)
\end{exercise}

\begin{exercise}[uses \sectionref{sec:monotonefunc}]
a) Suppose $f \colon [a,b] \to \R$ is increasing, by
\propref{prop:monotoneintegrable},
%\exerciseref{exercise:boundedvariationintegrable},
$f$ is Riemann integrable.  Suppose $f$ has a discontinuity at $c \in
(a,b)$, show that $F(x) := \int_a^x f$ is not differentiable at $c$.
\\
b) In \exerciseref{exercise:increasingfuncdiscatQ}, you constructed an increasing
function $f \colon [0,1] \to \R$ that is discontinuous at every
$x \in [0,1] \cap \Q$.  Use this $f$ to construct a function $F(x)$ that is
continuous on $[0,1]$, but not differentiable at every $x \in [0,1] \cap \Q$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{The logarithm and the exponential}
\label{sec:logandexp}

\sectionnotes{1 lecture (optional, requires the optional sections 
\sectionref{sec:limitatinf},
\sectionref{sec:monotonefunc},
\sectionref{sec:ift})}

We now have all that is required to finally properly define the exponential
and the
logarithm that you know from calculus so well.
First, we have a good idea of what $x^n$ means as long as
$n$ is a positive integer.  Simply,
\begin{equation*}
x^n := \underbrace{x \cdot x \cdot \cdots \cdot x}_{\text{$n$ times}} .
\end{equation*}
It makes sense to define $x^0 := 1$.
For negative integers we define $x^{-n} := \nicefrac{1}{x^n}$.
If $x > 0$,
we defined $x^{1/n}$ as
the unique positive $n$th root.  Finally, for a rational
number $\nicefrac{n}{m}$ (in lowest terms), we define
\begin{equation*}
x^{n/m} := {\bigl(x^{1/m}\bigr)}^n .
\end{equation*}
It is not difficult to show 
we get the same number no matter what
representation of $\nicefrac{n}{m}$ we use, so we do not need to use
lowest terms.
%See part a) of
%\exerciseref{exercise:realpower}.

However, what do we mean by $\sqrt{2}^{\sqrt{2}}$?  Or
$x^y$ in general?  In particular, what is $e^x$ for all $x$?
And how do we solve $y=e^x$ for $x$?
This section answers these questions and more.

\subsection{The logarithm}
\index{logarithm}

It is convenient to start with the logarithm.  
Let us show that
a unique function with the right properties exists, and only then will
we call it \emph{the} logarithm.

\begin{prop}
There exists a unique function $L \colon (0,\infty) \to \R$ such that
\begin{enumerate}[(i)]
\item \label{it:log:i}
$L(1) = 0$.
\item \label{it:log:ii}
$L$ is differentiable and $L'(x) = \nicefrac{1}{x}$.
\item \label{it:log:iii}
$L$ is strictly increasing, bijective, and
\begin{equation*}
\lim_{x\to 0} L(x) = -\infty , \qquad \text{and} \qquad
\lim_{x\to \infty} L(x) = \infty .
\end{equation*}
\item \label{it:log:iv}
$L(xy) = L(x)+L(y)$ for all $x,y \in (0,\infty)$.
\item \label{it:log:v}
If $q$ is a rational number and $x > 0$, then
$L(x^q) = q L(x)$.
\end{enumerate}
\end{prop}

\begin{proof}
To prove existence, let us define a candidate and show it satisfies
all the properties.  Define
\begin{equation*}
L(x) := \int_1^x \frac{1}{t}~dt .
\end{equation*}

Obviously \ref{it:log:i} holds.  Property \ref{it:log:ii} holds
via the fundamental theorem of calculus.

To prove property \ref{it:log:iv},
we change variables $u=yt$ to obtain
\begin{equation*}
L(x) =
\int_1^{x} \frac{1}{t}~dt
=
\int_y^{xy} \frac{1}{u}~du
=
\int_1^{xy} \frac{1}{u}~du
-
\int_1^{y} \frac{1}{u}~du
=
L(xy)-L(y) .
\end{equation*}

Property \ref{it:log:ii} together with the fact that $L'(x) = \nicefrac{1}{x} > 0$ 
for $x > 0$, implies that $L$
is strictly increasing and hence one-to-one.
Let us show $L$ is onto.  
As $\nicefrac{1}{t} \geq \nicefrac{1}{2}$ when $t \in [1,2]$,
\begin{equation*}
L(2) = \int_1^2 \frac{1}{t} ~dt \geq \nicefrac{1}{2} .
\end{equation*}
By induction, \ref{it:log:iv} implies that for $n \in \N$
\begin{equation*}
L(2^n) = L(2) + L(2) + \cdots + L(2) = n L(2) .
\end{equation*}
Given any $y > 0$, 
by the \hyperref[thm:arch:i]{Archimedean property} of the real numbers
(notice $L(2) > 0$), there is an $n \in \N$ such that
$L(2^n) > y$.  By the
\hyperref[IVT:thm]{intermediate value theorem}
there is an $x_1 \in (1,2^n)$ such that $L(x_1) = y$.  We get
$(0,\infty)$ is in the image of $L$.
As $L$ is increasing, $L(x) > y$ for all $x > 2^n$, and so
\begin{equation*}
\lim_{x\to\infty} L(x) = \infty .
\end{equation*}
Next
$0 = L(\nicefrac{x}{x}) = L(x) + L(\nicefrac{1}{x})$, and
so $L(x) = - L(\nicefrac{1}{x})$.  Using $x=2^{-n}$, we obtain
as above that $L$ achieves all negative numbers.  And
\begin{equation*}
\lim_{x \to 0} L(x) = 
\lim_{x \to 0} -L(\nicefrac{1}{x})
=
\lim_{x \to \infty} -L(x)
=  - \infty .
\end{equation*}
In the limits, note that only $x > 0$ are in the domain of $L$.
%Once the two limits are proved, the fact that $L$ is onto follows by
%\hyperref[IVT:thm]{intermediate value theorem}.

Let us now prove \ref{it:log:v}.
As above, \ref{it:log:iv} implies for $n \in \N$ we have
$L(x^n) = n L(x)$.
We already saw that
$L(x) = - L(\nicefrac{1}{x})$
so $L(x^{-n}) = - L(x^n) = -n L(x)$.  Then for $m \in \N$
\begin{equation*}
L(x) = L\Bigl({(x^{1/m})}^m\Bigr) = m L(x^{1/m}) .
\end{equation*}
Putting everything together for $n \in \Z$ and $m \in \N$ we have
$L(x^{n/m}) = n L(x^{1/m}) = (\nicefrac{n}{m}) L(x)$.

Finally for uniqueness, let us use properties \ref{it:log:i} and
\ref{it:log:ii}.  Via the fundamental theorem of calculus
\begin{equation*}
L(x) = \int_1^x \frac{1}{t}~dt
\end{equation*}
is the unique function such that $L(1) = 0$ and $L'(x) = \nicefrac{1}{x}$.
\end{proof}

Having proved
that there is a unique function with these properties
we simply define the \emph{\myindex{logarithm}} or sometimes called the
\emph{\myindex{natural logarithm}}:
\begin{equation*}
\ln(x) := L(x) .
\end{equation*}
Often mathematicians write $\log(x)$ instead of $\ln(x)$, which is
more familiar to calculus students.

\subsection{The exponential}
\index{exponential}

Just as with the logarithm we define the exponential via a list of
properties.

\begin{prop}
There exists a unique function $E \colon \R \to (0,\infty)$ such that
\begin{enumerate}[(i)]
\item \label{it:exp:i}
$E(0) = 1$.
\item \label{it:exp:ii}
$E$ is differentiable and $E'(x) = E(x)$.
\item \label{it:exp:iii}
$E$ is strictly increasing, bijective, and
\begin{equation*}
\lim_{x\to -\infty} E(x) = 0 , \qquad \text{and} \qquad
\lim_{x\to \infty} E(x) = \infty .
\end{equation*}
\item \label{it:exp:iv}
$E(x+y) = E(x)E(y)$ for all $x,y \in \R$.
\item \label{it:exp:v}
If $q \in \Q$, then
$E(qx) = {E(x)}^q$.
\end{enumerate}
\end{prop}

\begin{proof}
Again, let us prove existence of such a function by defining a candidate,
and prove that it satisfies all the properties.
The $L$ defined above is invertible.  Let $E$ be the
inverse function of $L$.  Property \ref{it:exp:i} is immediate.

Property \ref{it:exp:ii} follows
via the inverse function theorem, in particular
\lemmaref{lemma:ift}:  $L$ satisfies
all the hypotheses of the lemma, and hence
\begin{equation*}
E'(x) = \frac{1}{L'\bigl(E(x)\bigr)} = E(x) .
\end{equation*}

Let us look at property \ref{it:exp:iii}.
The function $E$ is strictly increasing since $E(x) > 0$ and
$E'(x) = E(x) > 0$.  As $E$ is the inverse of $L$, it must also
be bijective.  
To find the limits, we use that 
$E$ is strictly increasing and onto $(0,\infty)$.
For every $M > 0$, there is an $x_0$ such that
$E(x_0) = M$ and $E(x) \geq M$ for all $x \geq x_0$.
Similarly for every $\epsilon > 0$, there is
an $x_0$ such that $E(x_0) = \epsilon$ and
$E(x) < \epsilon$ for all $x < x_0$.
Therefore,
\begin{equation*}
\lim_{n\to -\infty} E(x) = 0 , \qquad \text{and} \qquad
\lim_{n\to \infty} E(x) = \infty .
\end{equation*}

To prove property \ref{it:exp:iv} we use the corresponding
property for the logarithm.
Take $x, y \in \R$.
As $L$ is bijective, find $a$ and $b$ such that $x = L(a)$ and $y = L(b)$.  Then
\begin{equation*}
E(x+y) =
E\bigl(L(a)+L(b)\bigr) = 
E\bigl(L(ab)\bigr) = ab = E(x)E(y)  .
\end{equation*}

Property \ref{it:exp:v} also follows from the corresponding property of $L$.
Given $x \in \R$, let $a$ be such that $x = L(a)$ and
\begin{equation*}
E(qx) = E\bigl(qL(a)\bigr)
E\bigl(L(a^q)\bigr) = a^q = {E(x)}^q .
\end{equation*}

Finally, uniqueness follows from
\ref{it:exp:i} and
\ref{it:exp:ii}.
Let $E$ and $F$
be two functions satisfying
\ref{it:exp:i} and \ref{it:exp:ii}.  
\begin{equation*}
\frac{d}{dx} \Bigl( F(x)E(-x) \Bigr)
=
F'(x)E(-x) - E'(-x)F(x)
=
F(x)E(-x) - E(-x)F(x) = 0 .
\end{equation*}
Therefore by \propref{prop:derzeroconst},
$F(x)E(-x) = F(0)E(-0) = 1$ for all $x \in \R$.
Doing the computation with $F = E$,
we obtain $E(x)E(-x) = 1$.  Then
\begin{equation*}
0 = 1-1 = F(x)E(-x) - E(x)E(-x) = \bigl(F(x)-E(x)\bigr) E(-x) .
\end{equation*}
Since $E(x)E(-x) = 1$, then $E(-x) \not= 0$ for
all $x$.  So
$F(x)-E(x) = 0$ for all $x$, and we are done.
\end{proof}

Having proved $E$ is unique, we define the
\emph{\myindex{exponential}} function as
\begin{equation*}
\exp(x) := E(x) .
\end{equation*}

We can now make sense of exponentiation $x^y$ for arbitrary numbers
when $x > 0$.  First suppose $y \in \Q$.  Then
\begin{equation*}
x^y = \exp\bigl(\ln(x^y)\bigr) = \exp\bigl(y\ln(x)\bigr) .
\end{equation*}
Therefore when $x > 0$ and $y$ is irrational let us define
\begin{equation*}
x^y := \exp\bigl(y\ln(x)\bigr) .
\end{equation*}
As $\exp$ is continuous then $x^y$ is a continuous function of $y$.
Therefore, we would
obtain the same result had we taken a sequence of rational numbers $\{ y_n \}$
approaching $y$ and defined $x^y = \lim\, x^{y_n}$.

Define the number $e$ as
\begin{equation*}
e := \exp(1) .
\end{equation*}
The number $e$ is sometimes called \emph{\myindex{Euler's number}} or
the \emph{\myindex{base of the natural logarithm}}.
We notice 
\begin{equation*}
e^x = \exp\bigl(x \ln(e) \bigr) = \exp(x) .
\end{equation*}
We have justified the notation $e^x$ for $\exp(x)$.

Finally, let us extend properties of logarithm and exponential to
irrational powers.  The proof is immediate.

\begin{prop}
Let $x, y \in \R$.
\begin{enumerate}[(i)]
\item
$\exp(xy) = {\bigl(\exp(x)\bigr)}^y$.
\item
If $x > 0$ then $\ln(x^y) = y \ln (x)$.
\end{enumerate}
\end{prop}

\subsection{Exercises}

\begin{exercise}
Let $y$ be any real number and $b > 0$.  Define $f \colon (0,\infty) \to \R$
and $g \colon \R \to \R$ as, $f(x) := x^y$ and $g(x) := b^x$.  Show that $f$
and $g$ are differentiable and find their derivative.
\end{exercise}

\begin{exercise}
Let $b > 0$ be given.\\
a) Show that for every $y > 0$, there exists a unique number $x$
such that $y = b^x$.  Define
the \emph{\myindex{logarithm base $b$}},
$\log_b \colon (0,\infty) \to \R$, by
$\log_b(y) := x$.
\\
b) Show that $\log_b(x) = \frac{\ln(x)}{\ln(b)}$.
\\
c) Prove that if $c > 0$, then
$\log_b(x) = \frac{\log_c(x)}{\log_c(b)}$.
\\
d) Prove $\log_b(xy) =
\log_b(x)+\log_b(y)$, and $\log_b(x^y) = y \log_b(x)$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:taylor}]
Use \hyperref[thm:taylor]{Taylor's theorem} to study the remainder term and show that for
all $x \in \R$
\begin{equation*}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} .
\end{equation*}
Hint: Do not differentiate the series term by term (unless you would prove that it
works).
\end{exercise}

\begin{exercise}
Use the geometric sum formula to show (for $t\not= -1$)
\begin{equation*}
1-t+t^2-\cdots+{(-1)}^n t^n = \frac{1}{1+t} - \frac{{(-1)}^{n+1}t^{n+1}}{1+t}.
\end{equation*}
Using this fact show
\begin{equation*}
\ln (1+x) = \sum_{n=1}^\infty \frac{{(-1)}^{n+1}x^n}{n} 
\end{equation*}
for all $x \in (-1,1]$ (note that $x=1$ is included).  Finally,
find the limit of the alternating harmonic series
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^{n+1}}{n} = 1 - \nicefrac{1}{2} +
\nicefrac{1}{3} - \nicefrac{1}{4} + \cdots
% = \ln 2 .
\end{equation*}

\begin{exercise}
Show 
\begin{equation*}
e^x = \lim_{n\to\infty} {\left( 1 + \frac{x}{n} \right)}^n .
\end{equation*}
Hint: Take the logarithm.\\
Note: The expression 
${\left( 1 + \frac{x}{n} \right)}^n$ arises in compound interest
calculations.  It is the amount of money in a bank account after 1 year
if 1 dollar was deposited initially at interest $x$
and the interest was compounded $n$
times during the year.  Therefore $e^x$ is the result of continuous
compounding.
%Oftentimes this limit with $x=1$ is used to define the number $e$.
\end{exercise}

\end{exercise}

\begin{exercise}
a)
Prove that for $n \in \N$ we have
\begin{equation*}
\sum_{k=2}^{n}
\frac{1}{k}
\leq
\ln (n)
\leq
\sum_{k=1}^{n-1}
\frac{1}{k} .
\end{equation*}
\\
b) Prove that the limit
\begin{equation*}
\gamma := \lim_{n\to\infty}
\left( \sum_{k=1}^{n}
\frac{1}{k} - \ln (n) \right)
\end{equation*}
exists.  This constant is known as the
\emph{\myindex{Euler--Mascheroni constant}}%
\footnote{Named for the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Paul Euler} (1707 -- 1783)
and the Italian mathematician
\href{http://en.wikipedia.org/wiki/Lorenzo_Mascheroni}{Lorenzo Mascheroni}
(1750 -- 1800).}.  It is not known if this constant is rational or not,
it is approximately $\gamma \approx 0.5772$.
\end{exercise}

\begin{exercise}
Show
\begin{equation*}
\lim_{x\to\infty} \frac{\ln(x)}{x} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $e^x$ is \emph{\myindex{convex}}, in other words, show that 
if $a \leq x \leq b$ then
$e^x \leq e^a \frac{b-x}{b-a} + e^b \frac{x-a}{b-a}$.
\end{exercise}

\begin{exercise}
Using the logarithm find
\begin{equation*}
%\lim_{n\to\infty} {\left( 1 + \nicefrac{1}{n} \right)}^n = e .
\lim_{n\to\infty} n^{1/n} .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $E(x) = e^x$ is the unique continuous function such that
$E(x+y) = E(x)E(y)$ and $E(1) = e$.   Similarly prove that $L(x) = \ln(x)$
is the unique continuous
function defined on positive $x$ such that $L(xy) = L(x)+L(y)$
and $L(e) = 1$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:taylor}]\label{exercise:nonanalytic}
Since $(e^x)' = e^x$, it is easy to see that $e^x$ is infinitely
differentiable (that is, has infinitely many derivatives).  Define the function $f \colon \R \to \R$.
\begin{equation*}
f(x) := \begin{cases}
e^{-1/x} & \text{if $x > 0$,} \\
0 & \text{if $x \leq 0$}.
\end{cases}
\end{equation*}
a)~Prove that for any $m \in \N$,
\begin{equation*}
\lim_{x \to 0} \frac{e^x}{x^m} = 0 .
\end{equation*}
b)~Prove that $f$ is infinitely differentiable.\\
c)~Compute the Taylor series for $f$ at the origin, that is,
\begin{equation*}
\sum_{k=0}^\infty
\frac{f^{(k)}(0)}{k!}x^k .
\end{equation*}
Show that it converges, but show that it does not converge to $f(x)$
for any $x > 0$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Improper integrals}
\label{sec:impropriemann}

\sectionnotes{2--3 lectures (optional section, can safely be skipped, 
requires the optional \sectionref{sec:limitatinf})}

Often it is necessary to integrate over the
entire real line, or a infinite interval of the form $[a,\infty)$ or
$(\infty,b]$.  Also, we may wish to integrate functions defined on a finite
interval $(a,b)$ but not bounded.
Such functions are not Riemann integrable, but we may want to write down
the integral anyway in the spirit of \lemmaref{lemma:boundedimpriemann}.
These integrals are called \emph{\myindex{improper integrals}},
and are limits
of integrals rather than integrals themselves.

\begin{defn}
Suppose $f \colon [a,b) \to \R$ is a function (not necessarily bounded)
that is Riemann integrable on $[a,c]$ for all $c < b$.  We define
\begin{equation*}
\int_a^b f := \lim_{c \to b^-} \int_a^{c} f ,
\end{equation*}
if the limit exists.%  Of course in the limit we are ``going up to'' $b$
%as the integral is only defined when $c < b$.

Suppose $f \colon [a,\infty) \to \R$ is a function such that
$f$ is Riemann integrable on $[a,c]$ for all $c < \infty$.  
We define
\begin{equation*}
\int_a^\infty f := \lim_{c \to \infty} \int_a^c f ,
\end{equation*}
if the limit exists.

If the limit exists, we say the improper integral
\emph{\myindex{converges}}\index{convergent improper integral}.
If the limit does not exist, we say the improper integral
\emph{\myindex{diverges}}\index{divergent improper integral}.

We similarly define improper integrals for the left hand endpoint, we leave
this to the reader.
\end{defn}

For a finite endpoint $b$,
using \lemmaref{lemma:boundedimpriemann} we see that if
$f$ is bounded, then we have defined nothing new.  What is new is that
we can apply this definition to unbounded functions.
The following set of examples is
so useful that we state it as a proposition.

\begin{prop}[$p$-test for integrals]\index{p-test for integrals}
\label{impropriemann:ptest}
The improper integral
\begin{equation*}
\int_1^\infty \frac{1}{x^p} ~dx
\end{equation*}
converges to $\frac{1}{p-1}$ if $p > 1$ and diverges if $0 < p \leq 1$.

The improper integral
\begin{equation*}
\int_0^1 \frac{1}{x^p} ~dx
\end{equation*}
converges to $\frac{1}{1-p}$ if $0 < p < 1$ and diverges if $p \geq 1$.
\end{prop}

\begin{proof}
The proof follows by application of the fundamental theorem of calculus.
Let us do the proof for $p > 1$ for the infinite right endpoint, and
we leave the rest to the reader.  Hint: You should handle $p=1$
separately.

Suppose $p > 1$.  Then
\begin{equation*}
\int_1^b \frac{1}{x^p} ~dx
=
\int_1^b x^{-p} ~dx
=
\frac{b^{-p+1}}{-p+1}
-
\frac{1^{-p+1}}{-p+1}
=
-
\frac{1}{(p-1)b^{p-1}}
+
\frac{1}{p-1} .
\end{equation*}
As $p > 1$, then $p-1 > 0$.  Taking the limit as $b \to \infty$
we obtain that $\frac{1}{b^{p-1}}$ goes to 0, and the result follows.
\end{proof}

We state the following proposition for just one type
of improper integral, though the proof is straight
forward and the same for other types of improper integrals.

\begin{prop} \label{impropriemann:tail}
Let $f \colon [a,\infty) \to \R$ be a function
that is Riemann integrable on $[a,b]$ for all $b > a$.
Given any $b > a$,
$\int_b^\infty f$ converges if and only if $\int_a^\infty f$
converges, in which case
\begin{equation*}
\int_a^\infty f
=
\int_a^b f +
\int_b^\infty f .
\end{equation*}
\end{prop}

\begin{proof}
Let $c > b$.  Then
\begin{equation*}
\int_a^c f
=
\int_a^b f +
\int_b^c f .
\end{equation*}
Taking the limit $c \to \infty$ finishes the proof.
\end{proof}

Nonnegative functions are easier to work with
as the following proposition demonstrates.
The exercises will show that this proposition
holds only for nonnegative functions.
Analogues of this proposition
exist for all the other types of improper limits are left to the
student.

\begin{prop} \label{impropriemann:possimp}
Suppose $f \colon [a,\infty) \to \R$ is nonnegative ($f(x)
\geq 0$ for all $x$) and such that
$f$ is Riemann integrable on $[a,b]$ for all $b > a$.
\begin{enumerate}[(i)]
\item 
\begin{equation*}
\int_a^\infty f = \sup \left\{ \int_a^x f : x \geq a \right\} .
\end{equation*}
\item
Suppose $\{ x_n \}$
is a sequence with $\lim\, x_n = \infty$.  Then
$\int_a^\infty f$ converges if and only if $\lim\, \int_a^{x_n} f$ exists, in
which case
\begin{equation*}
\int_a^\infty f = \lim_{n\to\infty} \int_a^{x_n} f .
\end{equation*}
\end{enumerate}
\end{prop}

In the first item we allow for the value of $\infty$ in the
supremum indicating that the integral diverges to infinity.

\begin{proof}
Let us start with the first item.
Notice that as $f$ is nonnegative,
then $\int_a^x f$ is increasing as a function of $x$.
If the supremum is infinite, then for every $M \in \R$
we find $N$ such that $\int_a^N f \geq M$.  As $\int_a^x f$
is increasing then $\int_a^x f \geq M$ for all $x \geq N$.  So
$\int_a^\infty f$ diverges to infinity.
%  Similarly if
%$\int_a^\infty f$ diverges to infinity, then we can find
%$x$ with $\int_a^x f$ arbitrarily large, so the supremum
%is infinite.

Next suppose the supremum is finite, say
$A = \sup \left\{ \int_a^x f : x \geq a \right\}$.
For every $\epsilon > 0$, we find an $N$ such that
$A - \int_a^N f < \epsilon$.  As $\int_a^x f$ is increasing,
then
$A - \int_a^x f < \epsilon$ for all $x \geq N$ and hence
$\int_a^\infty f$ converges to $A$.

Let us look at the second item.
If $\int_a^\infty f$ converges then every sequence $\{ x_n \}$ going to
infinity works.  The trick is
proving the other direction.  Suppose $\{ x_n \}$ is such that $\lim\, x_n =
\infty$ and
\begin{equation*}
\lim_{n\to\infty} \int_a^{x_n} f = A
\end{equation*}
converges.  Given $\epsilon > 0$, pick $N$ such that for
all $n \geq N$ we have
%\begin{equation*}
%\abs{\int_a^{x_n} f - A} < \epsilon .
%\end{equation*}
%\begin{equation*}
$A - \epsilon < \int_a^{x_n} f < A + \epsilon$.
%\end{equation*}
Because $\int_a^x f$ is increasing as a function of $x$, we have that for all
$x \geq x_N$
\begin{equation*}
A - \epsilon < \int_a^{x_N} \leq \int_a^x f .
\end{equation*}
As $\{ x_n \}$ goes to $\infty$, then for any given
$x$, there is an $x_m$ such that $m \geq N$ and $x \leq x_m$.  Then
\begin{equation*}
\int_a^{x} f \leq \int_a^{x_m} f < A + \epsilon .
\end{equation*}
In particular, for all $x \geq x_N$ we have
$\abs{\int_a^{x} f - A} < \epsilon$.
%As $\{ x_n \}$ goes to infinity, we can pick
%a subsequence $\{ x_{n_k} \}$ that is monotone increasing (why?).
%Therefore
%$\{ \int_a^{x_{n_k}} f \}$ is monotone increasing.
%\begin{equation*}
%A = \lim_{n\to\infty} \int_a^{x_n} f =
%\lim_{k\to\infty} \int_a^{x_{n_k}} f
%=
%\sup_{k \in \N} \int_a^{x_{n_k}} f .
%\end{equation*}
%So $A \leq \sup \left\{ \int_a^x f : x \geq a \right\}$.  As for any $x$
%there exists an $x_{n_k}$ such that $x \leq x_{n_k}$ and so
%$\int_a^x f \leq \int_a^{x_{n_k}}$, we see that
%So $A \geq \sup \left\{ \int_a^x f : x \geq a \right\}$.  We finish
%by applying the first item.
\end{proof}

\begin{prop}[\myindex{Comparison test for improper integrals}]
Let
$f \colon [a,\infty) \to \R$ and
$g \colon [a,\infty) \to \R$ be functions
that are Riemann integrable on $[a,b]$ for all $b > a$.   Suppose
that for all $x \geq a$ we have
\begin{equation*}
\abs{f(x)} \leq g(x) .
\end{equation*}
\begin{enumerate}[(i)]
\item If $\int_a^\infty g$ converges, then $\int_a^\infty f$ converges,
and in this case 
$\abs{\int_a^\infty f} \leq \int_a^\infty g$.
\item If $\int_a^\infty f$ diverges, then $\int_a^\infty g$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.
For any $b$ and $c$, such that $a \leq b \leq c$, we have 
$-g(x) \leq f(x) \leq g(x)$, and so
\begin{equation*}
\int_b^c -g \leq \int_b^c f \leq \int_b^c g  .
\end{equation*}
In other words, $\abs{\int_b^c f} \leq \int_b^c g$.

Let $\epsilon > 0$ be given.  Because
of \propref{impropriemann:tail} we have
\begin{equation*}
\int_a^\infty g =
\int_a^b g +
\int_b^\infty g .
\end{equation*}
As $\int_a^b g$ goes to
$\int_a^\infty g$ as $b$ goes to infinity, then
$\int_b^\infty g$ goes to 0 as $b$ goes to infinity.  Choose $B$
such that
\begin{equation*}
\int_B^\infty g < \epsilon .
\end{equation*}
As $g$ is nonnegative, then if $B \leq b < c$, then
$\int_b^c g < \epsilon$ as well.
Let $\{ x_n \}$ be a sequence going to infinity.  Let $M$ be such that
$x_n \geq B$ for all $n \geq M$.  Take $n, m \geq M$,
with $x_n \leq x_m$,
\begin{equation*}
\abs{\int_a^{x_m} f - \int_a^{x_n} f} 
=
\abs{\int_{x_n}^{x_m} f} 
\leq \int_{x_n}^{x_m} g < \epsilon .
\end{equation*}
Therefore the sequence $\{ \int_a^{x_n} f \}_{n=1}^\infty$ is Cauchy and hence converges.

We need to show that the limit is unique.  Suppose $\{ x_n \}$ is a sequence
converging to infinity such that
$\{ \int_a^{x_n} f \}$ converges to $L_1$, and $\{ y_n \}$ is a sequence
converging to infinity is such that
$\{ \int_a^{y_n} f \}$ converges to $L_2$.  Then there must be some $n$ such
that
$\abs{\int_a^{x_n} f - L_1} < \epsilon$ and 
$\abs{\int_a^{y_n} f - L_2} < \epsilon$.  We can also suppose $x_n \geq B$
and $y_n \geq B$.  Then
\begin{equation*}
\abs{L_1 - L_2} \leq
\abs{L_1 - \int_a^{x_n} f}
+
\abs{\int_a^{x_n} f- \int_a^{y_n} f}
+
\abs{\int_a^{y_n} f - L_2}
<
\epsilon
+
\abs{\int_{x_n}^{y_n} f}
+
\epsilon
<
3 \epsilon.
\end{equation*}
As $\epsilon > 0$ was arbitrary, $L_1 = L_2$, and hence
$\int_a^\infty f$ converges.
Above we have shown that $\abs{\int_a^c f} \leq \int_a^c g$ for all $c > a$.
By taking the limit $c \to \infty$, the first item is proved.

The second item is simply a contrapositive of the first item.
%Now suppose that $\int_a^\infty f$ diverges.  Applying the contrapositive
%of the first item with $g(x) = \abs{f(x)}$ we conclude that $\int_a^\infty \abs{f(x)}$
%must diverge.
%Because $\abs{f(x)} \geq 0$, the sequence
%$\{ \int_a^n \abs{f(x)}~dx \}_{n=1}^\infty$ is monotone increasing.  Since it diverges it must go to
%infinity:  For any $M \in \R$, there exists an $N \in \N$
%such that for all $n \geq N$, we have $\int_a^n \abs{f(x)}~dx > M$.
%For such $n$ we have
%\begin{equation*}
%M < \int_a^n \abs{f(x)}~dx \leq \int_a^n g .
%\end{equation*}
%The sequence $\{ \int_a^n g \}_{n=1}^\infty$ diverges (to infinity), and therefore
%the improper integral $\int_a^\infty g$ diverges.
\end{proof}

\begin{example}
The improper integral
\begin{equation*}
\int_0^\infty \frac{\sin(x^2)(x+2)}{x^3+1} ~dx
\end{equation*}
converges.

Proof:  First observe we simply need to show
that the integral converges when going from 1 to infinity.
For $x \geq 1$ we obtain
\begin{equation*}
\abs{\frac{\sin(x^2)(x+2)}{x^3+1}}
\leq
\frac{x+2}{x^3+1}
\leq \frac{x+2}{x^3} \leq
\frac{x+2x}{x^3} \leq \frac{3}{x^2} .
\end{equation*}
Then
\begin{equation*}
3 \int_1^\infty \frac{1}{x^2}~dx
=
\lim_{c\to\infty} \int_1^c \frac{3}{x^2} ~dx.
\end{equation*}
So the integral converges.
\end{example}

\begin{example}
You should be careful when doing formal manipulations with improper
integrals.
For example,
\begin{equation*}
\int_2^\infty \frac{2}{x^2-1}~dx
\end{equation*}
converges via the comparison test again using $\frac{1}{x^2}$.  However, if you
succumb to the temptation to write
\begin{equation*}
\frac{2}{x^2-1} = 
\frac{1}{x-1}
-
\frac{1}{x+1} 
\end{equation*}
and try to integrate each part separately, you will not succeed.
It is \emph{not} true that you can split the improper
integral in two; you cannot split the limit.
\begin{equation*}
\begin{split}
\int_2^\infty \frac{2}{x^2-1} ~dx &=
\lim_{b\to \infty} \int_2^b \frac{2}{x^2-1} ~dx
\\
&=
\lim_{b\to \infty}
\left(
\int_2^b \frac{1}{x-1}~dx
-
\int_2^b \frac{1}{x+1}~dx
\right)
\\
&\not=
\int_2^\infty \frac{1}{x-1}~dx
-
\int_2^\infty \frac{1}{x+1}~dx .
\end{split}
\end{equation*}
The last line in the computation does not even make sense.  Both of the
integrals there diverge to infinity since we can
apply the comparison test appropriately with
$\nicefrac{1}{x}$.  We get $\infty - \infty$.
\end{example}

Now let us suppose that we need to take limits at both endpoints.

\begin{defn}
Suppose $f \colon (a,b) \to \R$ is a function
that is Riemann integrable on $[c,d]$ for all $c$, $d$
such that $a < c < d < b$, then we define
\begin{equation*}
\int_a^b f := \lim_{c \to a^+} \, \lim_{d \to b^-} \, \int_{c}^{d} f ,
\end{equation*}
if the limits exist.

Suppose $f \colon \R \to \R$ is a function such that
$f$ is Riemann integrable on all finite intervals $[a,b]$.  Then
we define
\begin{equation*}
\int_{-\infty}^\infty f := \lim_{c \to -\infty} \, \lim_{d \to \infty} \, \int_c^d f ,
\end{equation*}
if the limits exist.

We similarly define improper integrals with one infinite and one finite
improper endpoint, we leave this to the reader.
\end{defn}

One ought to always be careful about double limits.  The definition
given above says that we first take the limit as $d$ goes to $b$ or
$\infty$ for a fixed $c$, and then we take the limit in $c$.
We will have to prove that in this case it does not matter which limit
we compute first.

\begin{example}
Let us see an example:
\begin{equation*}
\int_{-\infty}^\infty \frac{1}{1+x^2} ~ dx
=
\lim_{a \to -\infty} \, \lim_{b \to \infty} \,
\int_{a}^b \frac{1}{1+x^2} ~ dx
=
\lim_{a \to -\infty} \, \lim_{b \to \infty}
\bigl( \arctan(b) - \arctan(a) \bigr)
=
\pi .
\end{equation*}
\end{example}

In the definition the order of the limits can always be switched if they
exist.  Let us prove this fact only for the infinite limits.

\begin{prop}
If $f \colon \R \to \R$ is a function integrable on every interval.
Then 
\begin{equation*}
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
\quad \text{converges if and only if} \qquad
\lim_{b \to \infty}
\,
\lim_{a \to -\infty}
\,
\int_a^b f
\quad
\text{converges,}
\end{equation*}
in which case the two
expressions are equal.  If either of the
expressions converges then the improper integral converges and
\begin{equation*}
\lim_{a\to\infty}
\int_{-a}^a f
=
\int_{-\infty}^\infty f .
\end{equation*}
\end{prop}

\begin{proof}
Without loss of generality assume $a < 0$ and $b > 0$.  Suppose
the first expression converges.  Then
\begin{equation*}
\begin{split}
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
& =
\lim_{a \to -\infty} \, \lim_{b \to \infty}
\left(
\int_a^0 f
+
\int_0^b f
\right)
=
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right)
+
\left(
 \lim_{b \to \infty}
\int_0^b f
\right) \\
& = 
 \lim_{b \to \infty}
\left(
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right) 
+
\int_0^b f
\right)
=
 \lim_{b \to \infty} \,
\lim_{a \to -\infty}
\left(
\int_a^0 f
+
\int_0^b f
\right)  .
\end{split}
\end{equation*}
Similar computation shows the other direction.  Therefore, if
either expression converges then the improper integral converges
and
\begin{equation*}
\begin{split}
\int_{-\infty}^\infty f
=
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
& =
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right)
+
\left(
 \lim_{b \to \infty}
\int_0^b f
\right)
\\
& =
\left(
\lim_{a \to \infty}
\int_{-a}^0 f
\right)
+
\left(
 \lim_{a \to \infty}
\int_0^a f
\right)
=
\lim_{a \to \infty}
\left(
\int_{-a}^0 f
+
\int_0^a f
\right)
=
\lim_{a \to \infty}
\int_{-a}^a f .
\end{split}
\end{equation*}
\end{proof}

\begin{example}
On the other hand, you must be careful to
take the limits independently before you know convergence.  Let
$f(x) = \frac{x}{\abs{x}}$ for $x \not= 0$ and $f(0) = 0$.
If $a < 0$ and $b > 0$, then
\begin{equation*}
\int_{a}^b f
=
\int_{a}^0 f
+
\int_{0}^b f
=
a+b .
\end{equation*}
For any fixed $a < 0$ the limit as $b \to \infty$ is infinite, so even
the first limit does not exist, and hence the improper integral
$\int_{-\infty}^\infty f$
%of $f$ from $-\infty$ to $\infty$
does not converge.  On the other hand if $a > 0$, then
\begin{equation*}
\int_{-a}^{a} f
=
(-a)+a = 0 .
\end{equation*}
Therefore,
\begin{equation*}
\lim_{a\to\infty}
\int_{-a}^{a} f
= 0 .
\end{equation*}
\end{example}

\begin{example}
An example to keep in mind for improper integrals
is the so-called \emph{\myindex{sinc function}}%
\footnote{Shortened from Latin: \emph{sinus cardinalis}}.
This function comes up quite often
in both pure and applied mathematics.  Define
\begin{equation*}
\operatorname{sinc}(x) =
\begin{cases}
\frac{\sin(x)}{x} & \text{if $x \not= 0$} , \\
0 & \text{if $x = 0$} .
\end{cases}
\end{equation*}
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sincfig.eepic}
%\includegraphics{figures/sincfig}
\caption{The sinc function.
%The $x$ axis runs
%from -15 to 15 and the $y$ axis from $\nicefrac{-1}{4}$ to 1.
\label{figsinc}}
%\end{center}
\end{myfigureht}

It is not difficult to show that
the sinc function is continuous at zero, but that is
not important right now.  What is important is that
\begin{equation*}
\int_{-\infty}^\infty \operatorname{sinc}(x) ~dx = \pi ,
\qquad \text{while} \qquad
\int_{-\infty}^\infty \abs{\operatorname{sinc}(x)} ~dx = \infty .
\end{equation*}
The integral of the sinc function is a continuous analogue of the
alternating harmonic series $\sum \nicefrac{{(-1)}^n}{n}$, while the
absolute value is like the regular harmonic series $\sum \nicefrac{1}{n}$.
In particular, the fact that the integral converges must be done directly
rather than using comparison test.

We will not prove the first statement exactly.  Let us simply prove
that the integral of the sinc function converges, but we will not worry
about the exact limit.  Because $\frac{\sin(-x)}{-x} = \frac{\sin(x)}{x}$, it is
enough to show that
\begin{equation*}
\int_{2\pi}^\infty \frac{\sin(x)}{x}~dx
\end{equation*}
converges.  We
also avoid $x=0$ this way to make our life simpler.

For any $n \in \N$, we have that for $x \in [\pi 2n, \pi (2n+1)]$
\begin{equation*}
\frac{\sin(x)}{\pi (2n+1)}
\leq
\frac{\sin(x)}{x}
\leq
\frac{\sin(x)}{\pi 2n} ,
\end{equation*}
as $\sin(x) \geq 0$.  On $x \in [\pi (2n+1), \pi (2n+2)]$
\begin{equation*}
\frac{\sin(x)}{\pi (2n+1)}
\leq
\frac{\sin(x)}{x}
\leq
\frac{\sin(x)}{\pi (2n+2)} ,
\end{equation*}
as $\sin(x) \leq 0$.

Via the fundamental theorem of calculus,
\begin{equation*}
\frac{2}{\pi (2n+1)}
=
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{\pi (2n+1)}
~dx
\leq
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{x}
~dx
\leq
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{\pi 2n}
~dx
=
\frac{1}{\pi n} .
\end{equation*}
Similarly
\begin{equation*}
\frac{-2}{\pi (2n+1)}
\leq
\int_{\pi (2n+1)}^{\pi (2n+2)}
\frac{\sin(x)}{x}
~dx
\leq
\frac{-1}{\pi (n+1)} .
\end{equation*}
Adding the two together we find
\begin{equation*}
0
=
\frac{2}{\pi (2n+1)}
+
\frac{-2}{\pi (2n+1)}
\leq
\int_{2\pi n}^{2\pi (n+1)}
\frac{\sin(x)}{x}
~dx
\leq
\frac{1}{\pi n} 
+
\frac{-1}{\pi (n+1)} 
=
\frac{1}{\pi n(n+1)} .
\end{equation*}
See \figureref{fig:sincbound}.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{sincbound.eepic}
\caption{Bound of $\int_{2\pi n}^{2\pi (n+1)} \frac{\sin(x)}{x} ~dx$ using
the shaded integral (signed area
$\frac{1}{\pi n} 
+
\frac{-1}{\pi (n+1)}$).\label{fig:sincbound}}
%\end{center}
\end{myfigureht}

For $k \in \N$, 
\begin{equation*}
\int_{2\pi}^{2k\pi} \frac{\sin(x)}{x} ~dx
=
\sum_{n=1}^{k-1}
\int_{2\pi n}^{2\pi (n+1)} \frac{\sin(x)}{x} ~dx 
\leq
\sum_{n=1}^{k-1}
\frac{1}{\pi n(n+1)} .
\end{equation*}
We find the partial sums of a series with positive terms.
%as $0 \leq \int_{2\pi n}^{2\pi (n+1)} \frac{\sin(x)}{x} ~dx$.
The series
converges as
$\sum \frac{1}{\pi n (n+1)}$ is a convergent series.  Thus
as a sequence,
\begin{equation*}
\lim_{k\to \infty} \int_{2\pi}^{2k\pi} \frac{\sin(x)}{x} ~dx
=L \leq
\sum_{n=1}^{\infty}
\frac{1}{\pi n(n+1)} < \infty .
\end{equation*}

Let $M > 2\pi$ be arbitrary, and let $k \in \N$
be the largest integer such that $2k\pi \leq M$.
For $x \in [2k\pi,M]$ we have 
$\frac{-1}{2k\pi} \leq \frac{\sin(x)}{x} \leq \frac{1}{2k\pi}$, and so
\begin{equation*}
\abs{\int_{2k\pi}^{M} \frac{\sin(x)}{x} ~dx }  \leq
\frac{M-2k\pi}{2k\pi} \leq \frac{1}{k} .
\end{equation*}
As $k$ is the largest $k$ such that $2k\pi \leq M$,
then as $M\in \R$ goes to infinity, so does $k \in \N$.

Then
\begin{equation*}
\int_{2\pi}^M \frac{\sin(x)}{x}~dx
=
\int_{2\pi}^{2k\pi} \frac{\sin(x)}{x} ~dx
+
\int_{2k\pi}^{M} \frac{\sin(x)}{x} ~dx .
\end{equation*}
As $M$ goes to infinity,
the first term on the
right hand side goes to $L$,
and the second term on the
right hand side
goes to zero.  Hence
\begin{equation*}
\int_{2\pi}^\infty \frac{\sin(x)}{x} ~dx = L .
%\leq \sum_{n=1}^{\infty}
%\frac{1}{\pi n(n+1)} < \infty .
\end{equation*}

The double sided integral of sinc also exists as noted above.
We leave the other statement---that the integral
of the absolute value of the sinc function diverges---as an exercise.
\end{example}

\subsection{Integral test for series}

It can be very useful to apply the fundamental theorem 
of calculus in proving a series is summable and to estimate its sum.

\begin{prop}
Suppose $f \colon [k,\infty) \to \R$ is a decreasing nonnegative
function where $k \in \Z$.  Then
\begin{equation*}
\sum_{n=k}^\infty f(n)
\quad \text{converges if and only if}
\qquad
\int_k^\infty f
\quad \text{converges}.
\end{equation*}
In this case 
\begin{equation*}
\int_k^\infty f
\leq
\sum_{n=k}^\infty f(n)
\leq
f(k)+
\int_k^\infty f .
\end{equation*}
\end{prop}
See \figureref{fig:integraltest}, for an illustration with $k=1$.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{integraltest.eepic}
\caption{The area under the curve, 
$\int_1^\infty f$, is bounded below
by the area of the shaded rectangles,
$f(2)+f(3)+f(4)+\cdots$, and bounded above
by the area entire rectangles,
$f(1)+f(2)+f(3)+\cdots$.\label{fig:integraltest}}
%\end{center}
\end{myfigureht}
By \propref{prop:monotoneintegrable},
%By \exerciseref{exercise:boundedvariationintegrable},
$f$ is integrable
on every interval $[k,b]$ for all $b > k$, so the statement of the theorem
makes sense without additional hypotheses of integrability.

\begin{proof}
Let $\epsilon > 0$ be given.  And suppose $\int_k^\infty f$ converges.
Let $\ell, m \in \Z$ be such that $m > \ell \geq k$.
Because $f$ is decreasing we have
$\int_{n}^{n+1} f \leq f(n) \leq \int_{n-1}^{n} f$.  Therefore
\begin{equation} \label{impropriemann:eqseries}
\int_\ell^m f
=
\sum_{n=\ell}^{m-1} \int_{n}^{n+1} f
\leq
\sum_{n=\ell}^{m-1} f(n)
\leq
f(\ell) +
\sum_{n=\ell+1}^{m-1} \int_{n-1}^{n} f
\leq
f(\ell)+
\int_\ell^{m-1} f .
\end{equation}
As before, since $f$ is positive then there exists
an $L \in \N$ such that if $\ell \geq L$, then
$\int_\ell^{m} f < \nicefrac{\epsilon}{2}$ for all $m \geq \ell$.
We note 
$f$ must decrease to zero (why?).  So let us also suppose
that for $\ell \geq L$ we have $f(\ell) < \nicefrac{\epsilon}{2}$.
For such $\ell$ and $m$ we have via \eqref{impropriemann:eqseries}
\begin{equation*}
\sum_{n=\ell}^{m} f(n)
\leq
f(\ell)+
\int_\ell^{m} f < \nicefrac{\epsilon}{2} + \nicefrac{\epsilon}{2} = \epsilon .
\end{equation*}
The series is therefore Cauchy and thus converges.  The estimate in the
proposition is obtained by letting $m$ go to infinity in
\eqref{impropriemann:eqseries} with $\ell = k$.

Conversely suppose $\int_k^\infty f$ diverges.  
As $f$ is positive then by
\propref{impropriemann:possimp},
the sequence $\{ \int_k^m f \}_{m=k}^\infty$ diverges to infinity.
Using
\eqref{impropriemann:eqseries} with $\ell = k$ we find
\begin{equation*}
\int_k^m f
\leq
\sum_{n=k}^{m-1} f(n) .
\end{equation*}
As the left hand side goes to infinity as $m \to \infty$, so does the right
hand side.
\end{proof}

\begin{example}
Let us show $\sum_{n=1}^\infty \frac{1}{n^2}$ exists and let us
estimate its sum to within 0.01.  As this series is the $p$-series for
$p=2$, we already know it converges, but we have only very roughly
estimated its sum.

Using fundamental theorem of calculus we find that for $k \in \N$
we have
\begin{equation*}
\int_{k}^\infty \frac{1}{x^2}~dx = \frac{1}{k} .
\end{equation*}
In particular, the series must converge.  But we also have that
\begin{equation*}
\frac{1}{k} = \int_k^\infty \frac{1}{x^2}~dx
\leq
\sum_{n=k}^\infty \frac{1}{n^2}
\leq
\frac{1}{k^2}
+
\int_k^\infty \frac{1}{x^2}~dx
=
\frac{1}{k^2}
+
\frac{1}{k} .
\end{equation*}
Adding the partial sum up to $k-1$ we get
\begin{equation*}
\frac{1}{k} + \sum_{n=1}^{k-1} \frac{1}{n^2}
\leq
\sum_{n=1}^\infty \frac{1}{n^2}
\leq
\frac{1}{k^2}
+
\frac{1}{k} + \sum_{n=1}^{k-1} \frac{1}{n^2} .
\end{equation*}
In other words,
$\nicefrac{1}{k} + \sum_{n=1}^{k-1} \nicefrac{1}{n^2}$ is an estimate for
the sum to within $\nicefrac{1}{k^2}$.  Therefore, if we wish to
find the sum to within 0.01, we note $\nicefrac{1}{{10}^2} = 0.01$.  We
obtain
\begin{equation*}
1.6397\ldots
\approx
\frac{1}{10} + \sum_{n=1}^{9} \frac{1}{n^2}
\leq
\sum_{n=1}^\infty \frac{1}{n^2}
\leq
\frac{1}{100}
+
\frac{1}{10} + \sum_{n=1}^{9} \frac{1}{n^2}
\approx
1.6497\ldots .
\end{equation*}
The actual sum is $\nicefrac{\pi^2}{6} \approx 1.6449\ldots$. 
\end{example}

\subsection{Exercises}

\begin{exercise}
Finish the proof of \propref{impropriemann:ptest}.
\end{exercise}

\begin{exercise}
Find out for which $a \in \R$ does $\sum\limits_{n=1}^\infty e^{an}$ converge.
When the series converges, find an upper bound for the sum.
\end{exercise}

\begin{exercise}
a) Estimate $\sum\limits_{n=1}^\infty \frac{1}{n(n+1)}$ correct to within 0.01
using the integral test.  b) Compute the limit of the series exactly
and compare.  Hint: The sum telescopes.
\end{exercise}

\begin{exercise}
Prove 
\begin{equation*}
\int_{-\infty}^\infty \abs{\operatorname{sinc}(x)}~dx = \infty .
\end{equation*}
Hint: Again, it is enough to show this on just one side.
\end{exercise}

\begin{exercise}
Can you interpret
\begin{equation*}
\int_{-1}^1 \frac{1}{\sqrt{\abs{x}}}~dx
\end{equation*}
as an improper integral?  If so, compute its value.
\end{exercise}

\begin{exercise}
Take $f \colon [0,\infty) \to \R$, Riemann integrable on
every interval $[0,b]$, and such that there exist $M$, $a$, and $T$,
such that $\abs{f(t)} \leq M e^{at}$ for all $t \geq T$.  Show that the
\emph{\myindex{Laplace transform}} of $f$ exists.  That is, for
every $s > a$ the following integral converges:
\begin{equation*}
F(s) := \int_{0}^\infty f(t) e^{-st} ~dt .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be a Riemann integrable function
on every interval $[a,b]$, and such
that $\int_{-\infty}^\infty \abs{f(x)}~dx < \infty$.  Show that the
\emph{\myindex{Fourier sine and cosine transforms}}
exist.  That is, for every $\omega \geq 0$ the
following integrals converge
\begin{equation*}
F^s(\omega) := \frac{1}{\pi} \int_{-\infty}^\infty f(t) \sin(\omega t) ~dt ,
\qquad
F^c(\omega) := \frac{1}{\pi} \int_{-\infty}^\infty f(t) \cos(\omega t) ~dt .
\end{equation*}
Furthermore, show that $F^s$ and $F^c$ are bounded functions.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,\infty) \to \R$ is Riemann integrable on every interval
$[0,b]$.  Show that  $\int_0^\infty f$ converges if and only if
for every $\epsilon > 0$ there exists an $M$ such that if $M \leq a < b$
then $\abs{\int_a^b f} < \epsilon$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,\infty) \to \R$ is nonnegative and
\emph{decreasing}.
a) Show that if $\int_0^\infty f < \infty$, then $\lim\limits_{x\to\infty} f(x) = 0$.
b) Show that the converse does not hold.
\end{exercise}

\begin{exercise}
Find an example of an \emph{unbounded} continuous function $f \colon
[0,\infty) \to \R$ that is nonnegative and such that $\int_0^\infty f < \infty$.
Note that this means that $\lim_{x\to\infty} f(x)$ does not exist; compare
previous exercise.
Hint: On each interval $[k,k+1]$, $k \in \N$, define a function whose
integral over this interval is less than say $2^{-k}$.
\end{exercise}

\begin{exercise}[More challenging]
Find an example of a function $f \colon [0,\infty) \to \R$ integrable on all
intervals such that $\lim_{n\to\infty} \int_0^n f$ converges as a
limit of a sequence, but such that
$\int_0^\infty f$ does not exist.
Hint: For all $n\in \N$, divide $[n,n+1]$ into two halves.  In one half
make the function negative, on the other make the function positive.
\end{exercise}

\begin{exercise}
Show that if $f \colon [1,\infty) \to \R$ is such that
$g(x) := x^2 f(x)$ is a bounded function, then
$\int_1^\infty f$ converges.
\end{exercise}

\begin{exnote}
It is sometimes desirable to assign a value to integrals that normally
cannot be interpreted as even improper integrals,
e.g.\ $\int_{-1}^1 \nicefrac{1}{x}~dx$.
Suppose $f \colon [a,b] \to \R$ is a function and $a < c < b$,
where $f$ is Riemann integrable on all intervals
$[a,c-\epsilon]$ and $[c+\epsilon,b]$ for all $\epsilon > 0$.
Define
the \emph{\myindex{Cauchy principal value}} of $\int_a^b f$ as
\begin{equation*}
p.v.\!\int_a^b f := \lim_{\epsilon\to 0^+}
\left(
\int_a^{c-\epsilon} f + 
\int_{c+\epsilon}^b f
\right) ,
\end{equation*}
if the limit exists.
\end{exnote}

\begin{exercise}
a) Compute $p.v.\!\int_{-1}^1 \nicefrac{1}{x}~dx$.
\\
b) Compute
$\lim_{\epsilon\to 0^+}
( \int_{-1}^{-\epsilon} \nicefrac{1}{x}~dx + 
\int_{2\epsilon}^1 \nicefrac{1}{x}~dx )$ and show it is not equal
to the principal value.
\\
c) Show that if $f$ is integrable on $[a,b]$, then
$p.v.\!\int_a^b f = \int_a^b f$.
\\
d) Find an example of an $f$ with a singularity at $c$ as above
such that 
$p.v.\!\int_a^b f$ exists, but the improper integrals
$\int_a^c f$ and $\int_c^b f$ diverge.
\\
e) Suppose 
$f \colon [-1,1] \to \R$ is continuous.  Show that
$p.v.\!\int_{-1}^1 \frac{f(x)}{x}~dx$ exists.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ and 
$g \colon \R \to \R$ be continuous functions, where
$g(x) = 0$ for all $x \notin [a,b]$ for some interval $[a,b]$.
\\
a) Show that the
\emph{\myindex{convolution}}
\begin{equation*}
(g * f)(x) := \int_{-\infty}^\infty f(t)g(x-t)~dt 
\end{equation*}
is well-defined for all $x \in \R$.
\\
b) Suppose $\int_{-\infty}^\infty \abs{f(x)}~dx < \infty$.  Prove that
\begin{equation*}
\lim_{x \to -\infty} (g * f)(x) = 0, \qquad \text{and} \qquad
\lim_{x \to \infty} (g * f)(x) = 0 .
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Sequences of Functions} \label{fs:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pointwise and uniform convergence}
\label{sec:puconv}

\sectionnotes{1--1.5 lecture}

Up till now when we talked about sequences we always talked about
sequences of numbers.  However, a very useful concept in analysis is to use a
sequence of functions.  For example, a solution to some
differential equation
might be found by finding only approximate solutions.  Then the real solution is
some sort of limit of those approximate solutions.

When talking about sequences of functions, the 
tricky part is that there are multiple notions of a limit.
Let us describe two common
notions of a limit of a sequence of functions.

\subsection{Pointwise convergence}

\begin{defn}
\index{pointwise convergence}
For every $n \in \N$
let $f_n \colon S \to \R$ be a function.  We say the sequence
$\{ f_n \}_{n=1}^\infty$
\emph{\myindex{converges pointwise}} to $f \colon S \to \R$, if for every $x
\in S$
we have
\begin{equation*}
f(x) =
\lim_{n\to\infty} f_n(x) .
\end{equation*}
\end{defn}

It is common to say that $f_n \colon S \to \R$
\emph{converges to $f$ on $T \subset S$}
for some $f \colon T \to \R$.  In that case we, of course, mean 
$f(x) = \lim\, f_n(x)$ for every $x \in T$.  We simply mean that the
restrictions of $f_n$ to $T$ converge pointwise to $f$.
Furthermore,
as limits of sequences of numbers are unique, the function $f$ is unique.

\begin{example}
On $[-1,1]$ the sequence of functions defined by $f_n(x) := x^{2n}$
converges pointwise to $f \colon [-1,1] \to \R$, where
\begin{equation*}
f(x) =
\begin{cases}
1 & \text{if $x=-1$ or $x=1$,} \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
See \figureref{x2nfig}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{x2nfig.eepic}
\caption{Graphs of $f_1$, $f_2$, $f_3$, and $f_8$ for $f_n(x) :=
x^{2n}$.\label{x2nfig}}
%\end{center}
\end{myfigureht}

To see this is so, first take $x \in (-1,1)$.  Then 
$0 \leq x^2 < 1$.
We have seen before that
\begin{equation*}
\abs{x^{2n} - 0} = {(x^2)}^n \to 0 \quad \text{as} \quad n \to \infty .
\end{equation*}
Therefore $\lim\,f_n(x) = 0$.

When $x = 1$ or $x=-1$, then $x^{2n} = 1$ for all $n$ and hence
$\lim\,f_n(x) = 1$.
For all other $x$, the sequence
$\{ f_n(x) \}$ does not converge.
\end{example}

Often, functions are given as a series.  In this case, we use
the notion of pointwise convergence to find the values of the function.

\begin{example} \label{exercise:geomsumptconv}
We write
\begin{equation*}
\sum_{k=0}^\infty x^k
\end{equation*}
to denote the limit of the functions
\begin{equation*}
f_n(x) := \sum_{k=0}^n x^k .
\end{equation*}
When studying series, 
we saw that on $x \in (-1,1)$ the $f_n$ converge pointwise to
\begin{equation*}
\frac{1}{1-x} .
\end{equation*}

The subtle point here is that while
$\frac{1}{1-x}$ is defined for all $x \not=1$, and $f_n$ are 
defined for all $x$ (even at $x=1$), convergence only happens on $(-1,1)$.

Therefore, when we write
\begin{equation*}
f(x) := \sum_{k=0}^\infty x^k
\end{equation*}
we mean that $f$ is defined on $(-1,1)$ and is the pointwise limit
of the partial sums.
\end{example}

\begin{example}
Let $f_n(x) := \sin(xn)$.  Then $f_n$ does not converge pointwise
to any function on any interval.  It may converge at certain points, such
as when $x=0$ or $x=\pi$.  It is left as an exercise that in any interval
$[a,b]$, there exists an $x$ such that $\sin(xn)$ does not have a limit
as $n$ goes to infinity.
\end{example}

Before we move to uniform convergence, let us reformulate pointwise
convergence in a different way.
We leave the proof to the reader, it is a simple application of the
definition of convergence of a sequence of real numbers.

\begin{prop} \label{ptwsconv:prop}
Let $f_n \colon S \to \R$ and $f \colon S \to \R$ be functions.
Then $\{ f_n \}$ converges pointwise to $f$ if and only if
for every $x \in S$, and every $\epsilon > 0$, there exists
an $N \in \N$ such that for all
$n \geq N$ we have
\begin{equation*}
\abs{f_n(x)-f(x)} < \epsilon .
\end{equation*}
\end{prop}

The key point here is that $N$ can depend on $x$, not just on
$\epsilon$.  That is, for each $x$ we can pick a different $N$.
If we can pick one $N$ for all $x$, we have what is called
uniform convergence.

\subsection{Uniform convergence}

\begin{defn}
\index{uniform convergence}
Let $f_n \colon S \to \R$
and $f \colon S \to \R$
be functions.  We say the sequence $\{ f_n \}$
\emph{\myindex{converges uniformly}} to $f$, if for
every $\epsilon > 0$ there exists an $N \in \N$ such that 
for all $n \geq N$ we have
\begin{equation*}
\abs{f_n(x) - f(x)} < \epsilon \qquad \text{for all $x \in S$.}
\end{equation*}
\end{defn}

In uniform convergence $N$ cannot depend on $x$.  Given $\epsilon > 0$
we must find an $N$ that works for all $x \in S$.  See
\figureref{fig:uniformconv} for an illustration.
\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{uniformconv.eepic}
\caption{In uniform convergence,
for $n \geq N$,
the functions $f_n$ are within a strip of $\pm\epsilon$ from $f$.%
\label{fig:uniformconv}}
%\end{center}
\end{myfigureht}

Uniform convergence
implies pointwise convergence, and the proof follows by
\propref{ptwsconv:prop}:

\begin{prop}
Let $\{ f_n \}$ be a sequence of functions $f_n \colon S \to \R$.
If $\{ f_n \}$ converges
uniformly to $f \colon S \to \R$, then $\{ f_n \}$ converges pointwise to $f$.
\end{prop}

The converse does not hold.

\begin{example}
The functions $f_n(x) := x^{2n}$ do not converge uniformly on $[-1,1]$,
even though they converge pointwise.  To see this, suppose for contradiction
that the convergence is uniform.  For $\epsilon := \nicefrac{1}{2}$, there would have
to exist an $N$ such that $x^{2N} = \abs{x^{2N} - 0} < \nicefrac{1}{2}$ for all $x \in
(-1,1)$ (as $f_n(x)$ converges to 0 on $(-1,1)$).  But that means that
for any sequence $\{ x_k \}$ in $(-1,1)$ such that $\lim\, x_k = 1$
we have $x_k^{2N} < \nicefrac{1}{2}$ for all $k$.  On the other hand
$x^{2N}$ is a continuous function of $x$ (it is a polynomial), therefore
we obtain a contradiction
\begin{equation*}
1 = 1^{2N}  = \lim_{k\to\infty} x_k^{2N} \leq \nicefrac{1}{2} .
\end{equation*}

However, if we restrict our domain to $[-a,a]$ where $0 < a < 1$, then
$\{ f_n \}$ converges uniformly to 0 on $[-a,a]$.  First note
that $a^{2n} \to 0$ as $n \to \infty$.  Thus given $\epsilon > 0$,
pick $N \in \N$ such that
$a^{2n} < \epsilon$ for all $n \geq N$.  Then for any $x \in [-a,a]$
we have $\abs{x} \leq a$.  Therefore, for $n \geq N$
\begin{equation*}
\abs{x^{2N}} = \abs{x}^{2N} \leq a^{2N} < \epsilon .
\end{equation*}
\end{example}

\subsection{Convergence in uniform norm}

For bounded functions there is another more abstract way to 
think of uniform convergence.  To every bounded function we assign
a certain nonnegative number (called the uniform norm).  This number
measures the ``distance'' of the function from 0.  We can then ``measure''
how far two functions are from each other.  We then translate
a statement about uniform convergence into a statement about a certain
sequence of real numbers converging to zero.

\begin{defn} \label{def:unifnorm}
Let $f \colon S \to \R$ be a bounded function.  Define
\begin{equation*}
\norm{f}_u :=
\sup \bigl\{ \abs{f(x)} : x \in S \bigr\} .
\end{equation*}
$\norm{\cdot}_u$ is called the \emph{\myindex{uniform norm}}.
\end{defn}

To use this notation%
\footnote{The notation nor terminology is not completely standardized.  The norm is
also called the
\emph{\myindex{sup norm}} or
\emph{\myindex{infinity norm}}, and in addition
to $\norm{f}_u$ and $\norm{f}_S$ it is sometimes written
as $\norm{f}_{\infty}$ or $\norm{f}_{\infty,S}$.}
and this concept, the domain $S$ must be fixed.  Some authors
use
the notation
$\norm{f}_S$ to emphasize the dependence on $S$.

\begin{prop}
A sequence of bounded functions $f_n \colon S \to \R$ converges
uniformly to $f \colon S \to \R$, if and only if
\begin{equation*}
\lim_{n\to\infty} \norm{f_n - f}_u = 0 .
\end{equation*}
\end{prop}

\begin{proof}
First suppose 
$\lim \norm{f_n - f}_u = 0$.  Let $\epsilon > 0$ be
given.  Then there exists an $N$ such that
for $n \geq N$ we have $\norm{f_n - f}_u < \epsilon$.  As $\norm{f_n-f}_u$
is the supremum of $\abs{f_n(x)-f(x)}$, we see that for all $x \in S$
we have $\abs{f_n(x)-f(x)} \leq \norm{f_n - f}_u < \epsilon$.

On the other hand, suppose $\{ f_n \}$ converges uniformly to $f$.
Let $\epsilon > 0$ be given.  Then find $N$ such that 
$\abs{f_n(x)-f(x)} < \epsilon$ for all $x \in S$.
Taking the supremum we see that
$\norm{f_n - f}_u \leq \epsilon$.  Hence $\lim \norm{f_n-f}_u = 0$.
\end{proof}

Sometimes it is said that \emph{$\{ f_n \}$ converges to $f$ in uniform norm}
\index{converges in uniform norm}
\index{uniform norm convergence}
instead of \emph{converges uniformly}.  The proposition
says that the two notions are the same thing.

\begin{example}
Let $f_n \colon [0,1] \to \R$ be defined by $f_n(x) := \frac{nx+ \sin(nx^2)}{n}$.
Then we claim $\{ f_n \}$ converges uniformly to $f(x) := x$.  Let us compute:
\begin{equation*}
\begin{split}
\norm{f_n-f}_u
& =
\sup \left\{ \abs{\frac{nx+ \sin(nx^2)}{n} - x} : x \in [0,1] \right\}
\\
& =
\sup \left\{ \frac{\abs{\sin(nx^2)}}{n} : x \in [0,1] \right\}
\\
& \leq
\sup \{ \nicefrac{1}{n} : x \in [0,1] \}
\\
& = \nicefrac{1}{n}.
\end{split}
\end{equation*}
\end{example}

Using uniform norm, we define Cauchy sequences in a similar way
as we define Cauchy sequences of real numbers.

\begin{defn}
Let $f_n \colon S \to \R$ be bounded functions.
The sequence is \emph{\myindex{Cauchy in the uniform norm}}
or \emph{\myindex{uniformly Cauchy}}
if for every $\epsilon > 0$, there exists an $N \in \N$ such
that for $m,k \geq N$ we have
\begin{equation*}
\norm{f_m-f_k}_u < \epsilon .
\end{equation*}
\end{defn}

\begin{prop} \label{prop:uniformcauchy}
Let $f_n \colon S \to \R$ be bounded functions.
Then $\{ f_n \}$ is Cauchy in the uniform norm if and only if
there exists an $f \colon S \to \R$ and $\{ f_n \}$ converges
uniformly to $f$.
\end{prop}

\begin{proof}
Let us first suppose $\{ f_n \}$ is Cauchy in the uniform norm.
Let us define $f$.  Fix $x$, then
the sequence $\{ f_n(x) \}$ is Cauchy because
\begin{equation*}
\abs{f_m(x)-f_k(x)}
\leq
\norm{f_m-f_k}_u .
\end{equation*}
Thus $\{ f_n(x) \}$ converges to some real number.  Define $f \colon S
\to \R$ by
\begin{equation*}
f(x) := \lim_{n \to \infty} f_n(x) .
\end{equation*}
The sequence
$\{ f_n \}$ converges pointwise to $f$.  To show that the convergence
is uniform, let $\epsilon > 0$ be given.  Find an $N$ such that
for $m, k \geq N$ we have
$\norm{f_m-f_k}_u < \nicefrac{\epsilon}{2}$.  In other words for
all $x$ we have
$\abs{f_m(x)-f_k(x)} < \nicefrac{\epsilon}{2}$.  We take the limit
as $k$ goes to infinity.  Then $\abs{f_m(x)-f_k(x)}$
goes to $\abs{f_m(x)-f(x)}$.
Consequently for all $x$ we get
\begin{equation*}
\abs{f_m(x)-f(x)} \leq \nicefrac{\epsilon}{2} < \epsilon .
\end{equation*}
And hence $\{ f_n \}$ converges uniformly.

For the other direction, suppose $\{ f_n \}$ converges uniformly to
$f$.  Given $\epsilon > 0$, find $N$ such that for all $n \geq N$
we have $\abs{f_n(x)-f(x)} < \nicefrac{\epsilon}{4}$ for all $x \in S$.
Therefore for all $m, k \geq N$ we have
\begin{equation*}
\abs{f_m(x)-f_k(x)} = 
\abs{f_m(x)-f(x)+f(x)-f_k(x)} \leq
\abs{f_m(x)-f(x)}+\abs{f(x)-f_k(x)} < \nicefrac{\epsilon}{4} +
\nicefrac{\epsilon}{4} .
\end{equation*}
Take supremum over all $x$ to obtain
\begin{equation*}
\norm{f_m-f_k}_u \leq \nicefrac{\epsilon}{2} < \epsilon .  \qedhere
\end{equation*}
\end{proof}

\subsection{Exercises}

\begin{exercise}
Let $f$ and $g$ be bounded functions on $[a,b]$.  Prove 
\begin{equation*}
\norm{f+g}_u \leq \norm{f}_u + \norm{g}_u .
\end{equation*}
\end{exercise}

%\pagebreak[2]

\begin{exercise}
a) Find the pointwise limit $\dfrac{e^{x/n}}{n}$ for $x \in \R$. \nopagebreak
\\
b) Is the limit uniform on $\R$?
\\
c) Is the limit uniform on $[0,1]$?
\end{exercise}

\begin{exercise}
Suppose $f_n \colon S \to \R$ are functions that converge uniformly
to $f \colon S \to \R$.  Suppose $A \subset S$.  Show that
the sequence of restrictions $\{ f_n|_A \}$ converges uniformly to $f|_A$.
\end{exercise}

\begin{exercise}
Suppose $\{ f_n \}$ and $\{ g_n \}$ defined on some set $A$ converge to
$f$ and $g$ respectively pointwise.  Show that $\{ f_n+g_n \}$ converges
pointwise to $f+g$.
\end{exercise}

\begin{exercise}
Suppose $\{ f_n \}$ and $\{ g_n \}$ defined on some set $A$ converge to
$f$ and $g$ respectively uniformly on $A$.  Show that $\{ f_n+g_n \}$
converges uniformly to $f+g$ on $A$.
\end{exercise}

\begin{exercise}
Find an example of a sequence of functions $\{ f_n \}$ and $\{ g_n \}$
that converge uniformly to some $f$ and $g$ on some set $A$, but such that
$\{ f_ng_n \}$ (the multiple) does not converge uniformly to $fg$ on $A$.
Hint: Let $A := \R$, let $f(x):=g(x) := x$.  You can even pick $f_n = g_n$.
\end{exercise}

\begin{exercise}
Suppose there exists a sequence of functions $\{ g_n \}$ uniformly
converging to $0$ on $A$.  Now suppose we have a sequence of functions
$\{ f_n \}$ and a function $f$ on $A$ such that
\begin{equation*}
\abs{f_n(x) - f(x)} \leq g_n(x) 
\end{equation*}
for all $x \in A$.  Show that $\{ f_n \}$ converges uniformly to $f$ on $A$.
\end{exercise}

\begin{exercise}
Let $\{ f_n \}$, $\{ g_n \}$ and $\{ h_n \}$ be sequences of functions on
$[a,b]$.  Suppose $\{ f_n \}$ and $\{ h_n \}$ converge uniformly to some function
$f \colon [a,b] \to \R$ and suppose $f_n(x) \leq g_n(x) \leq h_n(x)$
for all $x \in [a,b]$.  Show that $\{ g_n \}$ converges uniformly to $f$.
\end{exercise}

\begin{exercise}
Let $f_n \colon [0,1] \to \R$ be a sequence of increasing functions (that
is, $f_n(x) \geq f_n(y)$ whenever $x \geq y$).  Suppose $f_n(0) = 0$
and $\lim\limits_{n \to \infty} f_n(1) = 0$.  Show that
$\{ f_n \}$
converges uniformly to $0$.
\end{exercise}

\begin{exercise}
Let $\{f_n\}$ be a sequence of functions defined on $[0,1]$.
Suppose there exists a sequence of distinct numbers $x_n \in [0,1]$ such that
\begin{equation*}
f_n(x_n) = 1 .
\end{equation*}
Prove or disprove the following statements:
\\
a)
True or false: There exists $\{ f_n \}$ as above that converges to $0$
pointwise.
\\
b)
True or false: There exists $\{ f_n \}$ as above that converges to $0$
uniformly on $[0,1]$.
\end{exercise}

\begin{exercise}
Fix a continuous $h \colon [a,b] \to \R$.
Let $f(x) := h(x)$ for $x \in [a,b]$,
$f(x) := h(a)$ for $x < a$ and $f(x) := h(b)$ for all $x > b$.  First show
that $f \colon \R \to \R$ is continuous.
Now let $f_n$ be
the function $g$ from \exerciseref{exercise:smoothingout} with
$\epsilon = \nicefrac{1}{n}$, defined on the interval $[a,b]$.  That is,
\begin{equation*}
f_n(x) := \frac{n}{2} \int_{x-1/n}^{x+1/n} f .
\end{equation*}
Show that $\{ f_n \}$ converges uniformly to $h$ on $[a,b]$.
\end{exercise}


\begin{exercise}
Prove that
if $f_n \colon S \to \R$
converge uniformly to a bounded function $f \colon S \to \R$,
then there exists an $N$ such that for all $n \geq N$, the $f_n$
are bounded.
\end{exercise}

\begin{exercise}
Suppose there is a single constant $B$ and
a sequence of functions
$f_n \colon S \to \R$ that are bounded by $B$,
that is $\abs{f_n(x)} \leq B$ for all $x \in S$.
Suppose that $\{ f_n \}$ converges pointwise
to $f \colon S \to \R$.
Prove that $f$ is bounded.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:moreonseries}]
In \exampleref{exercise:geomsumptconv} we saw
$\sum_{k=0}^\infty x^k$ converges pointwise to $\frac{1}{1-x}$ on
$(-1,1)$.\\
a) Show that for any $0 \leq c < 1$, the series
$\sum_{k=0}^\infty x^k$ converges uniformly on $[-c,c]$.
\\
b) Show that the series $\sum_{k=0}^\infty x^k$ does not converge uniformly
on $(-1,1)$.
\end{exercise}

%FIXME: we're redoing much more than this in the next section
%\begin{exnote}
%In fact, all power series have this property of uniform convergence:
%\end{exnote}

%\begin{exercise}[requires \sectionref{sec:moreonseries}]
%Suppose that $0 < R < \infty$ is the radius of convergence of a
%power series $\sum_{k=0}^\infty c_k {(x-a)}^k$.
%Prove that for any $\delta > 0$, the
%series converges uniformly on $[a-R+\delta,a+R-\delta]$.
%Hint: Recall that on $(a-R,a+R)$ the series converges absolutely.
%\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Interchange of limits}
\label{sec:liminter}

\sectionnotes{1--2.5 lectures,
subsections on derivatives and power series (which
requires \sectionref{sec:moreonseries}) optional.}

Large parts of modern analysis deal mainly with the question of the
interchange of two limiting operations.  When
we have a chain of two limits, we cannot always just swap the limits.
For example,
\begin{equation*}
0 = 
\lim_{n\to\infty}
\left(
\lim_{k\to\infty}
\frac{\nicefrac{n}{k}}{\nicefrac{n}{k} + 1}
\right)
\not=
\lim_{k\to\infty}
\left(
\lim_{n\to\infty}
\frac{\nicefrac{n}{k}}{\nicefrac{n}{k} + 1}
\right)
= 1 .
\end{equation*}

When talking about sequences of functions, interchange of limits comes up
quite often.  We treat two cases.  First we look at continuity of
the limit, and second we look at the integral of the limit.

\subsection{Continuity of the limit}

If we have a sequence $\{ f_n \}$ of continuous functions, is the limit continuous?
Suppose $f$ is the (pointwise) limit of $\{ f_n \}$.
If $\lim\, x_k = x$
we are interested in the following
interchange of limits.  The equality we have to prove (it is not always true)
is marked with a question mark.  In fact the limits to the left
of the question mark might not even exist.
\begin{equation*}
\lim_{k \to \infty} 
f(x_k)
=
\lim_{k \to \infty} 
\Bigl(
\lim_{n \to \infty} f_n(x_k)
\Bigr)
\overset{\text{\textbf{?}}}{=}
\lim_{n \to \infty}
\Bigl(
\lim_{k \to \infty} 
f_n(x_k)
\Bigr)
=
\lim_{n \to \infty}
f_n(x)
=
f(x) .
\end{equation*}
In particular, we wish to find conditions on the sequence $\{ f_n \}$
so that the above equation holds.
It turns out that if we only require pointwise convergence, then the limit
of a sequence of functions need not be continuous, and the above equation
need not hold.

\begin{example}
Let $f_n \colon [0,1] \to \R$
be defined as
\begin{equation*}
f_n(x) :=
\begin{cases}
1-nx &  \text{if $x < \nicefrac{1}{n}$,}\\
0 &  \text{if $x \geq \nicefrac{1}{n}$.}
\end{cases}
\end{equation*}
See \figureref{contconvcntr:fig}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{contconvcntr.eepic}
\caption{Graph of $f_n(x)$.%
\label{contconvcntr:fig}}
%\end{center}
\end{myfigureht}

Each function $f_n$ is continuous.
Fix an $x \in (0,1]$.  If $n \geq \nicefrac{1}{x}$,
then $x \geq \nicefrac{1}{n}$.  Therefore for $n \geq \nicefrac{1}{x}$
we have $f_n(x) = 0$, and so
\begin{equation*}
\lim_{n \to \infty} f_n(x) = 0.
\end{equation*}
On the other hand if $x=0$, then
\begin{equation*}
\lim_{n \to \infty} f_n(0) = 
\lim_{n \to \infty} 1 = 1.
\end{equation*}
Thus the pointwise limit of $f_n$ is the function
$f \colon [0,1] \to \R$ defined by
\begin{equation*}
f(x) :=
\begin{cases}
1 &  \text{if $x = 0$,}\\
0 &  \text{if $x > 0$.}
\end{cases}
\end{equation*}
The function $f$ is not continuous at 0.
\end{example}

If we, however, require the convergence to be uniform, the limits can
be interchanged.

\begin{thm}
Let $\{ f_n \}$ be 
a sequence of continuous functions $f_n \colon S \to \R$ converging
uniformly to  $f \colon S \to \R$.  Then $f$ is continuous.
\end{thm}

\begin{proof}
Let $x \in S$ be fixed.  Let $\{ x_n \}$ be a sequence in $S$
converging to $x$.

Let $\epsilon > 0$ be given.
As $\{ f_k \}$ converges uniformly to $f$, we find a $k \in \N$ such that
\begin{equation*}
\abs{f_k(y)-f(y)} < \nicefrac{\epsilon}{3}
\end{equation*}
for all $y \in S$.  As $f_k$ is continuous at $x$,
we find an $N \in \N$ such that for $m \geq N$
we have 
\begin{equation*}
\abs{f_k(x_m)-f_k(x)} < \nicefrac{\epsilon}{3} .
\end{equation*}
Thus for
$m \geq N$ we have
\begin{equation*}
\begin{split}
\abs{f(x_m)-f(x)}
& =
\abs{f(x_m)-f_k(x_m)+f_k(x_m)-f_k(x)+f_k(x)-f(x)}
\\
& \leq
\abs{f(x_m)-f_k(x_m)}+
\abs{f_k(x_m)-f_k(x)}+
\abs{f_k(x)-f(x)}
\\
& <
\nicefrac{\epsilon}{3} +
\nicefrac{\epsilon}{3} +
\nicefrac{\epsilon}{3} = \epsilon .
\end{split}
\end{equation*}
Therefore $\{ f(x_m) \}$ converges to $f(x)$ and hence $f$ is continuous at
$x$.  As $x$ was arbitrary, $f$ is continuous everywhere.
\end{proof}

\subsection{Integral of the limit}

Again, if we simply require pointwise convergence, then the integral
of a limit of a sequence of functions need not be equal to the limit
of the integrals.

\begin{example}
Let $f_n \colon [0,1] \to \R$
be defined as
\begin{equation*}
f_n(x) :=
\begin{cases}
0 &  \text{if $x = 0$,}\\
n-n^2x &  \text{if $0 < x < \nicefrac{1}{n}$,}\\
0 &  \text{if $x \geq \nicefrac{1}{n}$.}
\end{cases}
\end{equation*}
See \figureref{intconvcntr:fig}.

\begin{myfigureht}
%\begin{center}
% To get fonts right we have to use .eepic not .pdf
\subimport*{figures/}{intconvcntr.eepic}
\caption{Graph of $f_n(x)$.%
\label{intconvcntr:fig}}
%\end{center}
\end{myfigureht}

Each $f_n$ is Riemann integrable (it is continuous on $(0,1]$ and bounded),
and it is easy to see
\begin{equation*}
\int_0^1 f_n =
\int_0^{\nicefrac{1}{n}} (n-n^2x)~dx = \nicefrac{1}{2} .
\end{equation*}
Let us compute the pointwise limit of $\{ f_n \}$.
Fix an $x \in (0,1]$.  For $n \geq \nicefrac{1}{x}$
we have $x \geq \nicefrac{1}{n}$ and so $f_n(x) = 0$.  Therefore
\begin{equation*}
\lim_{n \to \infty} f_n(x) = 0.
\end{equation*}
We also have $f_n(0) = 0$ for all $n$.  Therefore the pointwise
limit of $\{ f_n \}$ is the zero function.  Thus
\begin{equation*}
\nicefrac{1}{2} =
\lim_{n\to\infty}
\int_0^1 f_n (x)~dx
\not=
\int_0^1
\left(
\lim_{n\to\infty}
f_n(x)\right)~dx
=
\int_0^1 0~dx = 0 .
\end{equation*}
\end{example}

But
if we again require the convergence to be uniform, the limits can
be interchanged.

\begin{thm} \label{integralinterchange:thm}
Let $\{ f_n \}$ be a sequence of Riemann integrable
functions
$f_n \colon [a,b] \to \R$
converging uniformly to $f \colon [a,b]
\to \R$.  Then $f$ is Riemann integrable and
\begin{equation*}
\int_a^b f = \lim_{n\to\infty} \int_a^b f_n .
\end{equation*}
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given.
As $f_n$ goes to $f$ uniformly, we find an $M \in \N$ such that
for all $n \geq M$ we have 
$\abs{f_n(x)-f(x)} < \frac{\epsilon}{2(b-a)}$ for all $x \in [a,b]$.
In particular, by reverse triangle inequality
$\abs{f(x)} < \frac{\epsilon}{2(b-a)} + \abs{f_n(x)}$ for all $x$,
hence $f$ is bounded
as $f_n$ is bounded.
Note that $f_n$ is integrable and compute
\begin{equation*}
\begin{split}
\overline{\int_a^b} f
-
\underline{\int_a^b} f
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) + f_n(x) \bigr)~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) + f_n(x) \bigr)~dx
\\
& \leq
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx +  \overline{\int_a^b} f_n(x) ~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx -  \underline{\int_a^b} f_n(x) ~dx
\\
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx +  \int_a^b f_n(x) ~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx -  \int_a^b f_n(x) ~dx
\\
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx
\\
& \leq
\frac{\epsilon}{2(b-a)} (b-a) + 
\frac{\epsilon}{2(b-a)} (b-a) = \epsilon .
\end{split}
\end{equation*}
The first inequality is \propref{prop:upperlowerlinineq}.
The second inequality follows from \propref{intulbound:prop} and 
the fact that for all $x \in [a,b]$ we have
$\frac{-\epsilon}{2(b-a)} < f(x)-f_n(x) < \frac{\epsilon}{2(b-a)}$.
As $\epsilon > 0$ was arbitrary, $f$ is Riemann integrable.

Finally we compute $\int_a^b f$.  We apply \propref{intbound:prop}
in the calculation.  Again, for $n \geq M$ (where $M$ is the same as above) we have
\begin{equation*}
\begin{split}
\abs{\int_a^b f - \int_a^b f_n} & = 
\abs{ \int_a^b \bigl(f(x) - f_n(x)\bigr)~dx}
\\
& \leq
\frac{\epsilon}{2(b-a)} (b-a) = \frac{\epsilon}{2} < \epsilon .
\end{split}
\end{equation*}
Therefore $\{ \int_a^b f_n \}$ converges to $\int_a^b f$.
\end{proof}

\begin{example}
Suppose we wish to compute
\begin{equation*}
\lim_{n\to\infty} \int_0^1 \frac{nx+ \sin(nx^2)}{n} ~dx .
\end{equation*}
It is impossible to compute the integrals for any particular $n$ using 
calculus as $\sin(nx^2)$ has no closed-form antiderivative.  However,
we can compute the limit.
We have shown before that $\frac{nx+ \sin(nx^2)}{n}$ converges uniformly
on $[0,1]$ to $x$.
By \thmref{integralinterchange:thm}, the limit exists and
\begin{equation*}
\lim_{n\to\infty} \int_0^1 \frac{nx+ \sin(nx^2)}{n} ~dx
=
\int_0^1
x ~dx = \nicefrac{1}{2} .
\end{equation*}
\end{example}

\begin{example}
If convergence is only pointwise, the limit need not even be Riemann
integrable.  On $[0,1]$ define
\begin{equation*}
f_n(x) :=
\begin{cases}
1 & \text{if $x = \nicefrac{p}{q}$ in lowest terms and $q \leq n$,} \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
The function $f_n$ differs from the zero function at finitely many points;
there are only finitely many fractions in $[0,1]$ with denominator less than
or equal to $n$.   So $f_n$ is integrable and $\int_0^1 f_n = \int_0^1 0 =
0$.  It is an easy exercise to show that $\{ f_n \}$ converges pointwise to the
Dirichlet function
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{if $x \in \Q$,} \\
0 & \text{otherwise,}
\end{cases}
\end{equation*}
which is not Riemann integrable.
\end{example}

\begin{example}
In fact, if the convergence is only pointwise, the limit of bounded
functions is not even necessarily bounded.
Define $f_n \colon [0,1] \to \R$ by
\begin{equation*}
f_n(x) :=
\begin{cases}
0 & \text{ if $x < \nicefrac{1}{n}$,}\\
\nicefrac{1}{x} & \text{ else.}
\end{cases}
\end{equation*}
For every $n$ we get that $\abs{f_n(x)} \leq n$ for all $x \in [0,1]$ so the
functions are bounded.  However $f_n$ converge pointwise to
\begin{equation*}
f(x) :=
\begin{cases}
0 & \text{ if $x = 0$,}\\
\nicefrac{1}{x} & \text{ else,}
\end{cases}
\end{equation*}
which is unbounded.
\end{example}

\subsection{Derivative of the limit}

While uniform convergence is enough to swap limits with integrals, it is not,
however, enough to swap limits with derivatives, unless you also have
uniform convergence of the derivatives themselves.
See the exercises for some counterexamples.  Using 
the fundamental theorem of calculus we find an answer for continuously
differentiable functions.  The following theorem is true even if 
we do not assume continuity of the derivatives, but the proof is more
difficult.

\begin{thm} \label{thm:dersconverge}
Let $I$ be a bounded interval and let
$f_n \colon I \to \R$ be continuously differentiable functions.
Suppose $\{ f_n' \}$ converges uniformly to $g \colon I \to \R$,
and suppose $\{ f_n(c) \}_{n=1}^\infty$ is a
convergent sequence for some $c \in I$.  Then $\{ f_n \}$ converges uniformly to 
a continuously differentiable function $f \colon I \to \R$, and $f' = g$.
\end{thm}

\begin{proof}
Define $f(c) := \lim_{n\to \infty} f_n(c)$.
As $f_n'$ are continuous and hence Riemann integrable,
then
via the fundamental theorem of calculus, we find that for $x \in I$,
\begin{equation*}
f_n(x) = f_n(c) + \int_c^x f_n' .
\end{equation*}
As $\{ f_n' \}$ converges uniformly on $I$, it converges uniformly
on $[c,x]$ (or $[x,c]$ if $x < c$).
Therefore, we find that the limit on the right hand side exists.
Let us define $f$ at the remaining points by this limit:
\begin{equation*}
f(x) :=
\lim_{n\to\infty} f_n(c) + \lim_{n\to\infty} \int_c^x f_n'
=
f(c) + \int_c^x g .
\end{equation*}
The function $g$ is continuous, being the uniform limit of continuous
functions.  Hence $f$ is differentiable and $f'(x) = g(x)$ for all $x \in I$
by the second form of the fundamental theorem.

It remains to prove
uniform convergence.
Suppose $I$ has a lower bound $a$ and upper bound $b$.
Let $\epsilon > 0$ be given.  Take $M$
such that for $n \geq M$ we have
$\abs{f(c)-f_n(c)} < \nicefrac{\epsilon}{2}$,
and
$\abs{g(x)-f_n'(x)} < \nicefrac{\epsilon}{2(b-a)}$
for all $x \in I$.  Then
\begin{equation*}
\begin{split}
\abs{f(x) - f_n(x)} & =
\abs{f(c) + \int_c^x g - f_n(c) - \int_c^x f_n'}
\\
& \leq
\abs{f(c) - f_n(c)} + \abs{\int_c^x g - \int_c^x f_n'}
\\
& =
\abs{f(c) - f_n(c)} + \abs{\int_c^x \bigl(g(s) - f_n'(s)\bigr) \, ds}
\\
& <
\frac{\epsilon}{2}
+
\frac{\epsilon}{2(b-a)}
(b-a)
=\epsilon. \qedhere
\end{split}
\end{equation*}
\end{proof}

The proof goes through without boundedness of $I$, except for the
uniform convergence of $f_n$ to $f$.  As an example suppose $I = \R$ and let
$f_n(x) := \nicefrac{x}{n}$.  Then $f_n'(x)=\nicefrac{1}{n}$, which
converges uniformly to $0$.  However, $\{f_n\}$ converges to 0 only pointwise.

\subsection{Convergence of power series}

In \sectionref{sec:moreonseries} we saw that a power series converges
absolutely inside its radius of convergence.
By definition, it converges pointwise.
Let us show that it also converges uniformly.  This fact will allow us to
swap several types of limits.  Not only is the limit continuous,
we can differentiate
and integrate convergent power series term by term.

\begin{prop}
Let $\sum_{n=0}^\infty c_n {(x-a)}^n$ be a convergent power series with a radius
of convergence $0 < \rho \leq \infty$.
Then the series converges uniformly
in $[a-r,a+r]$ for any $0 < r < \rho$.

In particular, the series defines a continuous function
on $(a-\rho,a+\rho)$ (if $\rho < \infty$), or $\R$ (if $\rho = \infty$).
\end{prop}

\begin{proof}
Let $I := (a-\rho,a+\rho)$ if $\rho < \infty$,
or let $I := \R$ if $\rho= \infty$.
Take $0 < r < \rho$.
The series converges absolutely for any $x \in I$,
in particular if $x = a+r$.
Therefore $\sum_{n=0}^\infty \abs{c_n} r^n$ converges.
Given $\epsilon >0$, find $M$ such that for all $k \geq M$,
\begin{equation*}
\sum_{n=k+1}^\infty \abs{c_n} {r}^n < \epsilon .
\end{equation*}
For any $x \in [a-r,a+r]$ and any $m > k$
\begin{multline*}
\abs{\sum_{n=0}^m c_n {(x-a)}^n - 
\sum_{n=0}^k c_n {(x-a)}^n}
=
\abs{\sum_{n=k+1}^m c_n {(x-a)}^n}
\\
\leq
\sum_{n=k+1}^m \abs{c_n} {\abs{x-a}}^n
\leq
\sum_{n=k+1}^m \abs{c_n} {r}^n
\leq
\sum_{n=k+1}^\infty \abs{c_n} {r}^n
<\epsilon.
\end{multline*}
The partial sums are therefore uniformly Cauchy on $[a-r,a+r]$ and
hence converge uniformly on that set.

Moreover, the partial sums are polynomials, which are
continuous, and their uniform limit on $[a-r,a+r]$
is a continuous function.
As $r < \rho$ was arbitrary, the limit function
is continuous on all of $I$.
\end{proof}

As we said, we will show that
power series can be differentiated and integrated
term by term.  The 
differentiated or integrated series is again a power series,
and we will show it has the same radius of convergence.
Therefore, 
any power series defines an infinitely differentiable function.

We first prove that we can antidifferentiate, as integration only needs
uniform limits.

\begin{cor}
Let $\sum_{n=0}^\infty c_n {(x-a)}^n$ be a convergent power series with a radius
of convergence $0 < \rho \leq \infty$.
Let $I := (a-\rho,a+\rho)$ if $\rho < \infty$
or $I := \R$ if $\rho= \infty$.  Let $f \colon I \to \R$ be the limit.
Then
\begin{equation*}
\int_a^x f = \sum_{n=1}^\infty \frac{c_{n-1}}{n} {(x-a)}^{n} ,
\end{equation*}
where the radius of convergence of this series is at least $\rho$.
\end{cor}

\begin{proof}
Take $0 < r < \rho$.
The partial sums $\sum_{n=0}^k c_n {(x-a)}^n$ converge uniformly on $[a-r,a+r]$.
For any fixed $x \in [a-r,a+r]$, the convergence is also uniform
on $[a,x]$ (or $[x,a]$ if $x < a$).
%  The function $f$,
%being the uniform limit of $\{ f_k \}$ is continuous and hence integrable.
Hence,
\begin{equation*}
\int_a^x f =
\int_a^x \lim_{k\to\infty} \sum_{n=0}^k c_n {(s-a)}^n \, ds
=
\lim_{k\to\infty}
\int_a^x \sum_{n=0}^k c_n {(s-a)}^n \, ds
=
\lim_{k\to\infty}
\sum_{n=1}^k \frac{c_{n-1}}{n} {(x-a)}^{n} . \qedhere
\end{equation*}
\end{proof}


\begin{cor}
Let $\sum_{n=0}^\infty c_n {(x-a)}^n$ be a convergent power series with a radius
of convergence $0 < \rho \leq \infty$.
Let $I := (a-\rho,a+\rho)$ if $\rho < \infty$
or $I := \R$ if $\rho= \infty$.  Let $f \colon I \to \R$ be the limit.
Then $f$ is a differentiable function, and
\begin{equation*}
f'(x) = \sum_{n=0}^\infty (n+1) c_{n+1} {(x-a)}^{n} ,
\end{equation*}
where the radius of convergence of this series is $\rho$.
\end{cor}

\begin{proof}
Take $0 < r < \rho$.
We have uniform convergence of the series on $[a-r,a+r]$,
but we need uniform convergence of the derivative.
Let
\begin{equation*}
R := \limsup_{n \to \infty} \abs{c_n}^{1/n} .
\end{equation*}
As the series is convergent $R < \infty$, and
the radius of convergence is $\nicefrac{1}{R}$ (or $\infty$ if $R=0$).

Let $\epsilon > 0$ be given.  In \exampleref{example:nto1overn} 
we saw $\lim\,n^{1/n} = 1$.
Hence there exists an $N$ such that for all $n \geq N$, we have
$n^{1/n} < 1+\epsilon$.

So
\begin{equation*}
R = 
\limsup_{n \to \infty}
\abs{c_n}^{1/n}
\leq
\limsup_{n \to \infty}
\abs{n c_n}^{1/n}
\leq
(1+\epsilon)
\limsup_{n \to \infty}
\abs{c_n}^{1/n}
=
(1+\epsilon)R .
\end{equation*}
As $\epsilon$ was arbitrary, $\limsup_{n \to \infty} \abs{n c_n}^{1/n} = R$.
Therefore, $\sum_{n=1}^\infty n c_{n} {(x-a)}^{n}$ has radius of
convergence $\rho$, and by dividing by $(x-a)$ we find
$\sum_{n=0}^\infty (n+1) c_{n+1} {(x-a)}^{n}$ has radius of convergence
$\rho$ as well.

Consequently, the partial sums 
$\sum_{n=0}^k (n+1) c_{n+1} {(x-a)}^{n}$,
which are derivatives of the partial sums
$\sum_{n=0}^{k+1} c_{n} {(x-a)}^{n}$,
converge uniformly on $[a-r,a+r]$.  Furthermore,
the series clearly converges at $x=a$.
We may thus apply \thmref{thm:dersconverge}, and
we are done as $r < \rho$ was arbitrary.
\end{proof}

\begin{example}
We could have used this result to define the exponential function.  That is
the power series
\begin{equation*}
f(x) := \sum_{n=0}^\infty \frac{x^n}{n!}
\end{equation*}
has radius of convergence $\rho=\infty$.  By differentiating
term by term we find that $f'(x) = f(x)$.
\end{example}

\begin{example}
The series
\begin{equation*}
\sum_{n=1}^\infty n x^n
\end{equation*}
converges to $\frac{x}{{(1-x)}^2}$ on $(-1,1)$.

Proof:
We know that on $(-1,1)$, $\sum_{n=0}^\infty x^n$ converges to
$\frac{1}{1-x}$.  The derivative $\sum_{n=1}^\infty n x^{n-1}$ then converges
on the same interval to $\frac{1}{{(1-x)}^2}$.  Multiplying by $x$
we obtain the result.
\end{example}

\subsection{Exercises}

\begin{exercise}
While uniform convergence preserves continuity, it does not preserve
differentiability.  Find an explicit example of a sequence of
differentiable functions on $[-1,1]$ that converge uniformly to
a function $f$ such that $f$ is not differentiable.
Hint:
There are many possibilities,
simplest is perhaps to combine $\abs{x}$ and $\frac{n}{2}x^2 +
\frac{1}{2n}$, another is to
consider $\sqrt{x^2+{(\nicefrac{1}{n})}^2}$.  Show that these functions are differentiable,
converge uniformly, and then show that the limit is not differentiable.
\end{exercise}

\begin{exercise}
Let $f_n(x) = \frac{x^n}{n}$.  Show that $\{ f_n \}$ converges uniformly to
a differentiable function $f$ on $[0,1]$ (find $f$).  However, show that
$f'(1) \not= \lim\limits_{n\to\infty} f_n'(1)$.
\end{exercise}

\begin{exnote}
Note: The previous two exercises show that
we cannot simply swap limits with derivatives, even if the convergence is
uniform.  See also \exerciseref{c1uniflim:exercise} below.
\end{exnote}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be a Riemann integrable (hence bounded)
function.  Find
$\displaystyle \lim_{n\to\infty} \int_0^1 \frac{f(x)}{n} ~dx$.
\end{exercise}

\begin{exercise}
Show
$\displaystyle \lim_{n\to\infty} \int_1^2 e^{-nx^2} ~dx = 0$.  Feel free to
use
what you know about the exponential function from calculus.
\end{exercise}

\begin{exercise}
Find an example of a sequence of continuous functions on $(0,1)$ that converges 
pointwise to a continuous function on $(0,1)$, but the convergence is not
uniform.
\end{exercise}

\begin{exnote}
Note: In the previous exercise, $(0,1)$ was picked for simplicity.  For a
more challenging exercise, replace $(0,1)$ with $[0,1]$.
\end{exnote}

\begin{exercise}
True/False; prove or find a counterexample to the following statement:
If $\{ f_n \}$ is a sequence of everywhere discontinuous functions on $[0,1]$
that converge uniformly to a function $f$, then $f$ is everywhere
discontinuous.
\end{exercise}

\begin{exercise} \label{c1uniflim:exercise}
For a continuously differentiable function $f \colon [a,b] \to \R$, define
\begin{equation*}
\norm{f}_{C^1} := \norm{f}_u + \norm{f'}_u .
\end{equation*}
Suppose $\{ f_n \}$ is a sequence of continuously differentiable
functions such that for every $\epsilon >0$, there exists an $M$
such that for all $n,k \geq M$ we have
\begin{equation*}
\norm{f_n-f_k}_{C^1} < \epsilon .
\end{equation*}
Show that $\{ f_n \}$ converges uniformly to some continuously differentiable
function $f \colon [a,b] \to \R$.
\end{exercise}

\begin{exnote}
For the following two exercises let us define for a Riemann integrable
function $f \colon [0,1] \to
\R$ the following number
\begin{equation*}
\norm{f}_{L^1} := 
\int_0^1 \abs{f(x)}~dx .
\end{equation*}
It is true that $\abs{f}$ is integrable whenever $f$ is, see
\exerciseref{exercise:hardabsint}.
This norm defines another very common type of
convergence called the $L^1$-convergence, that is however a bit more
subtle.
\end{exnote}

\begin{exercise}
Suppose $\{ f_n \}$ is a sequence of Riemann integrable functions on $[0,1]$
that converges uniformly
to $0$.  Show that
\begin{equation*}
\lim_{n\to\infty} \norm{f_n}_{L^1} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Find a sequence of Riemann integrable functions 
$\{ f_n \}$ on $[0,1]$ that converges pointwise to $0$, but
\begin{equation*}
\lim_{n\to\infty} \norm{f_n}_{L^1} \text{ does not exist (is $\infty$).}
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Prove \emph{\myindex{Dini's theorem}}:
Let $f_n \colon [a,b] \to \R$ be a sequence of continuous functions such that
\begin{equation*}
0 \leq f_{n+1}(x) \leq f_n(x) \leq \cdots \leq f_1(x) 
\qquad \text{for all $n \in \N$.}
\end{equation*}
Suppose $\{ f_n \}$ converges pointwise to $0$.
Show that $\{ f_n \}$ converges to zero uniformly.
\end{exercise}

\begin{exercise}
Suppose $f_n \colon [a,b] \to \R$ is a sequence of continuous
functions that
converges pointwise
to a continuous $f \colon [a,b] \to \R$.  Suppose that
for any $x \in [a,b]$ the sequence $\{ \abs{f_n(x)-f(x)} \}$ is monotone.
Show that the sequence $\{f_n\}$ converges uniformly.
\end{exercise}

\begin{exercise}
Find a sequence of Riemann integrable functions $f_n \colon [0,1] \to \R$ such
that $\{ f_n \}$ converges to zero pointwise, and such that
a) $\bigl\{ \int_0^1 f_n \bigr\}_{n=1}^\infty$ increases without bound,
b) $\bigl\{ \int_0^1 f_n \bigr\}_{n=1}^\infty$ is the sequence $-1,1,-1,1,-1,1, \ldots$.
\end{exercise}

\begin{exnote}
It is possible to define a 
\emph{\myindex{joint limit}} of a double sequence $\{ x_{n,m} \}$ of real
numbers (that is a function from $\N \times \N$ to $\R$).
We say $L$ is the joint limit of $\{ x_{n,m} \}$ and write
\begin{equation*}
\lim_{\substack{n\to\infty\\m\to\infty}}
x_{n,m} = L ,
\qquad
\text{or}
\qquad
\lim_{(n,m) \to \infty}
x_{n,m} = L ,
\end{equation*}
if for every $\epsilon > 0$, there
exists an $M$ such that if $n \geq M$ and $m \geq M$, then
$\abs{x_{n,m} - L} < \epsilon$.
\end{exnote}

\begin{exercise}
Suppose the joint limit of $\{ x_{n,m} \}$ is $L$, and suppose
that for all $n$, $\lim\limits_{m \to \infty} x_{n,m}$ exists,
and for all $m$, $\lim\limits_{n \to \infty} x_{n,m}$ exists.  Then show
$\lim\limits_{n\to\infty}\lim\limits_{m \to \infty} x_{n,m}
=
\lim\limits_{m\to\infty}\lim\limits_{n \to \infty} x_{n,m} = L$.
\end{exercise}

\begin{exercise}
A joint limit does not mean the iterated limits even exist.
Consider $x_{n,m} := \frac{{(-1)}^{n+m}}{\min \{n,m \}}$.\\
a) Show that for no $n$ does
$\lim\limits_{m \to \infty} x_{n,m}$ exist, and for no $m$
does 
$\lim\limits_{n \to \infty} x_{n,m}$ exist.  So neither
$\lim\limits_{n\to\infty}\lim\limits_{m \to \infty} x_{n,m}$ nor
$\lim\limits_{m\to\infty}\lim\limits_{n \to \infty} x_{n,m}$ makes any sense
at all.\\
b) Show that the joint limit of $\{ x_{n,m} \}$ exists and is 0.
\end{exercise}

\begin{exercise}
We say that a sequence of functions $f_n \colon \R \to \R$ converges
in the \emph{\myindex{compact-open topology}} if for every $k \in \N$,
the sequence $\{ f_n \}$ converges uniformly on $[-k,k]$.
\\
a) Prove that if $f_n \colon \R \to \R$ is a sequence of
continuous functions converging in the compact-open topology, then
the limit is continuous.
\\
b) 
Prove that if $f_n \colon \R \to \R$ is a sequence of
functions Riemann integrable on any closed and bounded interval,
and converging in the compact-open topology to an $f \colon \R \to \R$,
then for any interval $[a,b]$, we have $f \in \sR[a,b]$, and
$\int_a^b f = \lim_{n\to\infty} \int_a^b f_n$.
\end{exercise}

\begin{exercise}[Challening]
Find a sequence of continuous functions $f_n \colon [0,1] \to \R$ that
converge to the popcorn function $f \colon [0,1] \to \R$, that is the
function such that $f(\nicefrac{p}{q}) := \frac{1}{q}$ (if $\nicefrac{p}{q}$
is in lowest terms) and $f(x) := 0$ if $x$ is not rational (note
that $f(0) = f(1) = 1$),
see \exampleref{popcornfunction:example}.
So a pointwise limit of continuous functions can have a dense set of
discontinuities.  See also the next exercise.
\end{exercise}

\begin{exercise}[Challening]
The Dirichlet function
$f \colon [0,1] \to \R$, that is the
function such that $f(x) := 1$ if $x \in \Q$
and $f(x) := 0$ if $x \notin \Q$,
is not the pointwise limit of
continuous functions, although this is difficult to show.
Prove, however, that $f$ is a pointwise limit of functions that are themselves
pointwise limits of
continuous functions themselves.
\end{exercise}

\begin{exercise}
a) Find a sequence of Lipschitz continuous functions on $[0,1]$
whose uniform limit is $\sqrt{x}$, which is a non-Lipschitz function.
b) On the other hand, show that if $f_n \colon S \to \R$ are Lipschitz
with a uniform constant $L$ (meaning all of them satisfy the definition
with the same constant) that converge pointwise to $f \colon S \to \R$,
then the limit $f$ is a Lipschitz continuous function
with Lipschitz constant $L$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:moreonseries}]
If $\sum_{n=0}^\infty c_n {(x-a)}^n$ has radius of convergence $\rho$,
show that the term by term integral
$\sum_{n=1}^\infty \frac{c_{n-1}}{n} {(x-a)}^n$ has radius of convergence
$\rho$.  Note that we only proved above that the radius of convergence was
at least $\rho$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:moreonseries} and \sectionref{sec:taylor}]
Suppose $f(x) := \sum_{n=0}^\infty c_n {(x-a)}^n$ converges in $(a-\rho,a+\rho)$.
\\
a)
Suppose that $f^{(k)}(a) = 0$ for all $k=0,1,2,3,\ldots$.  Prove that
$c_n = 0$ for all $n$, or in other words, 
$f(x) = 0$ for all $x \in (a-\rho,a+\rho)$.
\\
b) Using part a) prove a version of the so called ``identity
theorem for analytic functions'':  If there exists an $\epsilon > 0$
such that $f(x) = 0$ for all $x \in (a-\epsilon, a+\epsilon)$, then
$f(x) = 0$ for all $x \in (a-\rho,a+\rho)$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Picard's theorem}
\label{sec:picard}

\sectionnotes{1--2 lectures (can be safely skipped)}

A first semester course in analysis should have
a \emph{pi\`ece de r\'esistance} caliber
theorem.  We pick a theorem whose proof combines everything we have
learned.  It is more sophisticated than the fundamental theorem of calculus,
the first highlight theorem of this course.  The
theorem we are talking about is Picard's
theorem%
\footnote{Named for the French mathematician
\href{http://en.wikipedia.org/wiki/\%C3\%89mile_Picard}{Charles \'Emile Picard}
(1856--1941).}
on existence and uniqueness of a solution to an ordinary differential equation.
Both the statement and the proof are beautiful examples of what one can do
with all we have learned.  It is also a good example of how analysis is
applied as differential equations are indispensable in science.

\subsection{First order ordinary differential equation}

Modern science is described in the language of
\emph{differential equations}\index{differential equation}.
That is, equations involving not only the unknown, but also its
derivatives.  The simplest nontrivial form of a differential equation is
the so-called \emph{\myindex{first order ordinary differential equation}}
\begin{equation*}
y' = F(x,y) .
\end{equation*}
Generally we also specify $y(x_0)=y_0$.  The solution of
the equation is a function $y(x)$ such that 
$y(x_0)=y_0$ and $y'(x) = F\bigl(x,y(x)\bigr)$.

When $F$ involves only the $x$ variable, the solution is given by the
fundamental theorem of calculus.  On the other hand, when $F$ depends
on both $x$ and $y$ we need far more firepower.  It is not always
true that a solution exists, and if it does, that it is the unique solution.
Picard's theorem gives us certain sufficient conditions for existence
and uniqueness.

\subsection{The theorem}

We need a definition of continuity in two variables.  First, a point in the
plane $\R^2 = \R \times \R$ is denoted by an ordered pair $(x,y)$.
To make matters simple, let
us give the following sequential definition of continuity.

\begin{defn}
Let $U \subset \R^2$ be a set and $F \colon U \to \R$ be a function.
Let $(x,y) \in U$ be a point.  The function $F$ is \emph{continuous}
\index{continuous function of two variables}
at $(x,y)$ if 
for every sequence
$\{ (x_n,y_n) \}_{n=1}^\infty$ of points in $U$ such that
$\lim\, x_n = x$ and 
$\lim\, y_n = y$, we have 
\begin{equation*}
\lim_{n \to \infty} F(x_n,y_n) = 
F(x,y) .
\end{equation*}
We say $F$ is continuous if it is continuous at all points in $U$.
\end{defn}

\begin{thm}[Picard's theorem on existence and uniqueness]%
\index{existence and uniqueness theorem}\index{Picard's theorem}
Let $I, J \subset \R$ be closed bounded intervals, 
let $I_0$ and $J_0$ be their interiors, and
let $(x_0,y_0) \in I_0 \times J_0$.
Suppose $F \colon I \times J \to \R$ is continuous
and Lipschitz in the second variable, that is, there exists a number $L$
such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Then there exists an $h > 0$ and a unique differentiable
function $f \colon [x_0 - h, x_0 + h] \to J \subset \R$, such that
\begin{equation} \label{picard:diffeq}
f'(x) = F\bigl(x,f(x)\bigr) \qquad \text{and} \qquad f(x_0) = y_0.
\end{equation}
\end{thm}

\begin{proof}
Suppose we could find a solution $f$.  Using the fundamental
theorem of calculus we integrate the equation 
$f'(x) = F\bigl(x,f(x)\bigr)$, $f(x_0) = y_0$, and write \eqref{picard:diffeq}
as the integral equation
\begin{equation} \label{picard:inteq}
f(x) = y_0 + \int_{x_0}^x F\bigl(t,f(t)\bigr)~dt .
\end{equation}
The idea of our proof is that we try to plug in approximations to
a solution to the right-hand side of \eqref{picard:inteq} to get better approximations on the left
hand side of  \eqref{picard:inteq}.  We hope that in the end the sequence 
converges and solves
\eqref{picard:inteq} and hence \eqref{picard:diffeq}.
The technique below is called \emph{\myindex{Picard iteration}},
and the individual functions $f_k$ are called the 
\emph{Picard iterates}\index{Picard iterate}.

Without loss of generality, suppose $x_0 = 0$ (exercise below).
Another
exercise tells us that $F$ is bounded as it is continuous.
Therefore pick some $M > 0$ so that 
%Let $M := \sup \{  \abs{F(x,y)} : (x,y) \in I\times J \}$.
$\abs{F(x,y)} \leq M$ for all $(x,y) \in I\times J$.
%Without loss of generality, we can assume $M > 0$ (why?).
Pick $\alpha > 0$ such that
$[-\alpha,\alpha] \subset I$ and $[y_0-\alpha, y_0 + \alpha] \subset J$.
Define
\begin{equation*}
h := \min \left\{ \alpha, \frac{\alpha}{M+L\alpha} \right\} .
\end{equation*}
Observe
$[-h,h] \subset I$.

Set $f_0(x) := y_0$.
We define $f_k$ inductively.
Assuming $f_{k-1}([-h,h]) \subset [y_0-\alpha,y_0+\alpha]$,
we see 
$F\bigl(t,f_{k-1}(t)\bigr)$ is
a well defined function of $t$ for $t \in [-h,h]$.
Further if $f_{k-1}$ is continuous
on $[-h,h]$, then
$F\bigl(t,f_{k-1}(t)\bigr)$ is
continuous as
a function of $t$ on $[-h,h]$ (left as an exercise).
Define
\begin{equation*}
f_k(x) := y_0+ \int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt ,
\end{equation*}
and $f_k$ is continuous on $[-h,h]$ by the fundamental theorem of calculus.
To see that $f_k$ maps $[-h,h]$ to $[y_0-\alpha,y_0+\alpha]$, we compute for
$x \in [-h,h]$
\begin{equation*}
\abs{f_k(x) - y_0} = 
\abs{\int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt }
\leq
M\abs{x}
\leq
Mh
\leq
M
\frac{\alpha}{M+L\alpha}
\leq \alpha .
\end{equation*}
We now define $f_{k+1}$ and so on, and
we have defined a sequence $\{ f_k \}$ of functions.  We need
to show that it converges to a function $f$ that solves
the equation \eqref{picard:inteq} and therefore \eqref{picard:diffeq}.

We wish to show that the sequence $\{ f_k \}$ converges uniformly
to some function on $[-h,h]$.  First, for $t \in [-h,h]$
we have the following
useful bound
\begin{equation*}
\abs{F\bigl(t,f_{n}(t)\bigr) - 
F\bigl(t,f_{k}(t)\bigr)}
\leq
L \abs{f_n(t)-f_k(t)}
\leq
L \norm{f_n-f_k}_u ,
\end{equation*}
where $\norm{f_n-f_k}_u$ is the uniform norm, that
is the supremum of $\abs{f_n(t)-f_k(t)}$ for $t \in [-h,h]$.
Now note that $\abs{x} \leq h \leq \frac{\alpha}{M+L\alpha}$.
Therefore
\begin{equation*}
\begin{split}
\abs{f_n(x) - f_k(x)}
& =
\abs{\int_{0}^x F\bigl(t,f_{n-1}(t)\bigr)~dt 
-
\int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt}
\\
& =
\abs{\int_{0}^x F\bigl(t,f_{n-1}(t)\bigr)-
F\bigl(t,f_{k-1}(t)\bigr)~dt}
\\
& \leq
L\norm{f_{n-1}-f_{k-1}}_u
\abs{x}
\\
& \leq
\frac{L\alpha}{M+L\alpha}
\norm{f_{n-1}-f_{k-1}}_u .
\end{split}
\end{equation*}
Let $C := \frac{L\alpha}{M+L\alpha}$ and note that $C < 1$.
Taking supremum on the left-hand side we get
\begin{equation*}
\norm{f_n-f_k}_u \leq C \norm{f_{n-1}-f_{k-1}}_u .
\end{equation*}
Without loss of generality,
suppose $n \geq k$.  Then by \hyperref[induction:thm]{induction} we can show 
\begin{equation*}
\norm{f_n-f_k}_u \leq C^{k} \norm{f_{n-k}-f_{0}}_u .
\end{equation*}
For $x \in [-h,h]$ we have
\begin{equation*}
\abs{f_{n-k}(x)-f_{0}(x)}
=
\abs{f_{n-k}(x)-y_0}
\leq \alpha .
\end{equation*}
Therefore,
\begin{equation*}
\norm{f_n-f_k}_u \leq C^{k} \norm{f_{n-k}-f_{0}}_u \leq C^{k} \alpha .
\end{equation*}
As $C < 1$, $\{f_n\}$ is uniformly Cauchy and by
\propref{prop:uniformcauchy} we obtain that $\{ f_n \}$
converges uniformly on $[-h,h]$ to some function $f \colon [-h,h] \to \R$.
The function $f$ is the uniform limit of continuous functions and therefore
continuous.  Furthremore since all the $f_n([-h,h]) \subset
[y_0-\alpha,y_0+\alpha]$, then $f([-h,h]) \subset [y_0-\alpha,y_0+\alpha]$
(why?).


We now need to show that $f$ solves \eqref{picard:inteq}.
First, as before we notice
\begin{equation*}
\abs{F\bigl(t,f_{n}(t)\bigr) - 
F\bigl(t,f(t)\bigr)}
\leq
L \abs{f_n(t)-f(t)}
\leq
L \norm{f_n-f}_u .
\end{equation*}
As $\norm{f_n-f}_u$ converges to 0, then
$F\bigl(t,f_n(t)\bigr)$ converges uniformly to $F\bigl(t,f(t)\bigr)$
for $t \in [-h,h]$.  Hence, for $x \in [-h,h]$
the convergence is uniform %(why?)
for $t \in [0,x]$ (or $[x,0]$ if $x < 0$).  Therefore,
\begin{align*}
y_0
+
\int_0^{x}
F(t,f(t)\bigr)~dt
& =
y_0
+
\int_0^{x}
F\bigl(t,\lim_{n\to\infty} f_n(t)\bigr)~dt
& &
\\
& =
y_0
+
\int_0^{x}
\lim_{n\to\infty} F\bigl(t,f_n(t)\bigr)~dt
& & \text{(by continuity of $F$)}
\\
& =
\lim_{n\to\infty} 
\left(
y_0
+
\int_0^{x}
F\bigl(t,f_n(t)\bigr)~dt
\right)
& & \text{(by uniform convergence)}
\\
& =
\lim_{n\to\infty} 
f_{n+1}(x)
=
f(x) .
& &
\end{align*}
We apply the fundamental theorem of calculus to show that
$f$ is differentiable and its derivative is $F\bigl(x,f(x)\bigr)$.  It is obvious
that $f(0) = y_0$.

Finally, what is left to do is to show uniqueness.  Suppose $g \colon [-h,h]
\to J \subset \R$ is another solution.
As before we use the fact that
$\abs{F\bigl(t,f(t)\bigr) - F\bigl(t,g(t)\bigr)} \leq L \norm{f-g}_u$.
Then
\begin{equation*}
\begin{split}
\abs{f(x)-g(x)}
& =
\abs{
y_0
+
\int_0^{x}
F\bigl(t,f(t)\bigr)~dt
-
\left(
y_0
+
\int_0^{x}
F\bigl(t,g(t)\bigr)~dt
\right)
}
\\
& =
\abs{
\int_0^{x}
F\bigl(t,f(t)\bigr)
-
F\bigl(t,g(t)\bigr)~dt
}
\\
& \leq
L\norm{f-g}_u\abs{x}
\leq
Lh\norm{f-g}_u
\leq
\frac{L\alpha}{M+L\alpha}\norm{f-g}_u .
\end{split}
\end{equation*}
As 
before, $C = \frac{L\alpha}{M+L\alpha} < 1$.  By taking supremum over $x \in
[-h,h]$ on the left
hand side we obtain
\begin{equation*}
\norm{f-g}_u \leq C \norm{f-g}_u .
\end{equation*}
This is only possible if $\norm{f-g}_u = 0$.  Therefore, $f=g$, and the
solution is unique.
\end{proof}

\subsection{Examples}

Let us look at some examples.  The proof of the theorem 
gives us an explicit way to find an $h$ that works.  It does not, however, give
us the best $h$.  It is often possible to find a much larger $h$ for
which the conclusion of the theorem holds.

The proof also gives us the Picard iterates as approximations to the
solution.  So the proof actually tells us how to obtain
the solution, not just that the solution exists.

\begin{example}
Consider
\begin{equation*}
f'(x) = f(x), \qquad f(0) = 1 .
\end{equation*}
That is, we let $F(x,y) = y$, and we are looking for a function 
$f$ such that $f'(x) = f(x)$.  We pick any $I$ that contains 0
in the interior.
We pick an arbitrary $J$ that contains 1 in its interior.  We can
use $L = 1$.
The theorem guarantees an $h > 0$ such that
there exists a unique solution $f \colon [-h,h] \to \R$.  This solution
is usually denoted by
\begin{equation*}
e^x := f(x) .
\end{equation*}
We leave it to the reader to verify that by picking $I$ and $J$
large enough the proof of the theorem guarantees that
we are able to pick $\alpha$ such that we get any
$h$ we want as long as $h < \nicefrac{1}{2}$.  We omit the calculation.

Of course, we know %(though we have not proved)
this function exists
as a function for all $x$, so an arbitrary $h$ ought to work.
By same reasoning as above,
no matter what $x_0$ and $y_0$ are,
the proof guarantees an arbitrary $h$ as long as $h < \nicefrac{1}{2}$.
Fix such an $h$.
We get a unique function defined on $[x_0-h,x_0+h]$.  After defining the
function on $[-h,h]$ we find a solution on the interval $[0,2h]$
and notice that the two functions must coincide on $[0,h]$ by uniqueness.
We thus iteratively construct the exponential for all $x \in \R$.
Therefore Picard's theorem could be used to prove the existence and uniqueness
of the exponential.

Let us compute the Picard iterates.
We start with the constant function $f_0(x) := 1$.  Then
\begin{align*}
f_1(x) & = 1 + \int_0^x f_0(s)~ds =
1+x, \\
f_2(x) & = 1 + \int_0^x f_1(s)~ds =
1 + \int_0^x (1+s)~ds = 1 + x + \frac{x^2}{2}, \\
f_3(x) & = 1 + \int_0^x f_2(s)~ds =
1 + \int_0^x \left(1+ s + \frac{s^2}{2} \right)~ds =
1 + x + \frac{x^2}{2} + \frac{x^3}{6} .
\end{align*}
We recognize the beginning of the Taylor series for the exponential.
\end{example}

\begin{example}
Suppose we have the equation
\begin{equation*}
f'(x) = {\bigl(f(x)\bigr)}^2 \qquad \text{and} \qquad f(0)=1.
\end{equation*}
From elementary differential equations we know 
\begin{equation*}
f(x) = \frac{1}{1-x}
\end{equation*}
is the solution.
The solution is only defined on $(-\infty,1)$.  That is,
we are able to use $h < 1$, but never a larger $h$.
The function that takes $y$ to $y^2$ is
not Lipschitz as a function on all of $\R$.
As we approach $x=1$ from the left, the solution becomes larger
and larger.  The derivative of the solution grows as $y^2$, and therefore
the $L$ required will have to be larger and larger as $y_0$ grows.
Thus if we apply the
theorem with $x_0$ close to 1 and $y_0 = \frac{1}{1-x_0}$ we find
that the $h$ that the proof guarantees will be smaller and smaller as $x_0$
approaches 1.

By picking $\alpha$ correctly, the proof of the theorem guarantees
$h=1-\nicefrac{\sqrt{3}}{2} \approx 0.134$ (we omit the
calculation) for $x_0=0$ and $y_0=1$, even though
we saw above that any $h < 1$ should work.
\end{example}

\begin{example}
Consider the equation
\begin{equation*}
f'(x) = 2 \sqrt{\abs{f(x)}}, \qquad f(0) = 0 .
\end{equation*}
The function $F(x,y) = 2 \sqrt{\abs{y}}$ is continuous,
but not Lipschitz in $y$ (why?). 
The equation does not satisfy the hypotheses of the theorem.
The function
\begin{equation*}
f(x) =
\begin{cases}
x^2 & \text{ if $x \geq 0$,}\\
-x^2 & \text{ if $x < 0$,}
\end{cases}
\end{equation*}
is a solution, but $f(x) = 0$ is also a solution.
A solution exists, but is not unique.
\end{example}

\begin{example}
Consider $y' = \varphi(x)$ where $\varphi(x) := 0$ if $x \in \Q$ and
$\varphi(x):=1$ if $x
\not\in \Q$.  The equation has no solution regardless of the initial
conditions.
A solution would have
derivative $\varphi$, but $\varphi$ does not have the intermediate value property
at any point (why?).  No solution exists by
\hyperref[thm:darboux]{Darboux's theorem}.
Therefore to obtain existence of a solution, some continuity hypothesis on
$F$ is necessary.
\end{example}

\subsection{Exercises}

\begin{exercise}
Let $I, J \subset \R$ be intervals.
Let $F \colon I \times J \to \R$ be a continuous function
of two variables
and suppose $f \colon I \to J$ be a continuous function.
Show that $F\bigl(x,f(x)\bigr)$ is a continuous function on $I$.
\end{exercise}

\begin{exercise}
Let $I, J \subset \R$ be closed bounded intervals.
Show that if $F \colon I \times J \to \R$ is continuous,
then $F$ is bounded.
\end{exercise}

\begin{exercise}
We proved Picard's theorem under the assumption that $x_0 = 0$.
Prove the full statement of Picard's theorem for an arbitrary $x_0$.
\end{exercise}

\begin{exercise}
Let $f'(x)=x f(x)$ be our equation.  Start with the initial condition
$f(0)=2$ and find the Picard iterates $f_0,f_1,f_2,f_3,f_4$.
\end{exercise}

\begin{exercise}
Suppose $F \colon I \times J \to \R$
is a function that is continuous in the first variable,
that is, for any fixed $y$ the function that takes $x$ to $F(x,y)$ is
continuous.  Further, suppose $F$ is Lipschitz in the second variable,
that is, there exists a number $L$ such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Show that $F$ is continuous as a function of two variables.  Therefore, the
hypotheses in the theorem could be made even weaker.
\end{exercise}

\begin{exercise}
A common type of equation one encounters are
\emph{\myindex{linear first order differential equations}}, that is
equations of the form
\begin{equation*}
y' + p(x) y = q(x) , \qquad y(x_0) = y_0 .
\end{equation*}
Prove Picard's theorem for linear equations.  Suppose $I$ is an
interval, $x_0 \in I$, and $p \colon I \to \R$ and $q \colon I \to \R$ are
continuous.
Show that there exists a unique differentiable $f \colon I \to \R$,
such that $y = f(x)$
satisfies the equation and the initial condition.
Hint: Assume existence of the exponential function and use the integrating
factor formula for existence of $f$ (prove that it works):
\begin{equation*}
f(x) := e^{-\int_{x_0}^x p(s)\, ds} \left( \int_{x_0}^x e^{\int_{x_0}^t p(s)\, ds}
q(t) ~dt + y_0 \right).
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metric Spaces} \label{ms:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Metric spaces}
\label{sec:metric}

\sectionnotes{1.5 lectures}

As mentioned in the introduction, the main idea in analysis is to take
limits.  In \chapterref{seq:chapter} we learned to take limits of sequences of
real numbers.  And in \chapterref{lim:chapter} we learned to take limits
of functions as a real number approached some other real number.

We want to take limits in more complicated contexts.  For
example, we want to have sequences of points in 3-dimensional space.
We wish to define continuous functions of several variables.
We even want to define functions on spaces that are a little harder to
describe, such as the surface of the earth.  We still want to talk about
limits there.

Finally, we have seen the limit of a sequence of
functions in \chapterref{fs:chapter}.
We wish to unify all these notions so that we do not have to
reprove theorems over and over again in each context.  The concept of a
metric space is an elementary yet powerful tool in analysis.  And while it
is not sufficient to describe every type of limit we find in modern
analysis, it gets us very far indeed.

\begin{defn}
Let $X$ be a set, and let
$d \colon X \times X \to \R$
be a function such that
\begin{enumerate}[(i)]
\item \label{metric:pos} $d(x,y) \geq 0$ for all $x, y$ in $X$,
\item \label{metric:zero} $d(x,y) = 0$ if and only if $x = y$,
\item \label{metric:com} $d(x,y) = d(y,x)$, 
\item \label{metric:triang} $d(x,z) \leq d(x,y)+ d(y,z)$ \qquad (\emph{\myindex{triangle inequality}}).
\end{enumerate}
Then the pair $(X,d)$ is called a \emph{\myindex{metric space}}.  The
function $d$ is called the \emph{\myindex{metric}} or sometimes the
\emph{\myindex{distance function}}.
Sometimes we just say $X$ is a metric space if the metric is clear from
context.
\end{defn}

The geometric idea is that $d$ is the distance between two points. 
Items \ref{metric:pos}--\ref{metric:com} have obvious geometric
interpretation: Distance is always nonnegative, the only point that is
distance 0 away from $x$ is $x$ itself, and finally that the distance from
$x$ to $y$ is the same as the distance from $y$ to $x$.  The triangle
inequality \ref{metric:triang} has the interpretation given in
\figureref{fig:mstriang}.
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{ms-triang.pdf_t}
\caption{Diagram of the triangle inequality in metric spaces.\label{fig:mstriang}}
%\end{center}
\end{myfigureht}

For the purposes of drawing, it is convenient to draw figures and
diagrams in the plane with the metric being the euclidean distance.
However, that is only one particular metric space.  Just because a
certain fact seems to be clear from drawing a picture does not mean it is
true.  You might be getting sidetracked by intuition from euclidean
geometry,
whereas the concept of a metric space is a lot more general.

Let us give some examples of metric spaces.

\begin{example}
The set of real numbers $\R$ is a metric space with the metric
\begin{equation*}
d(x,y) := \abs{x-y} .
\end{equation*}
Items \ref{metric:pos}--\ref{metric:com} of the definition
are easy to verify.  The
triangle inequality \ref{metric:triang} follows immediately
from the standard triangle inequality for real numbers:
\begin{equation*}
d(x,z) = \abs{x-z} = 
\abs{x-y+y-z} \leq
\abs{x-y}+\abs{y-z} =
d(x,y)+ d(y,z) .
\end{equation*}
This metric is the \emph{\myindex{standard metric on $\R$}}.  If we talk
about $\R$ as a metric space without mentioning a specific metric, we 
mean this particular metric.
\end{example}

\begin{example}
We can also put a different metric on the set of real numbers.
For example, take the set of real numbers $\R$ together with the metric
\begin{equation*}
d(x,y) :=
\frac{\abs{x-y}}{\abs{x-y}+1} .
\end{equation*}
Items \ref{metric:pos}--\ref{metric:com} are again easy to verify.  The
triangle inequality \ref{metric:triang} is a little bit more difficult.
Note that $d(x,y) = \varphi(\abs{x-y})$ where $\varphi(t) =
\frac{t}{t+1}$ and $\varphi$ is an increasing function
(positive derivative).  Hence
\begin{equation*}
\begin{split}
d(x,z) & = \varphi(\abs{x-z}) = 
\varphi(\abs{x-y+y-z}) \leq
\varphi(\abs{x-y}+\abs{y-z})
\\
& =
\frac{\abs{x-y}+\abs{y-z}}{\abs{x-y}+\abs{y-z}+1} =
\frac{\abs{x-y}}{\abs{x-y}+\abs{y-z}+1} +
\frac{\abs{y-z}}{\abs{x-y}+\abs{y-z}+1}
\\
& \leq
\frac{\abs{x-y}}{\abs{x-y}+1} +
\frac{\abs{y-z}}{\abs{y-z}+1} =
d(x,y)+ d(y,z) .
\end{split}
\end{equation*}
Here we have an example of a nonstandard metric on $\R$.  With this metric,
$d(x,y) < 1$ for all $x,y \in \R$.  That is,
any two points are less than 1 unit apart.
\end{example}

An important metric space is the
$n$-dimensional \emph{\myindex{euclidean space}} $\R^n = \R
\times \R \times \cdots \times \R$.   We use the following
notation for points: $x =(x_1,x_2,\ldots,x_n) \in \R^n$.  We also
simply write $0 \in \R^n$ to mean the vector $(0,0,\ldots,0)$.  Before
making $\R^n$ a metric space, let us prove an important inequality, the
so-called Cauchy--Schwarz inequality.

\begin{lemma}[\myindex{Cauchy--Schwarz inequality}]
Take $x =(x_1,x_2,\ldots,x_n) \in \R^n$ and $y =(y_1,y_2,\ldots,y_n) \in
\R^n$.  Then
\begin{equation*}
{\biggl( \sum_{j=1}^n x_j y_j \biggr)}^2
\leq
\biggl(\sum_{j=1}^n x_j^2 \biggr)
\biggl(\sum_{j=1}^n y_j^2 \biggr) .
\end{equation*}
\end{lemma}

\begin{proof}
Any square of a real number is nonnegative.  Hence any sum of squares is
nonnegative:
\begin{equation*}
\begin{split}
0 & \leq 
\sum_{j=1}^n \sum_{k=1}^n {(x_j y_k - x_k y_j)}^2
\\
& =
\sum_{j=1}^n \sum_{k=1}^n \bigl( x_j^2 y_k^2 + x_k^2 y_j^2 - 2 x_j x_k y_j
y_k \bigr)
\\
& =
\biggl( \sum_{j=1}^n x_j^2 \biggr)
\biggl( \sum_{k=1}^n y_k^2 \biggr)
+
\biggl( \sum_{j=1}^n y_j^2 \biggr)
\biggl( \sum_{k=1}^n x_k^2 \biggr)
-
2
\biggl( \sum_{j=1}^n x_j y_j \biggr)
\biggl( \sum_{k=1}^n x_k y_k \biggr)
\end{split}
\end{equation*}
We relabel and divide by 2 to obtain
\begin{equation*}
0 \leq 
\biggl( \sum_{j=1}^n x_j^2 \biggr)
\biggl( \sum_{j=1}^n y_j^2 \biggr)
-
{\biggl( \sum_{j=1}^n x_j y_j \biggr)}^2 ,
\end{equation*}
which is precisely what we wanted.
\end{proof}

\begin{example}
Let us construct the
standard metric\index{standard metric on $\R^n$} for $\R^n$.  Define
\begin{equation*}
d(x,y) :=
\sqrt{
{(x_1-y_1)}^2 + 
{(x_2-y_2)}^2 + 
\cdots +
{(x_n-y_n)}^2
} =
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2 
} .
\end{equation*}
For $n=1$, the real line, this metric agrees with what we did above.  Again,
the only tricky part of the definition to check is the triangle inequality.
It is less messy to work with the square of the metric.  In the
following, note the use of the Cauchy--Schwarz inequality.
\begin{equation*}
\begin{split}
{\bigl(d(x,z)\bigr)}^2 & =
\sum_{j=1}^n
{(x_j-z_j)}^2 
\\
& =
\sum_{j=1}^n
{(x_j-y_j+y_j-z_j)}^2 
\\
& =
\sum_{j=1}^n
\Bigl(
{(x_j-y_j)}^2+{(y_j-z_j)}^2 + 2(x_j-y_j)(y_j-z_j)
\Bigr)
\\
& =
\sum_{j=1}^n
{(x_j-y_j)}^2
+
\sum_{j=1}^n
{(y_j-z_j)}^2 
+
\sum_{j=1}^n
 2(x_j-y_j)(y_j-z_j)
\\
& \leq
\sum_{j=1}^n
{(x_j-y_j)}^2
+
\sum_{j=1}^n
{(y_j-z_j)}^2 
+
2
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2
\sum_{j=1}^n
{(y_j-z_j)}^2
}
\\
& =
{\left(
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2
}
+
\sqrt{
\sum_{j=1}^n
{(y_j-z_j)}^2 
}
\right)}^2
=
{\bigl( d(x,y) + d(y,z) \bigr)}^2 .
\end{split}
\end{equation*}
Taking the square root of both sides we obtain the correct inequality,
because the square root is an increasing function.
\end{example}

\begin{example}
The set of complex numbers $\C$ is the set of numbers $z = x+iy$, where $x$
and $y$ are in $\R$.  By imposing $i^2 = -1$ we make $\C$ into a
field.
For the purposes of taking limits,
the set $\C$ is regarded as the metric space $\R^2$, where $z=x+iy \in \C$
corresponds to $(x,y) \in \R^2$.
For any $z=x+iy$ define the \emph{\myindex{complex modulus}}\index{modulus},
by $\sabs{z} := \sqrt{x^2+y^2}$.
Then for any two complex numbers
$z_1 = x_1 + iy_1$ and $z_2 = x_2 + iy_2$, the distance is
\begin{equation*}
d(z_1,z_2) = \sqrt{{(x_1-x_2)}^2+ {(y_1-y_2)}^2} = \sabs{z_1-z_2}.
\end{equation*}
Furthermore, when working with complex numbers
it is often convenient to write the metric in the terms of
the so called
\emph{\myindex{complex conjugate}}: that is, the conjugate of $z=x+iy$
is $\bar{z} := x-iy$.  Then 
${\sabs{z}}^2 = x^2 +y^2 = z\bar{z}$, and so ${\sabs{z_1-z_2}}^2 =
(z_1-z_2)\overline{(z_1-z_2)}$.

\end{example}

\begin{example}
An example to keep in mind is the so-called \emph{\myindex{discrete
metric}}.
For any set $X$, define
\begin{equation*}
d(x,y) :=
\begin{cases}
1 & \text{if $x \not= y$}, \\
0 & \text{if $x = y$}.
\end{cases}
\end{equation*}
That is, all points are equally distant from each other.  When $X$ is a
finite set, we can draw a diagram, see for example
\figureref{fig:msdiscmetric}.
Things become subtle when $X$ is an infinite set such
as the real numbers.
\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{msdiscmetric.pdf_t}
\caption{Sample discrete metric space $\{ a,b,c,d,e \}$, the distance
between any two points is $1$.\label{fig:msdiscmetric}}
%\end{center}
\end{myfigureht}

While this particular
example seldom comes up in practice, it gives a useful 
``smell test.''  If you make a statement about metric spaces,
try it with the discrete metric.
To show that $(X,d)$ is indeed a metric space is left as an exercise.
\end{example}

\begin{example} \label{example:msC01}
Let $C([a,b],\R)$ be the set of continuous real-valued functions on the
interval $[a,b]$.  Define the metric on $C([a,b],\R)$ as
\begin{equation*}
d(f,g) := \sup_{x \in [a,b]} \abs{f(x)-g(x)} .
\end{equation*}
Let us check the properties.  First, $d(f,g)$ is finite as
$\abs{f(x)-g(x)}$ is a continuous function on a closed bounded interval
$[a,b]$, and so is bounded.
It is clear that $d(f,g) \geq 0$, 
it is the supremum of nonnegative numbers.  If $f = g$
then $\abs{f(x)-g(x)} = 0$ for all $x$ and hence $d(f,g) = 0$.  Conversely
if $d(f,g) = 0$, then for any $x$ we have $\abs{f(x)-g(x)} \leq d(f,g) = 0$
and hence $f(x) = g(x)$ for all $x$ and $f=g$.  That $d(f,g) = d(g,f)$
is equally trivial.  To show the triangle inequality we use the standard
triangle inequality.
\begin{equation*}
\begin{split}
d(f,g) & =
\sup_{x \in [a,b]} \abs{f(x)-g(x)} =
\sup_{x \in [a,b]} \abs{f(x)-h(x)+h(x)-g(x)}
\\
& \leq
\sup_{x \in [a,b]} ( \abs{f(x)-h(x)}+\abs{h(x)-g(x)} )
\\
& \leq
\sup_{x \in [a,b]} \abs{f(x)-h(x)}+
\sup_{x \in [a,b]} \abs{h(x)-g(x)} = d(f,h) + d(h,g) .
\end{split}
\end{equation*}
When treating $C([a,b],\R)$ as a metric space without mentioning a metric, we mean this
particular metric.
Notice that $d(f,g) = \norm{f-g}_u$, the uniform norm of \defnref{def:unifnorm}.

This example may seem esoteric at first, but it turns out that working with
spaces such as $C([a,b],\R)$ is really the meat of a large part of modern 
analysis.  Treating sets of functions as metric spaces allows us to
abstract away a lot of the grubby detail and prove powerful results such as
Picard's theorem with less work.
\end{example}

Oftentimes it is useful to consider a subset of a larger metric space
as a metric space itself.  We obtain the following proposition, which has
a trivial proof.

\begin{prop}
Let $(X,d)$ be a metric space and $Y \subset X$, then the restriction
$d|_{Y \times Y}$ is a metric on $Y$.
\end{prop}

\begin{defn}
If $(X,d)$ is a metric space, $Y \subset X$, and $d' := d|_{Y \times Y}$,
then $(Y,d')$ is said to be a \emph{\myindex{subspace}} of $(X,d)$.
\end{defn}

It is common to simply write $d$ for the metric on $Y$, as it is 
the restriction of the metric on $X$.  Sometimes we say $d'$ is
the \emph{\myindex{subspace metric}} and $Y$ has the
\emph{\myindex{subspace topology}}.

\medskip

A subset of the real
numbers is bounded whenever all its elements are at most some fixed distance
from 0.
We also define bounded sets in a metric space.
When dealing with an arbitrary metric space there may not be some
natural fixed point 0.  For the purposes of boundedness it does not matter.

\begin{defn}
Let $(X,d)$ be a metric space.  A subset $S \subset X$ is said to be
\emph{bounded}\index{bounded set} if there exists a $p \in X$ and a
$B \in \R$ such that
\begin{equation*}
d(p,x) \leq B \quad \text{for all $x \in S$}.
\end{equation*}
We say $(X,d)$ is bounded if $X$ itself is a bounded subset.
\end{defn}

For example, the set of real numbers with the standard metric is not a
bounded metric space.  It is not hard to see that a
subset of the real numbers is bounded in the
sense of \chapterref{rn:chapter} if and only if it is bounded as a subset of the
metric space of real numbers with the standard metric.

On the other hand, if we take the real numbers with the discrete metric,
then we obtain a bounded metric space.  In fact, any set with the
discrete metric is bounded.

\subsection{Exercises}

\begin{exercise}
Show that for any set $X$, the discrete metric ($d(x,y) = 1$ if $x\not=y$ and
$d(x,x) = 0$) does give a metric space $(X,d)$.
\end{exercise}

\begin{exercise}
Let $X := \{ 0 \}$ be a set.  Can you make it into a metric space?
\end{exercise}

\begin{exercise}
Let $X := \{ a, b \}$ be a set.  Can you make it into two distinct metric
spaces?  (define two distinct metrics on it)
\end{exercise}

\begin{exercise}
Let the set $X := \{ A, B, C \}$ represent 3 buildings on campus.  Suppose we
wish our distance to be the time it takes to walk from one building to
the other.
It takes 5 minutes either way between buildings $A$ and $B$.  However,
building $C$ is on a hill and it takes 10 minutes from $A$ and 15 minutes
from $B$ to get to $C$.  On the other hand it takes 5 minutes to go
from $C$ to $A$ and 7 minutes to go from $C$ to $B$, as we are going
downhill.  Do these distances define a metric?  If so, prove it, if not, say
why not.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is a metric space and
$\varphi \colon [0,\infty) \to \R$ is
%an increasing
a function such that 
$\varphi(t) \geq 0$ for all $t$ and $\varphi(t) = 0$ if and only if
$t=0$.  Also suppose $\varphi$ is \emph{\myindex{subadditive}},
that is, $\varphi(s+t) \leq \varphi(s)+\varphi(t)$.
Show that with $d'(x,y) := \varphi\bigl(d(x,y)\bigr)$, we obtain a new
metric space $(X,d')$.
\end{exercise}

\begin{exercise} \label{exercise:mscross}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.\\
a) Show that $(X \times Y,d)$ with
$d\bigl( (x_1,y_1), (x_2,y_2) \bigr) := d_X(x_1,x_2) + d_Y(y_1,y_2)$ is
a metric space. \\
b) Show that $(X \times Y,d)$ with
$d\bigl( (x_1,y_1), (x_2,y_2) \bigr) := \max \{ d_X(x_1,x_2) , d_Y(y_1,y_2) \}$ is
a metric space.
\end{exercise}

\begin{exercise}
Let $X$ be the set of continuous functions on $[0,1]$.  Let $\varphi \colon
[0,1] \to (0,\infty)$ be continuous.  Define
\begin{equation*}
d(f,g) := \int_0^1 \abs{f(x)-g(x)}\varphi(x)~dx .
\end{equation*}
Show that $(X,d)$ is a metric space.
\end{exercise}

\pagebreak[2]

\begin{exercise} \label{exercise:mshausdorffpseudo}
Let $(X,d)$ be a metric space.  For nonempty bounded subsets $A$ and $B$ let
\begin{equation*}
d(x,B) := \inf \{ d(x,b) : b \in B \}
\qquad \text{and} \qquad
d(A,B) := \sup \{ d(a,B) : a \in A \} .
\end{equation*}
Now define the \emph{\myindex{Hausdorff metric}} as
\begin{equation*}
d_H(A,B) := \max \{ d(A,B) , d(B,A) \} .
\end{equation*}
Note: $d_H$ can be defined for arbitrary nonempty subsets if we allow the
extended reals.
\\
a) Let $Y \subset \sP(X)$ be the set of bounded nonempty subsets.
Prove that
$(Y,d_H)$ is a so-called \emph{\myindex{pseudometric space}}:
$d_H$ satisfies the metric properties
\ref{metric:pos},
\ref{metric:com}, 
\ref{metric:triang}, and further
$d_H(A,A) = 0$ for all $A \in Y$. 
\\
b) Show by example that $d$ itself is not symmetric, that is $d(A,B) \not=
d(B,A)$.
\\
c) Find a metric space $X$ and two different
nonempty bounded subsets $A$ and $B$ such that $d_H(A,B) = 0$.
\end{exercise}

\begin{exercise} \label{exercise:C1ab}
Let $C^1([a,b],\R)$ be the set of once continuously differentiable
functions on $[a,b]$.
Define
\begin{equation*}
d(f,g) := \snorm{f-g}_u + \snorm{f'-g'}_u,
\end{equation*}
where $\snorm{\cdot}_u$ is the uniform norm.  Prove that $d$ is a metric.
\end{exercise}

\begin{exercise}
a) Find a metric $d$ on $\N$ that makes it into an unbounded metric space.
\\
b) Find a metric $d$ on $\N$ that makes it into a bounded metric space.
\\
c) Find a metric $d$ on $\N$ such that for any $n \in \N$ and any $\epsilon > 0$
there exists an $m \in \N$ such that $d(n,m) < \epsilon$.
\end{exercise}

\begin{exercise}
Consider $\ell^2$ the set of sequences $\{ x_n \}$
of real numbers
such that $\sum_{n=1}^\infty x_n^2 < \infty$.
\\
a) Prove the Cauchy--Schwarz inequality for two sequences
$\{x_n \}$ and $\{ y_n \}$ in $\ell^2$:
\begin{equation*}
{\biggl( \sum_{j=1}^\infty x_j y_j \biggr)}^2
\leq
\biggl(\sum_{j=1}^\infty x_j^2 \biggr)
\biggl(\sum_{j=1}^\infty y_j^2 \biggr) .
\end{equation*}
\\
b) Prove that $\ell^2$ is a metric space with the metric
$d(x,y) := \sqrt{\sum_{j=1}^\infty {(x_j-y_j)}^2}$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Open and closed sets}
\label{sec:mettop}

\sectionnotes{2 lectures}

\subsection{Topology}

It is useful to define a so-called \emph{\myindex{topology}}.  That is
we define closed and open sets in a metric space.  Before doing so,
let us define two special sets.

\begin{defn}
Let $(X,d)$ be a metric space, $x \in X$ and $\delta > 0$.  Then define
the \emph{\myindex{open ball}} or simply \emph{\myindex{ball}} of radius $\delta$
around $x$ as
\begin{equation*}
B(x,\delta) := \{ y \in X : d(x,y) < \delta \} .
\end{equation*}
Similarly we define the \emph{\myindex{closed ball}} as
\begin{equation*}
C(x,\delta) := \{ y \in X : d(x,y) \leq \delta \} .
\end{equation*}
\end{defn}

When we are dealing with different metric spaces, it is sometimes 
convenient to emphasize which metric space the ball is in.  We do this by
writing $B_X(x,\delta) := B(x,\delta)$ or $C_X(x,\delta) := C(x,\delta)$.

\begin{example}
Take the metric space $\R$ with the standard metric.  For
$x \in \R$, and $\delta > 0$ we get 
\begin{equation*}
B(x,\delta) = (x-\delta,x+\delta) \qquad \text{and} \qquad
C(x,\delta) = [x-\delta,x+\delta] .
\end{equation*}
\end{example}

\begin{example}
Be careful when working on a subspace.  Suppose we take the
metric space $[0,1]$ as a subspace of $\R$.  Then in $[0,1]$
we get
\begin{equation*}
B(0,\nicefrac{1}{2}) = B_{[0,1]}(0,\nicefrac{1}{2}) = [0,\nicefrac{1}{2}) .
\end{equation*}
This is different from $B_{\R}(0,\nicefrac{1}{2}) =
(-\nicefrac{1}{2},\nicefrac{1}{2})$.
The important thing to keep in mind is which metric space we are working
in.
\end{example}

\begin{defn}
Let $(X,d)$ be a metric space.  A set $V \subset X$
is \emph{open}\index{open set}
if for every $x \in V$, there exists a $\delta > 0$ such that
$B(x,\delta) \subset V$.  See \figureref{fig:msopenset}.  A set $E \subset X$ is 
\emph{closed}\index{closed set} if the complement $E^c = X \setminus E$ is open.
When the ambient space $X$ is not clear from context we say
$V$ is open in $X$ and $E$ is closed in $X$.

If $x \in V$ and $V$ is open, then we say 
$V$ is an \emph{\myindex{open neighborhood}} of $x$ (or
sometimes just \emph{\myindex{neighborhood}}).
\end{defn}

\begin{myfigureht}
%\begin{center}
\subimport*{figures/}{msopenset.pdf_t}
\caption{Open set in a metric space.  Note that $\delta$ depends on $x$.\label{fig:msopenset}}
%\end{center}
\end{myfigureht}

Intuitively, an open set is a set that does not include its ``boundary,''
wherever we are at in the set, we are allowed to ``wiggle'' a little bit and
stay in the set.
Note that not every set is either open or closed, in fact generally
most subsets are neither.

\begin{example}
The set $[0,1) \subset \R$ is neither open nor closed.  First,
every ball in $\R$ around $0$, $(-\delta,\delta)$, contains negative
numbers and hence is not contained in $[0,1)$ and so $[0,1)$ is not open.
Second, every ball in $\R$ around $1$, $(1-\delta,1+\delta)$ contains
numbers strictly less than 1 and greater than 0
(e.g.\ $1-\nicefrac{\delta}{2}$ as long as $\delta < 2$).
Thus $\R \setminus
[0,1)$ is not open, and so $[0,1)$ is not closed.
\end{example}

\begin{prop} \label{prop:topology:open}
Let $(X,d)$ be a metric space.
\begin{enumerate}[(i)]
\item \label{topology:openi} $\emptyset$ and $X$ are open in $X$.
\item \label{topology:openii} If $V_1, V_2, \ldots, V_k$ are open then
\begin{equation*}
\bigcap_{j=1}^k V_j
\end{equation*}
is also open.  That is, finite intersection of open sets is open.
\item \label{topology:openiii} If $\{ V_\lambda \}_{\lambda \in I}$ is
an arbitrary collection of open sets, then
\begin{equation*}
\bigcup_{\lambda \in I} V_\lambda
\end{equation*}
is also open.  That is, union of open sets is open.
\end{enumerate}
\end{prop}

Note that the index set in \ref{topology:openiii} is arbitrarily large.
By $\bigcup_{\lambda \in I} V_\lambda$ we simply mean the set of
all $x$ such that $x \in V_\lambda$ for at least one $\lambda \in I$.

\begin{proof}
The sets $X$ and $\emptyset$ are obviously open in $X$.

Let us prove \ref{topology:openii}.
If $x \in \bigcap_{j=1}^k V_j$, then $x \in V_j$ for all $j$.
As $V_j$ are all open, for every $j$ there exists a $\delta_j > 0$ 
such that $B(x,\delta_j) \subset V_j$.  Take $\delta := \min \{
\delta_1,\delta_2,\ldots,\delta_k \}$ and notice $\delta > 0$.  We have
$B(x,\delta) \subset B(x,\delta_j) \subset V_j$ for every $j$ and so
$B(x,\delta) \subset \bigcap_{j=1}^k V_j$.  Consequently the intersection is open.

Let us prove \ref{topology:openiii}.
If $x \in \bigcup_{\lambda \in I} V_\lambda$, then $x \in V_\lambda$ for some
$\lambda \in I$.
As $V_\lambda$ is open, there exists a $\delta > 0$
such that $B(x,\delta) \subset V_\lambda$.  But then
$B(x,\delta) \subset \bigcup_{\lambda \in I} V_\lambda$
and so the union is open.
\end{proof}

\begin{example}
The main thing to notice is the difference between
items
\ref{topology:openii} and \ref{topology:openiii}.
Item \ref{topology:openii} is not true for an arbitrary intersection,
for example $\bigcap_{n=1}^\infty (-\nicefrac{1}{n},\nicefrac{1}{n}) = \{ 0
\}$, which is not open.
\end{example}


The proof of the following analogous proposition for closed sets
is left as an exercise.

\begin{prop} \label{prop:topology:closed}
Let $(X,d)$ be a metric space.
\begin{enumerate}[(i)]
\item \label{topology:closedi} $\emptyset$ and $X$ are closed in $X$.
\item \label{topology:closedii} If $\{ E_\lambda \}_{\lambda \in I}$ is
an arbitrary collection of closed sets, then
\begin{equation*}
\bigcap_{\lambda \in I} E_\lambda
\end{equation*}
is also closed.  That is, intersection of closed sets is closed.
\item \label{topology:closediii} If $E_1, E_2, \ldots, E_k$ are closed then
\begin{equation*}
\bigcup_{j=1}^k E_j
\end{equation*}
is also closed.  That is, finite union of closed sets is closed.
\end{enumerate}
\end{prop}

We have not yet shown that the open ball is open and the closed ball is
closed.  Let us show this fact now to justify the terminology.

\begin{prop} \label{prop:topology:ballsopenclosed}
Let $(X,d)$ be a metric space, $x \in X$, and $\delta > 0$.  Then
$B(x,\delta)$ is open and 
$C(x,\delta)$ is closed.
\end{prop}

\begin{proof}
Let $y \in B(x,\delta)$.  Let $\alpha := \delta-d(x,y)$.  Of course $\alpha
> 0$.  Now let $z \in B(y,\alpha)$.  Then
\begin{equation*}
d(x,z) \leq d(x,y) + d(y,z) < d(x,y) + \alpha = d(x,y) + \delta-d(x,y) =
\delta .
\end{equation*}
Therefore $z \in B(x,\delta)$ for every $z \in B(y,\alpha)$.  So $B(y,\alpha) \subset B(x,\delta)$ and
$B(x,\delta)$ is open.

The proof that $C(x,\delta)$ is closed is left as an exercise.
\end{proof}

Again be careful about what is the ambient metric space.
As $[0,\nicefrac{1}{2})$ is
an open ball in $[0,1]$, this means that $[0,\nicefrac{1}{2})$ is
an open set in $[0,1]$.  On the other hand $[0,\nicefrac{1}{2})$
is neither open nor closed in $\R$.

A useful way to think about an open set is as a union of open balls.  If $U$ is
open, then for each $x \in U$, there is a $\delta_x > 0$ (depending on $x$) such that
$B(x,\delta_x) \subset U$.  Then $U = \bigcup_{x\in U} B(x,\delta_x)$.

The proof of the following proposition is left as an exercise.  Note that
there are many other open and
closed sets in $\R$.

\begin{prop} \label{prop:topology:intervals:openclosed}
Let $a < b$ be two real numbers.  Then $(a,b)$, $(a,\infty)$,
and $(-\infty,b)$ are open in $\R$.
Also $[a,b]$, $[a,\infty)$,
and $(-\infty,b]$ are closed in $\R$.
\end{prop}


\subsection{Connected sets}

\begin{defn}
A nonempty
metric space $(X,d)$ is \emph{\myindex{connected}} if the
only subsets of $X$ that are both open and closed are $\emptyset$ and $X$ itself.
If $(X,d)$ is not connected we say it is
\emph{\myindex{disconnected}}.

When we apply the term \emph{connected} to a nonempty subset $A \subset X$, we simply
mean that $A$ with the subspace topology is connected.
\end{defn}

In other words, a nonempty $X$ is connected if whenever we write
$X = X_1 \cup X_2$ where $X_1 \cap X_2 = \emptyset$ and $X_1$ and $X_2$ are
open, then either $X_1 = \emptyset$ or $X_2 = \emptyset$.
So to show $X$ is disconnected, we need to find nonempty
disjoint open sets $X_1$ and
$X_2$ whose union is $X$.  For subsets, we state this idea as a proposition.

\begin{prop}
Let $(X,d)$ be a metric space.
A nonempty set $S \subset X$ is not connected if and only if
there exist open sets $U_1$ and
$U_2$ in $X$, such that $U_1 \cap U_2 \cap S = \emptyset$,
$U_1 \cap S \not= \emptyset$,
$U_2 \cap S \not= \emptyset$, and
\begin{equation*}
S = 
\bigl( U_1 \cap S \bigr)
\cup
\bigl( U_2 \cap S \bigr) .
\end{equation*}
\end{prop}

\begin{proof}
If $U_j$ is open in $X$,
then $U_j \cap S$ is open in $S$ in the subspace topology (with subspace
metric).  To see this,
note that if $B_X(x,\delta) \subset U_j$, then as
$B_S(x,\delta) = S \cap B_X(x,\delta)$,
we have $B_S(x,\delta) \subset U_j \cap S$.
So if $U_1$ and $U_2$ as above exist, then
$X$ is disconnected based on the discussion above.

The proof of the other direction follows by using
\exerciseref{exercise:mssubspace} to find $U_1$ and $U_2$ from two
open disjoint subsets of $S$.
\end{proof}

\begin{example}
Let $S \subset \R$ be such that $x < z < y$ with $x,y \in S$
and $z \notin S$.  Claim: $S$ is not connected.  Proof:  Notice
\begin{equation*}
\bigl( (-\infty,z) \cap S \bigr)
\cup
\bigl( (z,\infty) \cap S \bigr)
= S .
\end{equation*}
\end{example}

\begin{prop}
A nonempty set $S \subset \R$ is connected if and only if it is
an interval or a single point.
\end{prop}

\begin{proof}
Suppose $S$ is connected.  If $S$ is a single point
then we are done.  So suppose $x < y$ and $x,y \in S$.  If $z$ is such
that $x < z < y$, then $(-\infty,z) \cap S$ is nonempty and $(z,\infty) \cap
S$ is nonempty.  The two sets are disjoint.  As
$S$ is connected, we must have they their union is not $S$, so $z \in S$.

Suppose $S$ is bounded, connected, but not a single point.
Let $\alpha := \inf \, S$ and
$\beta := \sup \, S$ and note that $\alpha < \beta$.  Suppose $\alpha < z < \beta$.  As $\alpha$ is the
infimum, then there is an $x \in S$ such that $\alpha \leq x < z$.  Similarly
there is a $y \in S$ such that $\beta \geq y > z$. 
We have shown above that $z \in S$, so $(\alpha,\beta) \subset S$.
If $w < \alpha$, then $w \notin S$
as $\alpha$ was the infimum,
similarly if $w > \beta$ then $w \notin S$.  Therefore the only
possibilities for $S$ are
$(\alpha,\beta)$,
$[\alpha,\beta)$,
$(\alpha,\beta]$,
$[\alpha,\beta]$.

The proof that an unbounded connected $S$ is an interval is left as an exercise.

On the other hand suppose $S$ is an interval.
Suppose $U_1$ and $U_2$ are open subsets of $\R$,
$U_1 \cap S$ and $U_2 \cap S$ are nonempty, and
$S = 
\bigl( U_1 \cap S \bigr)
\cup
\bigl( U_2 \cap S \bigr)$.  We will show that $U_1 \cap S$
and $U_2 \cap S$ contain a common point, so they are not disjoint,
and hence $S$ must be connected.
Suppose there is $x \in U_1 \cap S$
and $y \in U_2 \cap S$.  Without loss of generality, assume $x < y$.  As $S$ is an interval
$[x,y] \subset S$.  Let $z := \inf (U_2 \cap [x,y])$.
If $z = x$, then $z
\in U_1$.  If $z > x$,
then for any $\delta > 0$ the 
ball $B(z,\delta) =
(z-\delta,z+\delta)$ contains points of $S$ that are not
in $U_2$, and so $z \notin U_2$ as $U_2$ is open.
Therefore, $z \in U_1$.
As $U_1$ is open, $B(z,\delta) \subset U_1$ for a small enough $\delta >
0$.
As $z$ is the infimum of $U_2 \cap [x,y]$, 
there must exist some $w \in U_2 \cap [x,y]$
such that $w \in [z,z+\delta) \subset B(z,\delta) \subset U_1$.
Therefore $w \in U_1 \cap U_2 \cap [x,y]$.
So $U_1 \cap S$ and $U_2 \cap S$ are not disjoint and hence $S$ is connected.
\end{proof}

\begin{example}
In many cases a ball $B(x,\delta)$ is connected.  But this is not
necessarily true in every metric space.
For a simplest example, take a two point space $\{ a,
b\}$ with the discrete metric.  Then $B(a,2) = \{ a , b \}$, which is not
connected as $B(a,1) = \{ a \}$ and 
$B(b,1) = \{ b \}$ are open and disjoint.
\end{example}

\subsection{Closure and boundary}

Sometimes we wish to take a set and throw in everything that we can approach
from the set.  This concept is called the closure.

\begin{defn}
Let $(X,d)$ be a metric space and $A \subset X$.  Then
the \emph{\myindex{closure}} of $A$ is the set
\begin{equation*}
\overline{A} := \bigcap \{ E \subset X : \text{$E$ is closed and $A \subset
E$} \} .
\end{equation*}
That is, $\overline{A}$ is the intersection of all closed sets that contain
$A$.
\end{defn}

\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.  The closure $\overline{A}$
is closed.  Furthermore if $A$ is closed then $\overline{A} = A$.
\end{prop}

\begin{proof}
First, the closure is the intersection of closed sets, so it is closed.
Second, if $A$ is closed, then take $E = A$, hence the intersection of all
closed sets $E$ containing $A$ must be equal to $A$.
\end{proof}

\begin{example}
The closure of $(0,1)$ in $\R$ is $[0,1]$.  Proof:  Simply notice that if
$E$ is closed and contains $(0,1)$, then $E$ must contain $0$ and $1$ (why?).
Thus $[0,1] \subset E$.  But $[0,1]$ is also closed.
Therefore the closure $\overline{(0,1)} = [0,1]$.
\end{example}

\begin{example}
Be careful to notice what ambient metric space you are working with.
If $X = (0,\infty)$, then
the closure of $(0,1)$ in $(0,\infty)$ is $(0,1]$.  Proof:  Similarly as
above $(0,1]$ is closed in $(0,\infty)$ (why?).  Any closed set $E$
that contains $(0,1)$ must contain 1 (why?).  Therefore $(0,1] \subset E$,
and hence $\overline{(0,1)} = (0,1]$ when working in $(0,\infty)$.
\end{example}

Let us justify the statement that the closure is everything that we can
``approach'' from the set.

\begin{prop} \label{prop:msclosureappr}
Let $(X,d)$ be a metric space and $A \subset X$.  Then $x \in \overline{A}$
if and only if for every $\delta > 0$, $B(x,\delta) \cap A \not=\emptyset$.
\end{prop}

\begin{proof}
Let us prove the two contrapositives.
Let us show that $x \notin \overline{A}$ if and only if there exists
a $\delta > 0$ such that $B(x,\delta) \cap A = \emptyset$.

First suppose $x \notin \overline{A}$.  We know $\overline{A}$ is
closed.  Thus there is a $\delta > 0$ such that
$B(x,\delta) \subset \overline{A}^c$.  As $A \subset \overline{A}$ we
see that $B(x,\delta) \subset A^c$ and hence
$B(x,\delta) \cap A = \emptyset$.

On the other hand suppose there is a $\delta > 0$ such that
$B(x,\delta) \cap A = \emptyset$.  Then ${B(x,\delta)}^c$ is a closed set and we have
that $A \subset {B(x,\delta)}^c$, but
$x \notin {B(x,\delta)}^c$.  Thus as $\overline{A}$ is the intersection
of closed sets containing $A$, we have $x \notin \overline{A}$.
\end{proof}

We can also talk about what is in the interior of a set and what is on the
boundary.

\begin{defn}
Let $(X,d)$ be a metric space and $A \subset X$, then
the \emph{\myindex{interior}} of $A$ is the set
\begin{equation*}
A^\circ := \{ x \in A : \text{there exists a $\delta > 0$ such that
$B(x,\delta) \subset A$} \} .
\end{equation*}
The \emph{\myindex{boundary}} of $A$ is the set
\begin{equation*}
\partial A := \overline{A}\setminus A^\circ.
\end{equation*}
\end{defn}

\begin{example}
Suppose $A=(0,1]$ and $X = \R$.  Then it is not hard
to see that $\overline{A}=[0,1]$, $A^\circ = (0,1)$,
and $\partial A = \{ 0, 1 \}$.
\end{example}

\begin{example}
Suppose $X = \{ a, b \}$ with the discrete metric.
Let $A = \{ a \}$, then $\overline{A} = A^\circ = A$ and $\partial A =
\emptyset$.
\end{example}


\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.  Then $A^\circ$ is open
and $\partial A$ is closed.
\end{prop}

\begin{proof}
Given $x \in A^\circ$ we have $\delta > 0$ such that $B(x,\delta)
\subset A$.  If $z \in B(x,\delta)$, then as open balls are open,
there is an $\epsilon > 0$ such that $B(z,\epsilon) \subset B(x,\delta)
\subset A$, so $z$ is in $A^\circ$.  Therefore $B(x,\delta) \subset
A^\circ$ and so $A^\circ$ is open.

As $A^\circ$ is open, then
$\partial A = \overline{A} \setminus A^\circ = \overline{A} \cap
{(A^\circ)}^c$ is closed.
\end{proof}

The boundary is the set of points that are close to both the set and its
complement.  See \figureref{figmsboundary} for the a diagram
of the next proposition.

\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.  Then $x \in \partial A$
if and only if for every $\delta > 0$,
$B(x,\delta) \cap A$ and
$B(x,\delta) \cap A^c$ are both nonempty.
\end{prop}

\begin{myfigureht}
\subimport*{figures/}{msboundary.pdf_t}
\caption{Boundary is the set where every ball contains points in the set and
also its complement.\label{figmsboundary}}
\end{myfigureht}

\begin{proof}
Suppose $x \in \partial A =  \overline{A} \setminus A^\circ$ and
let $\delta > 0$ be arbitrary.
By \propref{prop:msclosureappr}, $B(x,\delta)$ contains
a point from $A$.  If $B(x,\delta)$ contained no points of $A^c$,
then $x$ would be in $A^\circ$.  Hence $B(x,\delta)$ contains a point of
$A^c$ as well.

Let us prove the other direction by contrapositive.
If $x \notin \overline{A}$, then there is some $\delta > 0$ such that
$B(x,\delta) \subset \overline{A}^c$ as $\overline{A}$ is closed.
So $B(x,\delta)$ contains no points of $A$, because $\overline{A}^c \subset
A^c$.

Now suppose $x \in A^\circ$, then there exists a $\delta > 0$
such that $B(x,\delta) \subset A$, but that means $B(x,\delta)$
contains no points of $A^c$.
\end{proof}

We obtain the following immediate corollary about closures of $A$ and $A^c$.  We
simply apply \propref{prop:msclosureappr}.

\begin{cor}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $\partial A = \overline{A} \cap \overline{A^c}$.
\end{cor}

\subsection{Exercises}

\begin{exercise}
Prove \propref{prop:topology:closed}.  Hint: Consider the complements of the
sets and apply \propref{prop:topology:open}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:topology:ballsopenclosed} by
proving that $C(x,\delta)$ is closed.
\end{exercise}

\begin{exercise}
Prove \propref{prop:topology:intervals:openclosed}.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is a nonempty metric space with the discrete topology.  Show
that $X$ is connected if and only if it contains exactly one element.
\end{exercise}

\begin{exercise}
Show that if $S \subset \R$ is a connected unbounded set, then it is an
(unbounded) interval.
\end{exercise}

\begin{exercise}
Show that every open set can be written as a union of closed sets.
\end{exercise}

\begin{exercise}
a) Show that $E$ is closed if and only if $\partial E \subset E$.
b) Show that $U$ is open if and only if $\partial U \cap U = \emptyset$.
\end{exercise}

\begin{exercise}
a) Show that $A$ is open if and only if $A^\circ = A$.
b) Suppose that $U$ is an open set and $U \subset A$.  Show
that $U \subset A^\circ$.
\end{exercise}

\begin{exercise}
Let $X$ be a set and $d$, $d'$ be two metrics on $X$.
Suppose there exists an $\alpha > 0$ and $\beta > 0$
such that $\alpha d(x,y) \leq d'(x,y) \leq \beta d(x,y)$ for all $x,y \in X$.
Show that $U$ is open in $(X,d)$ if and only if $U$ is open in $(X,d')$.
That is, the topologies of $(X,d)$ and $(X,d')$ are the same.
\end{exercise}


\begin{exercise}
Suppose $\{ S_i \}$, $i \in \N$
is a collection of connected subsets of a metric space $(X,d)$.  Suppose
there exists an $x \in X$ such that $x \in S_i$ for all $i \in \N$.
Show that $\bigcup_{i=1}^\infty S_i$ is connected.
\end{exercise}

\begin{exercise}
Let $A$ be a connected set.
a) \nolinebreak Is $\overline{A}$ connected?  Prove or find a counterexample.
b) \nolinebreak Is $A^\circ$ connected?  Prove or find a counterexample.
Hint: Think of sets in $\R^2$.
\end{exercise}

\begin{exnote}
The definition of open sets in the following exercise is usually called the
\emph{\myindex{subspace topology}}.  You are asked to show that
we obtain the same topology by considering the subspace metric.
\end{exnote}

\begin{exercise} \label{exercise:mssubspace}
Suppose $(X,d)$ is a metric space and $Y \subset X$.  Show that
with the subspace metric on $Y$, a set $U \subset Y$
is open (in $Y$) whenever there exists an open set $V \subset X$ such
that $U = V \cap Y$.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.
a) For any $x \in X$ and $\delta > 0$, show
$\overline{B(x,\delta)} \subset C(x,\delta)$.
b) Is it always true that
$\overline{B(x,\delta)} = C(x,\delta)$?  Prove or find a counterexample.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space and $A \subset X$.  Show that
$A^\circ = \bigcup \{ V : V \subset A \text{ is open} \}$.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.  Show that there exists a bounded metric
$d'$ such that $(X,d')$ has the same open sets, that is, the topology is
the same.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.\\
a) Prove that for every $x \in X$, there either exists a $\delta > 0$ such that
$B(x,\delta) = \{ x \}$, or $B(x,\delta)$ is infinite for every $\delta >  0$.
\\
b) Find an explicit example of $(X,d)$ where for every $\delta > 0$ and
every $x \in X$, the
ball $B(x,\delta)$ is finite.
\\
c) Find an explicit example of $(X,d)$ where for every $\delta > 0$ and
every $x \in X$, the
ball $B(x,\delta)$ is countably infinite.
\\
d) Prove that if $X = \R$, then no matter what the metric is, there exists
an $x \in X$ and a $\delta > 0$ such that $B(x,\delta)$ is uncountable.
\end{exercise}

\begin{exercise}
For every $x \in \R^n$ and every $\delta > 0$ define the rectangle
$R(x,\delta) :=
(x_1-\delta,x_1+\delta) \times
(x_2-\delta,x_2+\delta) \times \cdots \times
(x_n-\delta,x_n+\delta)$.  Show that these sets generate the same open
sets as the balls.  That is show that a set $U \subset \R^n$
is open in the sense of the standard metric if and only if for every
point $x \in U$, there exists a $\delta > 0$ such that $R(x,\delta) \subset
U$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Sequences and convergence}
\label{sec:metseqs}

\sectionnotes{1 lecture}

\subsection{Sequences}

The notion of a sequence in a metric space is very similar to a sequence of
real numbers.
The definitions are essentially the same as those for real numbers
in the sense of \chapterref{seq:chapter}, where $\R$ with
the standard metric $d(x,y)=\abs{x-y}$ is replaced
by an arbitrary metric space $(X,d)$.


\begin{defn}
A \emph{\myindex{sequence}} in a metric space $(X,d)$ is a function
$x \colon \N \to X$.  As before we write $x_n$ for the $n$th element in
the sequence and use the notation $\{ x_n \}$, or more precisely
\begin{equation*}
\{ x_n \}_{n=1}^\infty .
\end{equation*}

A sequence $\{ x_n \}$ is \emph{bounded}\index{bounded sequence} if
there exists a point $p \in X$ and $B \in \R$ such that
\begin{equation*}
d(p,x_n) \leq B \qquad \text{for all $n \in \N$.}
\end{equation*}
In other words, the sequence $\{x_n\}$ is bounded whenever
the set $\{ x_n : n \in \N \}$
is bounded.

If $\{ n_j \}_{j=1}^\infty$ is a sequence of natural numbers
such that $n_{j+1} > n_j$ for all $j$, then
the sequence $\{ x_{n_j} \}_{j=1}^\infty$ is said to be
a \emph{\myindex{subsequence}} of $\{x_n \}$.
\end{defn}

Similarly we define convergence.  Again, we cheat a little
and use the definite article in front of the word \emph{limit}
before we prove that the limit is unique.
See \figureref{fig:sequence-convergence-metric}, for an idea of
the definition.

\begin{defn}
A sequence $\{ x_n \}$ in a metric space $(X,d)$ is said
to \emph{\myindex{converge}} to a point
$p \in X$, if for every $\epsilon > 0$, there exists an $M \in \N$ such
that $d(x_n,p) < \epsilon$ for all $n \geq M$.  The point $p$
is said to be the \emph{limit}\index{limit of a sequence}
of $\{ x_n \}$.  We write
\begin{equation*}
\lim_{n\to \infty} x_n := p .
\end{equation*}

A sequence
that converges is \emph{convergent}\index{convergent sequence}.
Otherwise, the sequence is 
\emph{divergent}\index{divergent sequence}.
\begin{myfigureht}
%\begin{center}
% to get the fonts right input eepic
\subimport*{figures/}{sequence-convergence-metric.eepic}
\caption{Sequence converging to $p$.  The first 10 points 
are shown and $M=7$ for this $\epsilon$.\label{fig:sequence-convergence-metric}}
%\end{center}
\end{myfigureht}
\end{defn}

Let us prove that the limit is unique.  The proof is almost
identical (word for word) to the proof of the same fact for sequences of
real numbers,
\propref{prop:limisunique}.
Proofs of many results we know for sequences of real numbers can be
adapted to
the more general settings of metric spaces.  We must replace $\abs{x-y}$
with $d(x,y)$ in the proofs and apply the triangle inequality correctly.

\begin{prop} \label{prop:mslimisunique}
A convergent sequence in a metric space has a unique limit.
\end{prop}

\begin{proof}
%NOTE: should be word for word the same as 2.1.6
Suppose the sequence $\{ x_n \}$ has the limit $x$ and the limit $y$.
Take an arbitrary $\epsilon > 0$.
From the definition find an $M_1$ such that for all $n \geq M_1$,
$d(x_n,x) < \nicefrac{\epsilon}{2}$.  Similarly find an $M_2$
such that for all $n \geq M_2$ we have
$d(x_n,y) < \nicefrac{\epsilon}{2}$.  Now take an $n$ such that
$n \geq M_1$ and also $n \geq M_2$, and estimate
\begin{equation*}
\begin{split}
d(y,x)
& \leq
d(y,x_n) + d(x_n,x) \\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
As $d(y,x) < \epsilon$ for all $\epsilon > 0$, then $d(x,y) = 0$
and $y=x$.  Hence the limit (if it exists) is unique.
\end{proof}

The proofs of the following propositions are left as exercises.

\begin{prop} \label{prop:msconvbound}
A convergent sequence in a metric space is bounded.
\end{prop}

\begin{prop} \label{prop:msconvifa}
A sequence $\{ x_n \}$ in a metric space $(X,d)$ converges to $p \in X$
if and only
if there exists a sequence $\{ a_n \}$ of real numbers such that
\begin{equation*}
d(x_n,p) \leq a_n \quad \text{for all $n \in \N$},
\end{equation*}
and
\begin{equation*}
\lim_{n\to\infty} a_n = 0.
\end{equation*}
\end{prop}

\begin{prop} \label{prop:mssubseq}
Let $\{ x_n \}$ be a sequence in a metric space $(X,d)$.
\begin{enumerate}[(i)]
\item If $\{ x_n \}$ converges to $p \in X$, then every subsequence $\{ x_{n_k} \}$
converges to $p$.
\item If for some $K \in \N$ the $K$-tail $\{ x_n \}_{n=K+1}^\infty$
converges to $p \in X$, then
 $\{ x_n \}$ converges to $p$.
\end{enumerate}
\end{prop}

\begin{example}
Take $C([0,1],\R)$ be the set of continuous functions with the metric being
the uniform metric.  We saw that we obtain a metric space.
If we look at the definition of convergence, we notice that it is identical
to uniform convergence.  That is $\{ f_n \}$ converges uniformly if and only
if it converges in the metric space sense.
\end{example}

\begin{remark}
It is perhaps surprising that on the set of functions $f \colon [a,b] \to
\R$ (continuous or not)
there is no metric that gives pointwise convergence.  Although the proof of
this fact is beyond the scope of this book.
\end{remark}

\subsection{Convergence in euclidean space}

It is useful to note what convergence means in the euclidean space
$\R^n$.

\begin{prop} \label{prop:msconveuc}
Let $\{ x_j \}_{j=1}^\infty$ be a sequence in $\R^n$,
where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
Then $\{ x_j \}_{j=1}^\infty$ converges if and only if
$\{ x_{j,k} \}_{j=1}^\infty$ converges for every $k$, in which case
\begin{equation*}
\lim_{j\to\infty}
x_j =
\Bigl(
\lim_{j\to\infty} x_{j,1},
\lim_{j\to\infty} x_{j,2},
\ldots,
\lim_{j\to\infty} x_{j,n}
\Bigr) .
\end{equation*}
\end{prop}

%For $\R = \R^1$ the result is immediate.
%So let $n > 1$.

\begin{proof}
Let $\{ x_j \}_{j=1}^\infty$ be a convergent sequence
in $\R^n$, where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
Let $y = (y_1,y_2,\ldots,y_n) \in \R^n$ be the limit.
Given $\epsilon > 0$, there exists an $M$ such that for all
$j \geq M$ we have
\begin{equation*}
d(y,x_j) < \epsilon.
\end{equation*}
Fix some $k=1,2,\ldots,n$.  For $j \geq M$ we have
\begin{equation*}
\bigl\lvert y_k - x_{j,k} \bigr\rvert
=
\sqrt{{\bigl(y_k - x_{j,k} \bigr)}^2}
\leq
\sqrt{\sum_{\ell=1}^n {\bigl(y_\ell-x_{j,\ell}\bigr)}^2}
= d(y,x_j) < \epsilon .
\end{equation*}
Hence the sequence $\{ x_{j,k} \}_{j=1}^\infty$ converges to $y_k$.

For the other direction suppose 
$\{ x_{j,k} \}_{j=1}^\infty$ converges to $y_k$ for every $k=1,2,\ldots,n$.
Hence, given $\epsilon > 0$, pick an $M$, such that if $j \geq M$ then 
$\bigl\lvert y_k-x_{j,k} \bigr\rvert < \nicefrac{\epsilon}{\sqrt{n}}$ for all
$k=1,2,\ldots,n$.  Then
\begin{equation*}
d(y,x_j)
=
\sqrt{\sum_{k=1}^n {\bigl(y_k-x_{j,k}\bigr)}^2}
<
\sqrt{\sum_{k=1}^n {\left(\frac{\epsilon}{\sqrt{n}}\right)}^2}
=
\sqrt{\sum_{k=1}^n \frac{{\epsilon^2}}{n}}
= \epsilon .
\end{equation*}
The sequence $\{ x_j \}$ converges to $y \in \R^n$ and we are done.
\end{proof}

\begin{example}
As we said, the set $\C$ of complex numbers $z = x+iy$ is considered 
as the metric space $\R^2$.  The proposition says that the
sequence $\{ z_n \}_{n=1}^\infty = \{ x_n + iy_n \}_{n=1}^\infty$ converges
to $z = x+iy$
if and only if $\{ x_n \}$ converges to $x$ and 
$\{ y_n \}$ converges to $y$.
\end{example}

\subsection{Convergence and topology}

The topology, that is, the set of open sets of a space encodes which
sequences converge.

\begin{prop} \label{prop:msconvtopo}
Let $(X,d)$ be a metric space and $\{x_n\}$ a sequence in $X$.  Then
$\{ x_n \}$ converges to $x \in X$ if and only if for every open neighborhood
$U$ of $x$, there exists an $M \in \N$ such that for all $n \geq M$
we have $x_n \in U$.
\end{prop}

\begin{proof}
First suppose $\{ x_n \}$ converges.  Let $U$ be an open neighborhood
of $x$, then there exists an $\epsilon > 0$ such that $B(x,\epsilon) \subset
U$.  As the sequence converges, find an $M \in \N$ such that for all $n \geq
M$ we have $d(x,x_n) < \epsilon$ or in other words $x_n \in B(x,\epsilon)
\subset U$.

Let us prove the other direction.  Given $\epsilon > 0$ let $U :=
B(x,\epsilon)$ be the neighborhood of $x$.  Then there is an $M \in \N$
such that for $n \geq M$ we have $x_n \in U = B(x,\epsilon)$ or in other
words, $d(x,x_n) < \epsilon$.
\end{proof}

A set is closed when it contains the limits of its convergent sequences.

\begin{prop} \label{prop:msclosedlim}
Let $(X,d)$ be a metric space, $E \subset X$ a closed set
and $\{ x_n \}$ a sequence in $E$ that converges to some $x \in X$.
Then $x \in E$.
\end{prop}

\begin{proof}
Let us prove the contrapositive.
Suppose $\{ x_n \}$ is a sequence in $X$ that converges to $x \in E^c$.
As $E^c$ is open, \propref{prop:msconvtopo} says there is
an $M$ such that for all $n \geq M$,
$x_n \in E^c$.  So $\{ x_n \}$  is not a sequence in $E$.
%Let $\{ x_n \}$ be a sequence in $E$ that converges to some $x \in X$.
%Take $y \in E^c$, as $E^c$ is open then there exists a $\delta > 0$
%such that $B(y,\delta) \subset E^c$, or in particular $d(y,x_n) \geq \delta$
%for all $n$ as $x_n \in E$.  In particular, $\lim\, x_n = x \not= y$, and so
%$x \in E$.
\end{proof}

When we take a closure of a set $A$, we really throw in precisely 
those points that are limits of sequences in $A$.

\begin{prop} \label{prop:msclosureapprseq}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $x \in \overline{A}$ if and only if there exists a sequence $\{ x_n \}$ of
elements in $A$ such that $\lim\, x_n = x$.
\end{prop}

\begin{proof}
Let $x \in \overline{A}$.  For every $n \in \N$,
by
\propref{prop:msclosureappr} there
exists a point $x_n \in B(x,\nicefrac{1}{n}) \cap A$.
As $d(x,x_n) < \nicefrac{1}{n}$, we have $\lim\, x_n = x$.

For the other direction, see \exerciseref{exercise:reverseclosedseq}.
%Exercise
%Now suppose
%$\{x_n\}$ a sequence in $A$ convergent to some $x \in X$.
%Take any closed $E$ such that $A \subset E$.  The sequence is
%in $E$ and so by
%\propref{prop:msclosedlim}, $x \in E$.  As $E$ was arbitrary, $x \in
%\overline{A}$.
\end{proof}

\subsection{Exercises}

\begin{exercise} \label{exercise:reverseclosedseq}
Let $(X,d)$ be a metric space and
let $A \subset X$.  Let $E$ be the set of all $x \in X$ such that there
exists a sequence $\{ x_n \}$ in $A$ that converges to $x$.  Show 
$E = \overline{A}$.
\end{exercise}

\begin{exercise}
a) Show that $d(x,y) := \min \{ 1, \abs{x-y} \}$ defines a metric on $\R$.
b) Show that a sequence converges in $(\R,d)$ if and only if it converges
in the standard metric.  c) Find a bounded sequence in $(\R,d)$ that
contains no convergent subsequence.
\end{exercise}

\begin{exercise}
Prove \propref{prop:msconvbound}.
\end{exercise}

\begin{exercise}
Prove \propref{prop:msconvifa}.
\end{exercise}

\begin{exercise}
Suppose $\{x_n\}_{n=1}^\infty$ converges to $x$.  Suppose $f \colon \N
\to \N$ is a one-to-one function.  Show that
$\{ x_{f(n)} \}_{n=1}^\infty$ converges to $x$.
\end{exercise}

\begin{exercise}
If $(X,d)$ is a metric space where $d$ is the discrete metric.  Suppose 
$\{ x_n \}$ is a convergent sequence in $X$.  Show that there exists
a $K \in \N$ such that for all $n \geq K$ we have $x_n = x_K$.
\end{exercise}

\begin{exercise}
A set $S \subset X$ is said to be dense in $X$ if for every $x \in X$,
there exists a sequence $\{ x_n \}$ in $S$ that converges to $x$.  Prove
that $\R^n$ contains a countable dense subset.
\end{exercise}

\begin{exercise}[Tricky]
Suppose $\{ U_n \}_{n=1}^\infty$ be a decreasing ($U_{n+1} \subset U_n$ for
all $n$) sequence of open sets in a metric space $(X,d)$ such that
$\bigcap_{n=1}^\infty U_n = \{ p \}$ for some $p \in X$.  Suppose 
$\{ x_n \}$ is a sequence of points in $X$ such that $x_n \in U_n$.  Does
$\{ x_n \}$ necessarily converge to $p$?  Prove or construct a counterexample.
\end{exercise}

\begin{exercise}
Let $E \subset X$ be closed and
let $\{ x_n \}$ be a sequence in $X$ converging to $p \in X$.  Suppose
$x_n \in E$ for infinitely many $n \in \N$.  Show $p \in E$.
\end{exercise}

\begin{exercise}
Take $\R^* = \{ -\infty \} \cup \R \cup \{ \infty \}$ be the extended reals.
Define $d(x,y) := \bigl\lvert \frac{x}{1+\abs{x}} - \frac{y}{1+\abs{y}}
\bigr\rvert$
if $x, y \in \R$,
define $d(\infty,x) := \bigl\lvert 1 - \frac{x}{1+\abs{x}} \bigr\rvert$,
$d(-\infty,x) := \bigl\lvert 1 + \frac{x}{1+\abs{x}} \bigr\rvert$
for all $x \in \R$, and
let $d(\infty,-\infty) := 2$.
a)~Show that $(\R^*,d)$ is a metric space.
b)~Suppose $\{ x_n \}$ is a sequence of real numbers such that
for every $M \in \R$, there exists an $N$ such that
$x_n \geq M$ for all $n \geq N$.  Show that $\lim\, x_n = \infty$ in
$(\R^*,d)$.
c)~Show that a sequence of real numbers converges to a real number
in $(\R^*,d)$ if and
only if it converges in $\R$ with the standard metric.
\end{exercise}

\begin{exercise}
Suppose $\{ V_n \}_{n=1}^\infty$ is a collection of open sets
in $(X,d)$
such that $V_{n+1} \supset V_n$.  Let $\{ x_n \}$ be a sequence
such that $x_n \in V_{n+1} \setminus V_n$ and suppose 
$\{ x_n \}$ converges to $p \in X$.  Show that $p \in \partial V$
where $V = \bigcup_{n=1}^\infty V_n$.
\end{exercise}

\begin{exercise}
Prove \propref{prop:mssubseq}.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Completeness and compactness}
\label{sec:metcompact}

\sectionnotes{2 lectures}

\subsection{Cauchy sequences and completeness}

Just like with sequences of real numbers we define Cauchy sequences.

\begin{defn}
Let $(X,d)$ be a metric space.
A sequence $\{ x_n \}$ in $X$ is a \emph{\myindex{Cauchy sequence}} if
for every $\epsilon > 0$ there exists an $M \in \N$ such that
for all $n \geq M$ and all $k \geq M$ we have
\begin{equation*}
d(x_n, x_k) < \epsilon .
\end{equation*}
\end{defn}

The definition is again simply a translation of the concept
from the real numbers to metric spaces.  So a sequence of real
numbers is Cauchy in the sense of \chapterref{seq:chapter} if and only if
it is Cauchy in the sense above, provided we equip the real numbers with
the standard metric $d(x,y) = \abs{x-y}$.

\begin{prop}
A convergent sequence in a metric space is Cauchy.
\end{prop}

\begin{proof}
Suppose $\{ x_n \}$ converges to $x$.
Given $\epsilon > 0$ there is an $M$ such that for $n \geq M$
we have $d(x,x_n) < \nicefrac{\epsilon}{2}$.  Hence
for all $n,k \geq M$ we have
$d(x_n,x_k) \leq d(x_n,x) + d(x,x_k) < \nicefrac{\epsilon}{2} +
\nicefrac{\epsilon}{2} = \epsilon$.
\end{proof}

\begin{defn}
Let $(X,d)$ be a metric space.  We say $X$ is
\emph{\myindex{complete}} or \emph{\myindex{Cauchy-complete}}
if every Cauchy sequence $\{ x_n \}$ in $X$
converges to an $x \in X$.
\end{defn}

\begin{prop}
The space $\R^n$ with the standard metric is a complete metric space.
\end{prop}

For $\R = \R^1$ completeness was proved in \chapterref{seq:chapter}.  The proof of
the above proposition is a reduction to the one dimensional case.

\begin{proof}
%Take $n > 1$.
Let $\{ x_j \}_{j=1}^\infty$ be a Cauchy sequence
in $\R^n$, where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
As the sequence is Cauchy, given $\epsilon > 0$, there exists an $M$ such that for all
$i,j \geq M$ we have
\begin{equation*}
d(x_i,x_j) < \epsilon.
\end{equation*}

Fix some $k=1,2,\ldots,n$, for $i,j \geq M$ we have
\begin{equation*}
\bigl\lvert x_{i,k} - x_{j,k} \bigr\rvert
=
\sqrt{{\bigl(x_{i,k} - x_{j,k}\bigr)}^2}
\leq
\sqrt{\sum_{\ell=1}^n {\bigl(x_{i,\ell}-x_{j,\ell}\bigr)}^2}
= d(x_i,x_j) < \epsilon .
\end{equation*}
Hence the sequence $\{ x_{j,k} \}_{j=1}^\infty$ is Cauchy.  As $\R$ is
complete the sequence converges; there exists an $y_k \in \R$ such that
$y_k = \lim_{j\to\infty} x_{j,k}$.

Write $y = (y_1,y_2,\ldots,y_n) \in \R^n$.
By \propref{prop:msconveuc} we have that $\{ x_j \}$ converges
to $y \in \R^n$ and hence $\R^n$ is complete.
\end{proof}

Note that a subset of $\R^n$ with the subspace metric need not be
complete.  For example, $(0,1]$ with the subspace metric is not
complete as $\{ \nicefrac{1}{n} \}$ is a Cauchy sequence in $(0,1]$
with no limit in $(0,1]$.  But see also
\exerciseref{exercise:closedcomplete}.

\subsection{Compactness}

\begin{defn}
Let $(X,d)$ be a metric space and $K \subset X$. 
The set $K$ is said to be \emph{\myindex{compact}}
if for any collection
of open sets $\{ U_{\lambda} \}_{\lambda \in I}$ such that
\begin{equation*}
K \subset \bigcup_{\lambda \in I} U_\lambda ,
\end{equation*}
there exists a finite subset
$\{ \lambda_1, \lambda_2,\ldots,\lambda_k \} \subset I$
such that
\begin{equation*}
K \subset \bigcup_{j=1}^k U_{\lambda_j} .
\end{equation*}
\end{defn}

A collection of open sets $\{ U_{\lambda} \}_{\lambda \in I}$ as above is
said to be a \emph{\myindex{open cover}} of $K$.  So a way to say that
$K$ is compact is to say that \emph{every open cover of $K$ has a finite
\myindex{subcover}}.

\begin{example}
Let $\R$ be the metric space with the standard metric.

The set $\R$ is not compact.  Proof: Take the sets $U_j := (-j,j)$.
Any $x \in \R$ is in some $U_j$ (by the
\hyperref[thm:arch:i]{Archimedean property}), so we have an open cover.
Suppose we have a finite
subcover $\R \subset U_{j_1} \cup U_{j_2} \cup \cdots \cup U_{j_k}$,
and suppose $j_1 < j_2 < \cdots < j_k$.  Then $\R \subset U_{j_k}$, but that is
a contradiction as $j_k \in \R$ on one hand and $j_k \notin U_{j_k}$ on the
other.

The set $(0,1) \subset \R$ is also not compact.  Proof:  Take the 
sets $U_{j} := (\nicefrac{1}{j},1-\nicefrac{1}{j})$ for $j=3,4,5,\ldots$.
As above $(0,1) = \bigcup_{j=3}^\infty U_j$.  And similarly as above,
if there exists a finite subcover, then there is one $U_j$ such that $(0,1)
\subset U_j$, which again leads to a contradiction.

The set $\{ 0 \} \subset \R$ is compact.  Proof: Given any open cover $\{
U_{\lambda} \}_{\lambda \in I}$, there must exist a $\lambda_0$ such that $0
\in U_{\lambda_0}$ as it is a cover.  But then $U_{\lambda_0}$ gives a
finite subcover.

We will prove below that $[0,1]$, and in fact any closed and bounded
interval $[a,b]$, is compact by proving
that in $\R$, a closed and bounded set is compact.
\end{example}

\begin{prop}
Let $(X,d)$ be a metric space.  A compact set $K \subset X$ is closed and
bounded.
\end{prop}

\begin{proof}
First, we prove that a compact set is bounded.
Fix $p \in X$.  We have the open cover
\begin{equation*}
K \subset \bigcup_{n=1}^\infty B(p,n) = X .
\end{equation*}
If $K$ is compact, then there exists some set of indices
$n_1 < n_2 < \ldots < n_k$ such that
\begin{equation*}
K \subset \bigcup_{j=1}^k B(p,n_j) = B(p,n_k) .
\end{equation*}
As $K$ is contained in a ball, $K$ is bounded.

Next, we show a set that is not closed is not compact.  Suppose 
$\overline{K} \not= K$, that is, there is a point $x \in \overline{K}
\setminus K$.
If $y \not= x$, then for $n$
with $\nicefrac{1}{n} < d(x,y)$ we have
$y \notin C(x,\nicefrac{1}{n})$. Furthermore $x \notin K$, so
\begin{equation*}
K \subset \bigcup_{n=1}^\infty {C(x,\nicefrac{1}{n})}^c .
\end{equation*}
As a closed ball is closed, ${C(x,\nicefrac{1}{n})}^c$ is open, and
so we have an open cover.
If we take any
finite collection of indices $n_1 < n_2 < \ldots < n_k$, then 
\begin{equation*}
\bigcup_{j=1}^k {C(x,\nicefrac{1}{n_j})}^c 
=
{C(x,\nicefrac{1}{n_k})}^c 
\end{equation*}
As $x$ is in the closure,
$C(x,\nicefrac{1}{n_k}) \cap K \not= \emptyset$.  So there is no
finite subcover and $K$ is not compact.
\end{proof}

We prove below that 
in finite dimensional euclidean space
every closed bounded set is compact.
So closed bounded sets
of $\R^n$ are examples of compact sets.
It is not true that in every metric space, closed and bounded is equivalent
to compact.  A simple example would be an incomplete metric space such as
$(0,1)$ with the subspace metric.
But there are many complete and very useful metric spaces where closed and bounded is not
enough to give compactness, see
\exerciseref{exercise:msclbounnotcompt}: $C([a,b],\R)$ is a complete metric
space, but the closed unit ball $C(0,1)$ is not compact.  However, see
\exerciseref{exercise:mstotbound}.

A useful property of compact sets in a metric space is that every
sequence has a convergent subsequence.  Such sets are sometimes called
\emph{\myindex{sequentially compact}}.  Let us prove that in the
context of metric spaces, a set is compact if and only if it is sequentially
compact.
First we prove a lemma.

\begin{lemma}[Lebesgue covering lemma%
\footnote{Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Lebesgue}{Henri L\'eon Lebesgue}
(1875 -- 1941).
The number $\delta$ is sometimes called the \myindex{Lebesgue number} of the
cover.}]\label{ms:lebesgue}
\index{Lebesgue covering lemma}
Let $(X,d)$ be a metric space and $K \subset X$.  Suppose 
every sequence in $K$ has a subsequence convergent in $K$.  Given
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$, there exists a
$\delta > 0$ such that for every $x \in K$, there exists a $\lambda \in I$
with $B(x,\delta) \subset U_\lambda$.
\end{lemma}

It is important to recognize what the lemma says.  It says that given any
cover there is a single $\delta > 0$.  The $\delta$ can depend on the cover,
but of course it does not depend on $x$.

\begin{proof}
Let us prove the lemma by contrapositive.
If the conclusion is not true, then
there is
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$ with
the following property.
For every $n \in \N$ there exists an $x_n \in K$ such that
$B(x_n,\nicefrac{1}{n})$ is not a subset of any $U_\lambda$.
Given any $x \in K$, there is
a $\lambda \in I$ such that $x \in U_\lambda$.  Hence there
is an $\epsilon > 0$ 
such that $B(x,\epsilon) \subset U_\lambda$.  Take $M$ such that
$\nicefrac{1}{M} < \nicefrac{\epsilon}{2}$.  If $y \in 
B(x,\nicefrac{\epsilon}{2})$ and $n \geq M$, then
by triangle inequality
\begin{equation*}
B(y,\nicefrac{1}{n}) \subset
B(y,\nicefrac{1}{M}) \subset B(y,\nicefrac{\epsilon}{2}) \subset
B(x,\epsilon) \subset U_\lambda .
\end{equation*}
In other words, for all $n \geq M$, $x_n \notin B(x,\nicefrac{\epsilon}{2})$. 
Hence the sequence cannot have a subsequence converging to $x$.  As $x \in K$ was
arbitrary we are done.
\end{proof}

\begin{thm} \label{thm:mscompactisseqcpt}
Let $(X,d)$ be a metric space.  Then $K \subset X$ is a compact set if
and only if every sequence in $K$ has a subsequence converging to
a point in $K$.
\end{thm}

\begin{proof}
Let $K \subset X$ be a set and
$\{ x_n \}$ a sequence in $K$.  Suppose that for each $x \in K$,
there is a ball $B(x,\alpha_x)$ for some $\alpha_x > 0$ such that
$x_n \in B(x,\alpha_x)$ for only finitely many $n \in \N$.
Then
\begin{equation*}
K \subset \bigcup_{x \in K} B(x,\alpha_x) .
\end{equation*}
Any finite collection of these balls is going to contain only finitely many
$x_n$.  Thus for any finite collection of such balls there is an $x_n \in K$
that is not in the union.  Therefore, $K$ is not compact.

So if $K$ is compact,
then there exists an $x \in K$ such that
for any $\delta > 0$,
$B(x,\delta)$ contains $x_k$ for infinitely many $k \in \N$.
The ball $B(x,1)$ contains some $x_k$ so let $n_1 := k$.
If $n_{j-1}$ is defined, then there must
exist a $k > n_{j-1}$ such that $x_k \in B(x,\nicefrac{1}{j})$, so define
$n_j := k$.  Notice that
$d(x,x_{n_j}) < \nicefrac{1}{j}$.  By \propref{prop:msconvifa},
$\lim\, x_{n_j} = x$.

For the other direction, suppose every sequence in $K$
has a 
subsequence converging in $K$.
Take
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$.
Using the Lebesgue covering lemma above, we find a $\delta > 0$
such that for every $x$, there is a $\lambda \in I$ with
$B(x,\delta) \subset U_\lambda$.

Pick $x_1 \in K$ and find $\lambda_1 \in I$ such that $B(x_1,\delta) \subset
U_{\lambda_1}$.
If $K \subset U_{\lambda_1}$, we stop as we have found a
finite subcover.
Otherwise, there must be
a point $x_2 \in K \setminus U_{\lambda_1}$.
Note that $d(x_2,x_1) \geq \delta$.
There must exist some $\lambda_2 \in I$ such that
$B(x_2,\delta) \subset U_{\lambda_2}$.
We work inductively.  Suppose $\lambda_{n-1}$ is defined.
Either
$U_{\lambda_1} \cup
U_{\lambda_2} \cup \cdots \cup
U_{\lambda_{n-1}}$ is a finite cover of $K$, in which case we
stop, or
there must be 
a point $x_n \in K \setminus \bigl( U_{\lambda_1} \cup
U_{\lambda_2} \cup \cdots \cup
U_{\lambda_{n-1}}\bigr)$.
Note that $d(x_n,x_j) \geq \delta$ for all $j = 1,2,\ldots,n-1$.
Next, there must be some $\lambda_n \in I$
such that $B(x_n,\delta) \subset U_{\lambda_n}$.

Either at some point we obtain a finite subcover of $K$
or we obtain an
infinite
sequence $\{ x_n \}$ as above.
For contradiction suppose that
there is no finite subcover and we have the sequence $\{ x_n \}$.
For all $n$ and $k$, $n \not= k$, 
we have $d(x_n,x_k) \geq \delta$,
so no subsequence of $\{ x_n \}$ can be
Cauchy.  Hence no subsequence of $\{ x_n \}$ can be convergent,
which is a contradiction.
\end{proof}

\begin{example}
The Bolzano--Weierstrass theorem for sequences of real numbers
(\thmref{thm:bwseq})
says that any bounded sequence in $\R$ has a convergent
subsequence.  Therefore any sequence in a closed interval $[a,b] \subset \R$ has 
a convergent subsequence.  The limit must also be in $[a,b]$ as limits
preserve non-strict inequalities.  Hence a closed bounded interval $[a,b]
\subset \R$ is compact.
\end{example}

\begin{prop}
Let $(X,d)$ be a metric space and let $K \subset X$ be compact.  If
$E \subset K$ is a closed set, then $E$ is compact.
\end{prop}

\begin{proof}
Let $\{ x_n \}$ be a sequence in $E$.  It is also a sequence in $K$.
Therefore it has a convergent subsequence $\{ x_{n_j} \}$ that converges to
some $x \in K$.  As $E$ is closed the limit of a sequence in $E$ is also in $E$
and so $x \in E$.  Thus $E$ must be compact.
\end{proof}

\begin{thm}[Heine--Borel%
\footnote{Named after the German mathematician 
\href{http://en.wikipedia.org/wiki/Eduard_Heine}{Heinrich Eduard Heine}
(1821--1881),
and the French mathematician
\href{http://en.wikipedia.org/wiki/\%C3\%89mile_Borel}{F\'elix \'Edouard Justin \'Emile Borel}
(1871--1956).}]%
\index{Heine--Borel theorem}
\label{thm:msbw}
A closed bounded subset $K \subset \R^n$ is compact.
\end{thm}

So subsets of $\R^n$ are compact if and only if they are closed and bounded,
a condition that is much easier to check.
Let us reiterate that the Heine--Borel theorem only holds for $\R^n$ and not
for metric spaces in general.  In general, compact implies closed and
bounded, but not vice versa.

\begin{proof}
For $\R = \R^1$ if $K \subset \R$ is closed and bounded, then
any sequence $\{ x_k \}$ in $K$ is bounded, so it has a convergent
subsequence by
Bolzano--Weierstrass theorem (\thmref{thm:bwseq}).
As $K$ is closed, the limit of the subsequence must be an element of
$K$.  So $K$ is compact.

Let us carry out the proof for $n=2$ and leave arbitrary $n$ as an exercise.
As $K \subset \R^2$ is bounded, there exists a set
$B=[a,b]\times[c,d] \subset \R^2$ such that $K \subset B$.  We will show
that $B$ is compact.  Then $K$, being a closed subset of a compact $B$, is
also compact.  

Let $\{ (x_k,y_k) \}_{k=1}^\infty$ be a sequence in $B$.  That is,
$a \leq x_k \leq b$ and
$c \leq y_k \leq d$ for all $k$.  A bounded sequence of real numbers
has a convergent
subsequence so there is a subsequence $\{ x_{k_j} \}_{j=1}^\infty$
that is convergent.  The subsequence 
$\{ y_{k_j} \}_{j=1}^\infty$ is also a bounded sequence so there exists
a subsequence
$\{ y_{k_{j_i}} \}_{i=1}^\infty$ that is convergent.  A subsequence of a
convergent sequence is still convergent, so 
$\{ x_{k_{j_i}} \}_{i=1}^\infty$ is convergent.
Let
\begin{equation*}
x := \lim_{i\to\infty} x_{k_{j_i}}
\qquad \text{and} \qquad
y := \lim_{i\to\infty} y_{k_{j_i}} .
\end{equation*}
By \propref{prop:msconveuc},
$\bigl\{ (x_{k_{j_i}},y_{k_{j_i}}) \bigr\}_{i=1}^\infty$ converges to $(x,y)$.
Furthermore, as $a \leq x_k \leq b$ and
$c \leq y_k \leq d$ for all $k$, we know that $(x,y) \in B$.
\end{proof}

\begin{example}
The discrete metric provides interesting counterexamples again.
Let $(X,d)$ be a metric space with the discrete metric, that is $d(x,y) = 1$
if $x \not= y$.  Suppose
$X$ is an infinite set.  Then:
\begin{enumerate}[(i)]
\item $(X,d)$ is a complete metric space.
\item Any subset $K \subset X$ is closed and bounded.
\item A subset $K \subset X$ is compact if and only if it is a finite set.
\item The conclusion of the Lebesgue covering lemma is always satisfied with
e.g. $\delta = \nicefrac{1}{2}$, even for noncompact $K \subset X$.
\end{enumerate}
The proofs
of these statements are either trivial or are relegated to the exercises
below.
\end{example}

\subsection{Exercises}

\begin{exercise}
Let $(X,d)$ be a metric space and $A$ a finite subset of $X$.
Show that $A$ is compact.
\end{exercise}

\begin{exercise}
Let $A = \{ \nicefrac{1}{n} : n \in \N \} \subset \R$.  a)~Show that $A$ is
not compact directly using the definition.  b)~Show that $A \cup \{ 0 \}$ is
compact directly using the definition.
\end{exercise}


\begin{exercise}
Let $(X,d)$ be a metric space with the discrete metric.  a)~Prove that
$X$ is complete.  b)~Prove that $X$ is compact if and only if $X$ is a finite
set.
\end{exercise}

\begin{exercise}
a)~Show that the union of finitely many compact sets is a compact set.
b)~Find an example where the union of infinitely many compact sets is not
compact.
\end{exercise}

\begin{exercise}
Prove \thmref{thm:msbw} for arbitrary dimension.
Hint: The trick is to use the correct notation.
\end{exercise}

\begin{exercise}
Show that a compact set $K$ is a complete metric space (using the subspace
metric).
\end{exercise}

\begin{exercise} \label{exercise:CabRcomplete}
Let $C([a,b],\R)$ be the metric space as in \exampleref{example:msC01}.  Show that
$C([a,b],\R)$ is a complete metric space.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:msclbounnotcompt}
Let $C([0,1],\R)$ be the metric space of \exampleref{example:msC01}.  Let $0$
denote the zero function.  Then show that the closed ball
$C(0,1)$ is not compact (even though it is closed and bounded).
Hints: Construct a sequence of distinct continuous functions $\{ f_n \}$ such that
$d(f_n,0) = 1$ and $d(f_n,f_k) = 1$ for all $n \not= k$.  Show that
the set $\{ f_n : n \in \N \} \subset C(0,1)$ is closed but not compact.
See \chapterref{fs:chapter} for inspiration.
\end{exercise}

\begin{exercise}[Challenging]
Show that there exists a metric on $\R$ that makes $\R$ into a compact set.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is complete and suppose we have a countably infinite
collection of nonempty compact sets $E_1 \supset E_2 \supset E_3 \supset
\cdots$ then prove $\bigcap_{j=1}^\infty E_j \not= \emptyset$.
\end{exercise}

\begin{exercise}[Challenging]
Let $C([0,1],\R)$ be the metric space of \exampleref{example:msC01}.
Let $K$ be the set of $f \in C([0,1],\R)$ such that
$f$ is equal to a quadratic polynomial, i.e.\ $f(x) = a+bx+cx^2$, and such that
$\abs{f(x)} \leq 1$ for all $x \in [0,1]$,
that is $f \in C(0,1)$.  Show that $K$ is compact.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:mstotbound}
Let $(X,d)$ be a complete metric space.
Show that $K \subset X$ is compact if and only if $K$ is closed
and such that for every $\epsilon > 0$
there exists a finite set of points $x_1,x_2,\ldots,x_n$ with
%\begin{equation*}
$K \subset \bigcup_{j=1}^n B(x_j,\epsilon)$.
%\end{equation*}
Note: Such a set $K$ is said to be \emph{\myindex{totally bounded}},
so in a complete metric space a set is compact if and only
if it is closed and totally bounded.
\end{exercise}

\begin{exercise}
Take $\N \subset \R$ using the standard metric.  Find an open cover of $\N$
such that the conclusion of the Lebesgue covering lemma does not hold.
\end{exercise}

\begin{exercise}
Prove the general \myindex{Bolzano--Weierstrass theorem}:
Any bounded sequence $\{ x_k
\}$ in $\R^n$ has a convergent subsequence.
\end{exercise}

\begin{exercise}
Let $X$ be a metric space and
$C \subset \sP(X)$ the set of nonempty compact subsets of $X$.
Using the Hausdorff metric from \exerciseref{exercise:mshausdorffpseudo},
show that $(C,d_H)$ is a metric space.  That is, show that
if $L$ and $K$ are nonempty compact subsets then $d_H(L,K) = 0$
if and only if $L=K$.
\end{exercise}

\begin{exercise} \label{exercise:closedcomplete}
Let $(X,d)$ be a complete metric space and $E \subset X$ a closed set.
Show that $E$ with the subspace metric is a complete metric space.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Continuous functions}
\label{sec:metcont}

\sectionnotes{1 lecture}

\subsection{Continuity}

\begin{defn}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces and $c \in X$.
Then $f \colon X \to Y$ is
\emph{continuous at $c$}\index{continuous at $c$}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x \in X$ and $d_X(x,c) <
\delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.

\medskip

When $f \colon X \to Y$ is continuous at all $c \in X$, then we simply say
that $f$ is a \emph{\myindex{continuous function}}.
\end{defn}

The definition agrees with the definition from \chapterref{lim:chapter} when
$f$ is a real-valued function on the real line, if we take the standard
metric on $\R$.

\begin{prop} \label{prop:contiscont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Then $f \colon X \to Y$ is
continuous at $c \in X$
if and only if for every sequence $\{ x_n \}$ in $X$
converging to $c$, the sequence $\{ f(x_n) \}$ converges
to $f(c)$.
\end{prop}

\begin{proof}
Suppose $f$ is continuous at $c$.  Let $\{ x_n \}$ be a
sequence in $X$ converging to $c$.  Given $\epsilon > 0$,
there is a $\delta > 0$ such that $d_X(x,c) < \delta$ implies
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.  So take $M$ such that
for all $n \geq M$, we have $d_X(x_n,c) < \delta$, then
$d_Y\bigl(f(x_n),f(c)\bigr) < \epsilon$.  Hence $\{ f(x_n) \}$
converges to $f(c)$.

On the other hand suppose $f$ is not continuous at $c$.
Then there exists an $\epsilon > 0$,
such that for every $n \in \N$ there exists an $x_n \in X$,
with
$d_X(x_n,c) < \nicefrac{1}{n}$ such that $d_Y\bigl(f(x_n),f(c)\bigr) \geq
\epsilon$.  Then $\{ x_n \}$ converges to $c$, but $\{ f(x_n) \}$
does not converge to $f(c)$.
\end{proof}

\begin{example}
Suppose $f \colon \R^2 \to \R$ is a polynomial.  That is,
\begin{equation*}
f(x,y) =
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk}\,x^jy^k =
a_{0\,0} + a_{1\,0} \, x +
a_{0\,1} \, y+  
a_{2\,0} \, x^2+  
a_{1\,1} \, xy+  
a_{0\,2} \, y^2+ \cdots +
a_{0\,d} \, y^d ,
\end{equation*}
for some $d \in \N$ (the degree) and $a_{jk} \in \R$.  Then we claim 
$f$ is continuous.  Let $\{ (x_n,y_n) \}_{n=1}^\infty$ be a sequence
in $\R^2$ that converges to $(x,y) \in \R^2$.  We proved that this
means $\lim\, x_n = x$ and $\lim\, y_n = y$.
By \propref{prop:contalg} we have
\begin{equation*}
\lim_{n\to\infty}
f(x_n,y_n) =
\lim_{n\to\infty}
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk} \, x_n^jy_n^k 
=
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk} \, x^jy^k
=
f(x,y) .
\end{equation*}
So $f$ is continuous at $(x,y)$, and as $(x,y)$ was arbitrary $f$ is
continuous everywhere.  Similarly, a
polynomial in $n$ variables is continuous.
\end{example}

Be careful about taking limits separately.  In \exerciseref{exercise:dicontR2}
you are asked to prove that the function defined by $f(x,y) := \frac{xy}{x^2+y^2}$
outside the origin and $f(0,0) := 0$, is not continuous at the origin.  See
\figureref{fig:xyxsqysq}.
However, for any $y$, the function $g(x) := f(x,y)$ is
continuous,
and for any $x$, the function $h(y) := f(x,y)$ is continuous.
\begin{myfigureht}
%\begin{center}
% to get the fonts right input eepic
% But it's REALLY slow, so use the rendered .pdf version, and there is
% not much text
%\subimport*{figures/}{xyxsqysq.eepic}
\includegraphics{figures/xyxsqysq}
\caption{Graph of $\frac{xy}{x^2+y^2}$.\label{fig:xyxsqysq}}
%\end{center}
\end{myfigureht}

\begin{example}
Let $X$ be a metric space and take $f \colon X \to \C$ be a complex-valued
function.  We write $f(p) = g(p) + i h(p)$, where $g \colon X \to \R$
and $h \colon X \to \R$ are the real and imaginary parts of $f$.
Then $f$ is continuous at $c \in X$ if and only its real
and imaginary parts are continuous at $c$.  
The proof follows because $\{ f(p_n) = g(p_n) + i h(p_n) \}_{n=1}^\infty$
converges to $f(p) = g(p) + i h(p)$ if and only if
$\{ g(p_n) \}$ converges to $g(p)$ and
$\{ h(p_n) \}$ converges to $h(p)$.
\end{example}

\subsection{Compactness and continuity}

Continuous maps do not map closed sets to closed sets.  For example,
$f \colon (0,1) \to \R$ defined by $f(x) := x$ takes the set $(0,1)$, which
is closed in $(0,1)$, to the set $(0,1)$, which is not closed in $\R$.
On the other hand continuous maps do preserve compact sets.

\begin{lemma} \label{lemma:continuouscompact}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces
and $f \colon X \to Y$ a continuous function.  If
$K \subset X$ is a compact set, then $f(K)$ is a compact set.
\end{lemma}

\begin{proof}
A sequence in $f(K)$ can be written as
$\{ f(x_n) \}_{n=1}^\infty$, where
$\{ x_n \}_{n=1}^\infty$ is a sequence in $K$.  The set $K$ is compact and
therefore there is a subsequence
$\{ x_{n_i} \}_{i=1}^\infty$ that converges to some $x \in K$.
By continuity,
\begin{equation*}
\lim_{i\to\infty} f(x_{n_i}) = f(x) \in f(K) .
\end{equation*}
So every sequence in $f(K)$ has a subsequence convergent to 
a point in $f(K)$, and $f(K)$ is compact by \thmref{thm:mscompactisseqcpt}.
\end{proof}

As before, $f \colon X \to \R$ achieves an
\emph{\myindex{absolute minimum}} at $c \in X$ if
\begin{equation*}
f(x) \geq f(c) \qquad \text{ for all $x \in X$.}
\end{equation*}
On the other hand, $f$ achieves an 
\emph{\myindex{absolute maximum}} at $c \in X$ if
\begin{equation*}
f(x) \leq f(c) \qquad \text{ for all $x \in X$.}
\end{equation*}

\begin{thm}
Let $(X,d)$ be a compact metric space
and $f \colon X \to \R$ a continuous function.  Then
$f$ is bounded and in fact
$f$ achieves an absolute minimum and an absolute maximum on $X$.
\end{thm}

\begin{proof}
As $X$ is compact and $f$ is continuous, we have
that $f(X) \subset \R$ is compact.  Hence $f(X)$ is closed
and bounded.  In particular,
$\sup f(X) \in f(X)$ and
$\inf f(X) \in f(X)$, because both the sup and inf
can be achieved by sequences in $f(X)$ and $f(X)$ is closed.
Therefore there is some $x \in X$ such that $f(x) = \sup f(X)$
and some $y \in X$ such that $f(y) = \inf f(X)$.
\end{proof}

\subsection{Continuity and topology}

Let us see how to define continuity in terms of the topology, that is,
the open sets.  We have already seen that topology determines which 
sequences converge, and so it is no wonder that the topology also
determines continuity of functions.

\begin{lemma} \label{lemma:mstopocontloc}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
A function $f \colon X \to Y$ is continuous at $c \in X$
if and only if for every open neighborhood $U$ of $f(c)$ in $Y$, the set
$f^{-1}(U)$ contains an open neighborhood of $c$ in $X$.
\end{lemma}

\begin{proof}
First suppose that $f$ is continuous at $c$.
Let $U$ be an open neighborhood of $f(c)$
in $Y$, then $B_Y\bigl(f(c),\epsilon\bigr) \subset U$ for some $\epsilon >
0$.  By continuity of $f$, there exists a $\delta > 0$
such that whenever $x$ is such that $d_X(x,c) < \delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.  In other words,
\begin{equation*}
B_X(c,\delta) \subset f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr) \subset
f^{-1}(U) ,
\end{equation*}
and $B_X(c,\delta)$ is an open neighborhood of $c$.

For the other direction,
let $\epsilon > 0$ be given.  If
$f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr)$ contains an open
neighborhood $W$ of $c$, it contains a ball.  That is, there is some $\delta > 0$
such that
\begin{equation*}
B_X(c,\delta) \subset W \subset f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr) .
\end{equation*}
That means precisely that if $d_X(x,c) < \delta$ then $d_Y\bigl(f(x),f(c)\bigr)
< \epsilon$, and so $f$ is continuous at $c$.
\end{proof}

\begin{thm} \label{thm:mstopocont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.  A function $f \colon X \to Y$
is continuous if and only if
for every open $U \subset Y$, $f^{-1}(U)$ is open in $X$.
\end{thm}

The proof follows from \lemmaref{lemma:mstopocontloc} and is left as
an exercise.

\begin{example}
Let $f \colon X \to Y$ be a continuous function.
\thmref{thm:mstopocont} tells us that if $E \subset Y$ is closed, then 
$f^{-1}(E) = X \setminus f^{-1}(E^c)$ is also closed.  Therefore if
we have a continuous
function $f \colon X \to \R$, then the
\emph{\myindex{zero set}} of $f$, that is, 
$f^{-1}(0) = \{ x \in X :
f(x) = 0 \}$, is closed.  We have just proved the most basic result in
\emph{\myindex{algebraic geometry}}, the study of
zero sets of polynomials.

Similarly the set where $f$ is nonnegative, that is,
$f^{-1}\bigl( [0,\infty) \bigr) = \{ x \in X :
f(x) \geq 0 \}$ is closed.  On the other hand the
set where $f$ is positive,
$f^{-1}\bigl( (0,\infty) \bigr) = \{ x \in X :
f(x) > 0 \}$ is open.  
\end{example}

\subsection{Uniform continuity}

As for continuous
functions on the real line, in the definition of continuity
it is sometimes convenient to be able to pick
one $\delta$ for all points.

\begin{defn}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Then $f \colon X \to Y$ is
\emph{uniformly continuous}\index{uniformly continuous!metric spaces}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x,c \in X$ and $d_X(x,c) <
\delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.
\end{defn}

A uniformly continuous function is continuous, but not necessarily
vice-versa as we have seen.

\begin{thm} \label{thm:Xcompactfunifcont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Suppose $f \colon X \to Y$ is continuous and $X$ compact.  Then
$f$ is uniformly continuous.
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given.  For each $c \in X$, pick $\delta_c > 0$ such that
$d_Y\bigl(f(x),f(c)\bigr) < \nicefrac{\epsilon}{2}$
whenever
$d_X(x,c) < \delta_c$.
The balls
$B(c,\delta_c)$ cover $X$, and the space $X$ is compact.  
Apply the \hyperref[ms:lebesgue]{Lebesgue covering lemma} to obtain a 
$\delta > 0$ such that for every $x \in X$, there is a $c \in X$
for which $B(x,\delta) \subset B(c,\delta_c)$.

If $x_1, x_2 \in X$ where $d_X(x_1,x_2) < \delta$,
find a $c \in X$ such that $B(x_1,\delta) \subset B(c,\delta_c)$.
Then $x_2 \in B(c,\delta_c)$.  By the triangle inequality
and the definition of $\delta_c$ we have
\begin{equation*}
d_Y\bigl(f(x_1),f(x_2)\bigr)
\leq
d_Y\bigl(f(x_1),f(c)\bigr)
+
d_Y\bigl(f(c),f(x_2)\bigr)
<
\nicefrac{\epsilon}{2}+
\nicefrac{\epsilon}{2} = \epsilon .  \qedhere
\end{equation*}
\end{proof}

\begin{example}
Useful examples of uniformly continuous functions are again the so-called
\emph{\myindex{Lipschitz continuous}} functions.  That is if
$(X,d_X)$ and $(Y,d_Y)$ are metric spaces, then $f \colon X \to Y$
is called Lipschitz or $K$-Lipschitz if there exists a $K \in \R$ such that
\begin{equation*}
d_Y\bigl(f(x),f(c)\bigr) \leq K d_X(x,c)
\ \ \ \ \text{for all } x,c \in X.
\end{equation*}
It is not difficult to prove that Lipschitz implies uniformly continuous,
just take $\delta = \nicefrac{\epsilon}{K}$.  And we already saw in the case
of functions on the real line, a function can be uniformly continuous
but not Lipschitz.

It is worth mentioning that,
if a function is Lipschitz, it tends to be
easiest to simply show it is Lipschitz even if we are only
interested in knowing continuity.
\end{example}

\subsection{Exercises}

\begin{exercise}
Consider $\N \subset \R$ with the standard metric.  Let $(X,d)$ be a
metric space and $f \colon X \to \N$ a continuous function.  a) Prove that
if $X$ is connected, then $f$ is constant (the range of $f$ is a single
value).  b)~Find an example where $X$ is disconnected and $f$ is not constant.
\end{exercise}

\begin{exercise} \label{exercise:dicontR2}
Let $f \colon \R^2 \to \R$ be defined by $f(0,0) := 0$, and
$f(x,y) := \frac{xy}{x^2+y^2}$ if $(x,y) \not= (0,0)$.  a) Show that for any fixed $x$,
the function that takes $y$ to $f(x,y)$ is continuous.  Similarly
for any fixed $y$, the function that takes $x$ to $f(x,y)$ is continuous.
b)~Show that $f$ is not continuous.
\end{exercise}

\begin{exercise} 
Suppose that $f \colon X \to Y$ is continuous for metric spaces $(X,d_X)$
and $(Y,d_Y)$.  Let $A \subset X$.  a)~Show that $f(\overline{A}) \subset
\overline{f(A)}$.  b)~Show that the subset can be proper.
\end{exercise}

\begin{exercise}
Prove \thmref{thm:mstopocont}.  Hint: Use \lemmaref{lemma:mstopocontloc}.
\end{exercise}

\begin{exercise} \label{exercise:msconnconn}
Suppose $f \colon X \to Y$ is continuous for metric spaces $(X,d_X)$
and $(Y,d_Y)$.  Show that if $X$ is connected, then $f(X)$ is connected.
\end{exercise}

\begin{exercise}
Prove the following version of the
\hyperref[IVT:thm]{intermediate value theorem}.  Let $(X,d)$ be a connected
metric space and $f \colon X \to \R$ a continuous function.  Suppose that
there exist $x_0,x_1 \in X$ and $y \in \R$ such that $f(x_0) < y < f(x_1)$.
Then prove that there exists a $z \in X$ such that $f(z) = y$.
Hint: See \exerciseref{exercise:msconnconn}.
\end{exercise}

\begin{exercise}
A continuous function $f \colon X \to Y$ for metric spaces $(X,d_X)$ and
$(Y,d_Y)$ is said to be \emph{\myindex{proper}}
if for every compact set $K \subset Y$, the set $f^{-1}(K)$ is compact.
Suppose a continuous $f \colon (0,1) \to (0,1)$ is proper and $\{ x_n
\}$ is a sequence in $(0,1)$ that converges to $0$.  Show that
$\{ f(x_n) \}$ has no subsequence that converges in $(0,1)$.
\end{exercise}

\begin{exercise}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric space and
$f \colon X \to Y$ be a one-to-one and onto continuous function.  Suppose
$X$ is compact.  Prove that the inverse $f^{-1} \colon Y \to X$
is continuous.
\end{exercise}

\begin{exercise}
Take the metric space of continuous functions $C([0,1],\R)$.  Let
$k \colon [0,1] \times [0,1] \to \R$ be a continuous function.
Given $f \in C([0,1],\R)$ define
\begin{equation*}
\varphi_f(x) := \int_0^1 k(x,y) f(y) ~dy .
\end{equation*}
a) Show that $T(f) := \varphi_f$ defines a function $T \colon C([0,1],\R) \to
C([0,1],\R)$.
\\
b) Show that $T$ is continuous.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.\\
a) If $p \in X$,
show that $f \colon X \to \R$ defined
by $f(x) := d(x,p)$ is continuous.
\\
b) Define a metric on $X \times X$ as in \exerciseref{exercise:mscross} part
b, and show that $g \colon X \times X \to \R$ defined by
$g(x,y) := d(x,y)$ is continuous.
\\
c) Show that if $K_1$ and $K_2$ are compact subsets of $X$, then
there exists a $p \in K_1$ and $q \in K_2$ such that $d(p,q)$ is minimal,
that is, $d(p,q) = \inf \{ (x,y) \colon x \in K_1, y \in K_2 \}$.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a compact metric space, let $C(X,\R)$ be the set
of real-valued continuous functions.  Define
\begin{equation*}
d(f,g) := \snorm{f-g}_u := \sup_{x \in X} \abs{f(x)-g(x)} .
\end{equation*}
a) Show that $d$ makes $C(X,\R)$ into a metric space.\\
b) Show that for any $x \in X$, the evaluation function
$E_x \colon C(X,\R) \to \R$ defined by $E_x(f) := f(x)$
is a continuous function.
\end{exercise}

\begin{exercise}
Let $C([a,b],\R)$ be the set of continuous functions and
$C^1([a,b],\R)$ the set of once continuously differentiable
functions on $[a,b]$.
Define
\begin{equation*}
d_{C}(f,g) := \snorm{f-g}_u
\qquad \text{and} \qquad
d_{C^1}(f,g) := \snorm{f-g}_u + \snorm{f'-g'}_u,
\end{equation*}
where $\snorm{\cdot}_u$ is the uniform norm.
By \exampleref{example:msC01} and \exerciseref{exercise:C1ab} we know that
$C([a,b],\R)$ with $d_C$ is a metric space and
so is
$C^1([a,b],\R)$ with $d_{C^1}$.\\
a) Prove that the derivative operator $D \colon 
C^1([a,b],\R) \to C([a,b],\R)$ defined by
$D(f) := f'$ is continuous.
\\
b) On the other hand if we consider the metric $d_C$ on $C^1([a,b],\R)$,
then prove the derivative operator is no longer continuous.  Hint: Consider
$\sin(n x)$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Fixed point theorem and Picard's theorem again}
\label{sec:metpicard}

\sectionnotes{1 lecture (optional, does not require \sectionref{sec:picard})}

In this section we prove the fixed point theorem for contraction
mappings.  As an application we prove Picard's theorem, which we proved
without metric spaces in \sectionref{sec:picard}.
The proof we present here is similar, but the proof goes a lot
smoother with metric spaces and the fixed point theorem.

\subsection{Fixed point theorem}

\begin{defn}
Let $(X,d)$ and $(X',d')$ be metric spaces.
$f \colon X \to X'$ is said to be a \emph{contraction}
(or a contractive map) if it is
a $k$-Lipschitz map for some $k < 1$, i.e.\ if there exists a $k < 1$ such that
\begin{equation*}
d'\bigl(f(x),f(y)\bigr) \leq k d(x,y)
\ \ \ \ \text{for all } x,y \in X.
\end{equation*}

\medskip

If $f \colon X \to X$ is a map, $x \in X$ is called a \emph{fixed point}
if $f(x)=x$.
\end{defn}

\begin{thm}%
[\myindex{Contraction mapping principle} or \myindex{Fixed point theorem}]
\label{thm:contr}
Let $(X,d)$ be a nonempty complete metric space and $f \colon X \to X$ a
contraction.
Then $f$ has a unique fixed point.
\end{thm}

The words \emph{complete} and \emph{contraction} are necessary.
See \exerciseref{exercise:nofixedpoint}.

\begin{proof}
Pick any $x_0 \in X$.
Define a sequence $\{ x_n \}$ by $x_{n+1} := f(x_n)$.
\begin{equation*}
d(x_{n+1},x_n) = d\bigl(f(x_n),f(x_{n-1})\bigr)
\leq k d(x_n,x_{n-1})
\leq \cdots
\leq k^n d(x_1,x_0) .
\end{equation*}
Suppose $m > n$, then
\begin{equation*}
\begin{split}
d(x_m,x_n)
& \leq \sum_{i=n}^{m-1} d(x_{i+1},x_i) \\
& \leq \sum_{i=n}^{m-1} k^i d(x_1,x_0) \\
& = k^n d(x_1,x_0) \sum_{i=0}^{m-n-1} k^i \\
& \leq k^n d(x_1,x_0) \sum_{i=0}^{\infty} k^i
= k^n d(x_1,x_0) \frac{1}{1-k} .
\end{split}
\end{equation*}
In particular the sequence is Cauchy (why?).  Since $X$ is complete
we let $x := \lim\, x_n$, and we claim that $x$
is our unique fixed point.

Fixed point?  The function $f$ is continuous as it is a contraction, so
Lipschitz continuous.
Hence
\begin{equation*}
f(x) = f( \lim \, x_n) = \lim\, f(x_n) = \lim\, x_{n+1} = x .
\end{equation*}

Unique?  Let $x$ and $y$ both be fixed points.
\begin{equation*}
d(x,y) = d\bigl(f(x),f(y)\bigr) \leq k d(x,y) .
\end{equation*}
As $k < 1$ this means that $d(x,y) = 0$ and hence $x=y$.  The theorem is
proved.
\end{proof}

The proof is constructive.  Not only do we know 
a unique fixed point exists.  We also know how to find it.  We start with
any point $x_0 \in X$ and simply iterate $f(x_0)$,
$f(f(x_0))$,
$f(f(f(x_0)))$, etc\ldots  In fact, you can even find how far away
from the fixed point you are, see the exercises.  The idea of the proof is
therefore used in real world applications.

\subsection{Picard's theorem}

Before we get to Picard, let us mention what metric space we will be
applying the fixed point theorem to.  We will use the metric space
$C([a,b],\R)$ of \exampleref{example:msC01}.  That is, $C([a,b],\R)$
is the space of continuous functions $f \colon [a,b] \to \R$ with the metric
\begin{equation*}
d(f,g) = \sup_{x \in [a,b]} \abs{f(x)-g(x)} .
\end{equation*}
Convergence in this metric is convergence in uniform norm, or in other
words, uniform convergence.  Therefore, see
\exerciseref{exercise:CabRcomplete}, $C([a,b],\R)$ is a complete metric space.

\medskip

Let us use the
fixed point theorem
to prove the classical Picard theorem on the existence and uniqueness of
ordinary differential equations.
Consider the equation
\begin{equation*}
\frac{dy}{dx} = F(x,y) .
\end{equation*}
Given some $x_0, y_0$ we are looking for a function $y=f(x)$ such that
$f(x_0) = y_0$ and such that
\begin{equation*}
f'(x) = F\bigl(x,f(x)\bigr) .
\end{equation*}
To avoid having to come up with many names we often simply write $y' = F(x,y)$
and $y(x)$ for the solution.

The simplest example is for example the equation $y' = y$, $y(0) = 1$.
The solution is the exponential $y(x) = e^x$.  A somewhat more complicated
example is $y' = -2xy$, $y(0) = 1$, whose solution is the Gaussian
$y(x) = e^{-x^2}$.

There are some subtle issues, for example how long does the
solution exist.
Look at the equation $y' = y^2$, $y(0)=1$.  Then $y(x) = \frac{1}{1-x}$ is a
solution.  While $F$ is a reasonably ``nice'' function and in particular
exists for all $x$ and $y$, the solution ``blows up'' at $x=1$.
For more examples related to Picard's theorem see \sectionref{sec:picard}.

\begin{thm}[Picard's theorem on existence and uniqueness]%
\index{existence and uniqueness theorem}\index{Picard's theorem}
Let $I, J \subset \R$ be compact intervals, let $I_0$ and $J_0$
be their interiors, and 
let $(x_0,y_0) \in I_0 \times J_0$.
Suppose $F \colon I \times J \to \R$ is continuous
and Lipschitz in the second variable, that is, there exists
an $L \in \R$ such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Then there exists an $h > 0$ and a unique differentiable
function
$f \colon [x_0 - h, x_0 + h] \to J \subset \R$, such that
\begin{equation*}
f'(x) = F\bigl(x,f(x)\bigr) \qquad \text{and} \qquad f(x_0) = y_0.
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality assume $x_0 =0$.
As $I \times J$ is compact and
$F(x,y)$ is continuous, it is bounded.
So find an $M > 0$, such that
$\abs{F(x,y)} \leq M$ for all $(x,y) \in I\times J$.
%Let $M := \sup \{ \abs{F(x,y)} : (x,y) \in I\times J \}$, nah because we
%want $M > 0$.
Pick $\alpha > 0$ such that
$[-\alpha,\alpha] \subset I$ and $[y_0-\alpha, y_0 + \alpha] \subset J$.
Let
\begin{equation*}
h := \min \left\{ \alpha, \frac{\alpha}{M+L\alpha} \right\} .
\end{equation*}
Note $[-h,h] \subset I$.  Define the set
\begin{equation*}
Y := \{ f \in C([-h,h],\R) : f([-h,h]) \subset J \} . % [y_0-\alpha,y_0+\alpha] \} .
\end{equation*}
That is, $Y$ is the space of continuous functions on $[-h,h]$ with values in
$J$, in other words,
exactly those functions where $F\bigl(x,f(x)\bigr)$ makes sense.
The metric used is the standard metric given above.
%Here $C([-h,h],\R)$ is equipped with the standard metric $d(f,g) := 
%\sup \{ \abs{f(x)-g(x)} : x \in [-h,h] \}$.  With this metric
%you have shown in \exerciseref{exercise:CabRcomplete} that $C([-h,h],\R)$ is a complete metric space.
%Here $C([-h,h],\R)$ is equipped with the standard metric $d(f,g) := 
%\sup \{ \abs{f(x)-g(x)} : x \in [-h,h] \}$.  With this metric
%you have shown in \exerciseref{exercise:CabRcomplete} that $C([-h,h],\R)$ is a complete metric space.

\begin{exercise}
Show that $Y \subset C([-h,h],\R)$ is closed.  Hint: $J$ is closed.
\end{exercise}

The space $C([-h,h],\R)$ is complete, and
a closed subset of a complete metric space is a complete metric space with
the subspace metric, see \exerciseref{exercise:closedcomplete}.  So $Y$ with the subspace metric is
complete.

Define a mapping
$T \colon Y \to C([-h,h],\R)$ by
\begin{equation*}
T(f)(x)
:=
y_0 + \int_0^x F\bigl(t,f(t)\bigr)~dt .
\end{equation*}

\begin{exercise}
Show that if $f \colon [-h,h] \to J$ is continuous then $F\bigl(t,f(t)\bigr)$
is continuous on $[-h,h]$ as a function of $t$.  Use this to show that
$T$ is well defined and that $T(f) \in C([-h,h],\R)$.
\end{exercise}

Let $f \in Y$ and $\abs{x} \leq h$.
As $F$ is bounded by $M$ we have
\begin{equation*}
\begin{split}
\abs{T(f)(x) - y_0}
&= \abs{\int_0^x F\bigl(t,f(t)\bigr)~dt} \\
& \leq 
\abs{x}M \leq hM \leq \frac{\alpha M}{M+ L\alpha} \leq \alpha .
\end{split}
\end{equation*}
So $T(f)([-h,h]) \subset [y_0-\alpha,y_0+\alpha] \subset J$, and
$T(f) \in Y$.  In other words, $T(Y) \subset Y$.  We thus consider
$T$ as a mapping of $Y$ to $Y$.

We claim $T \colon Y \to Y$ is a contraction.  First, for $x \in [-h,h]$
and $f,g \in Y$ we have
\begin{equation*}
\abs{F\bigl(x,f(x)\bigr) - F\bigl(x,g(x)\bigr)} \leq
L\abs{f(x)- g(x)} \leq L \, d(f,g) .
\end{equation*}
Therefore,
\begin{equation*}
\begin{split}
\abs{T(f)(x) - T(g)(x)}
&= \abs{\int_0^x F\bigl(t,f(t)\bigr) - F\bigl(t,g(t)\bigr)~dt} \\
& \leq \abs{x} L \, d(f,g)
 \leq h L\, d(f,g)
 \leq \frac{L\alpha}{M+L\alpha} \, d(f,g) .
\end{split}
\end{equation*}
We chose $M > 0$ and so
$\frac{L\alpha}{M+L\alpha} < 1$.  The claim is proved by
taking supremum over $x \in [-h,h]$ of the left hand side above to obtain
$d\bigl(T(f),T(g)\bigr) \leq \frac{L\alpha}{M+L\alpha} \, d(f,g)$.

We apply the fixed point theorem (\thmref{thm:contr})
to find a unique $f \in Y$ such that $T(f) = f$, that is,
\begin{equation*} %\label{equation:msinteqpicard}
f(x) = y_0 + \int_0^x F\bigl(t,f(t)\bigr)~dt .
\end{equation*}
By the fundamental theorem of calculus, $T(f)$ is the unique differentiable 
function whose derivative is
$F\bigl(x,f(x)\bigr)$ and $T(f)(0) = y_0$.  Therefore $f$ is the unique
solution of $f'(x) = F\bigl(x,f(x)\bigr)$ and $f(0) = y_0$.
\end{proof}

%\begin{exercise}
%We have shown that $f$ is the unique function in $Y$.  Why is it the unique
%continuous function $f \colon [-h,h] \to J$ that solves
%\eqref{equation:msinteqpicard} above?
%\end{exercise}

\begin{exercise}
Prove that the statement ``Without loss of generality assume $x_0 = 0$'' is
justified.  That is, prove that if we know the theorem with $x_0 = 0$, the
theorem is true as stated.
\end{exercise}

\subsection{Exercises}

\begin{exnote}
For more exercises related to Picard's theorem see \sectionref{sec:picard}.
\end{exnote}

\begin{exercise}
Let $F \colon \R \to \R$ be defined by
$F(x) := kx + b$ where $0 < k < 1$, $b \in \R$.\\
a) Show that $F$ is a contraction.\\
b) Find the fixed
point and show directly that it is unique.
\end{exercise}

\begin{exercise}
Let $f \colon [0,\nicefrac{1}{4}] \to [0,\nicefrac{1}{4}]$ be defined by
$f(x) := x^2$ is a contraction.\\
a) Show that $f$
is a contraction, and find the best (smallest) $k$ from the definition that works.\\
b) Find the fixed point and show directly that it is unique.
\end{exercise}

\begin{exercise} \label{exercise:nofixedpoint}
a) Find an example of a contraction $f \colon X \to X$
of non-complete metric space $X$ with no
fixed point.
b) Find a 1-Lipschitz map $f \colon X \to X$ of a complete metric space $X$ with no fixed point.
\end{exercise}

\begin{exercise}
Consider $y' =y^2$, $y(0)=1$.  Use the iteration scheme
from the proof of the contraction mapping principle.
Start with $f_0(x) = 1$.  Find a 
few iterates (at least up to $f_2$).  Prove that
the pointwise limit of $f_n$ is $\frac{1}{1-x}$, that is for every $x$
with $\abs{x} < h$ for some $h > 0$,
prove that $\lim\limits_{n\to\infty}f_n(x) = \frac{1}{1-x}$.
\end{exercise}

\begin{exercise}
Suppose $f \colon X \to X$ is a contraction for $k < 1$.  Suppose you use the iteration
procedure with $x_{n+1} := f(x_n)$ as in the proof of the fixed point theorem.
Suppose $x$ is the fixed
point of $f$.\\
a) Show that $d(x,x_n) \leq k^n d(x_1,x_0) \frac{1}{1-k}$ for all $n \in \N$.\\
b) Suppose $d(y_1,y_2) \leq 16$ for all $y_1,y_2 \in X$, and $k=
\nicefrac{1}{2}$.  Find an $N$ such that starting at any point $x_0 \in X$, 
$d(x,x_n) \leq 2^{-16}$ for all $n \geq N$.
\end{exercise}

\begin{exercise}
Let $f(x) := x-\frac{x^2-2}{2x}$. (You may recognize Newton's method for
$\sqrt{2}$)\\
a) Prove $f\bigl([1,\infty)\bigr) \subset [1,\infty)$.\\
b) Prove that $f \colon [1,\infty) \to [1,\infty)$ is a contraction.\\
c) Apply the fixed point theorem to find an $x \geq 1$ such that
$f(x) = x$, and show that $x = \sqrt{2}$.
\end{exercise}

\begin{exercise}
Suppose $f \colon X \to X$ is a contraction, and $(X,d)$ is a metric space
with the discrete metric, that is $d(x,y) = 1$ whenever $x \not= y$.
Show that $f$ is constant, that is,
there exists a $c \in X$ such that $f(x) = c$ for all $x \in X$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage  
\phantomsection
\addcontentsline{toc}{chapter}{Further Reading}
\markboth{FURTHER READING}{FURTHER READING}
\begin{bibchapter}[Further Reading]
\begin{biblist}[\normalsize]

\bib{BS}{book}{
   author={Bartle, Robert G.},
   author={Sherbert, Donald R.},
   title={Introduction to real analysis},
   edition={3},
   publisher={John Wiley \& Sons Inc.},
   place={New York},
   date={2000},
}

\bib{DW}{book}{
   author={D'Angelo, John P.},
   author={West, Douglas B.},
   title={Mathematical Thinking: Problem-Solving and Proofs},
   edition={2},
   publisher={Prentice Hall},
   date={1999},
}

\bib{GIAM}{misc}{
   author={Fields, Joseph E.},
   title={A Gentle Introduction to the Art of Mathematics},
   note={Available at \url{http://ares.southernct.edu/~fields/GIAM/}},
}

\bib{Hammack}{misc}{
   author={Hammack, Richard},
   title={Book of Proof},
   note={Available at \url{http://www.people.vcu.edu/~rhammack/BookOfProof/}},
}

\bib{Rosenlicht}{book}{
   author={Rosenlicht, Maxwell},
   title={Introduction to analysis},
   note={Reprint of the 1968 edition},
   publisher={Dover Publications Inc.},
   place={New York},
   date={1986},
   pages={viii+254},
   isbn={0-486-65038-3},
   %review={\MR{851984 (87g:26001)}},
}

\bib{Rudin:baby}{book}{
   author={Rudin, Walter},
   title={Principles of mathematical analysis},
   edition={3},
   note={International Series in Pure and Applied Mathematics},
   publisher={McGraw-Hill Book Co.},
   place={New York},
   date={1976},
   pages={x+342},
   %review={\MR{0385023 (52 \#5893)}},
}

\bib{Trench}{book}{
   author={Trench, William F.},
   title={Introduction to real analysis},
   year={2003},
   publisher={Pearson Education},
   note={\url{http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF}},
}



\end{biblist}
\end{bibchapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage  
\phantomsection
\addcontentsline{toc}{chapter}{\indexname}  
\printindex

\end{document}
